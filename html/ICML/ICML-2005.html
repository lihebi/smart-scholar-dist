
<!doctype html>
<head><script type="text/javascript">/* <![CDATA[ */_cf_loadingtexthtml="<img alt=' ' src='/cf_scripts/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/cf_scripts/scripts/ajax";
_cf_jsonprefix='//';
_cf_websocket_port=0;
_cf_flash_policy_port=0;
_cf_clientid='046A63C29DF78FD8C2E1D62A7E98577E';/* ]]> */</script><script type="text/javascript" src="/cf_scripts/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/cfform.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/masks.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/ckeditor/ckeditor.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/chart/cfchart-server.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/ext/ext-all.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/cf_scripts/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/cf_scripts/scripts/ajax/resources/cf/cf.css" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />
<title>Proceedings of the 22nd international conference on Machine learning</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em;}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	
	.mono-text {font-size: 14px; font-family: Consolas, Menlo, Monaco, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace, serif;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}
		
	.small-link-text2 {font-size: .83em !important; 
	}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
		
.x-tabs-strip-wrap {
	overflow-y: hidden !important;
}
  --></style>
<script src="js/tagcanvas.min.js" type="text/javascript"></script>
<script type="text/javascript">
  function loadCloud() {
	try {
	  TagCanvas.Start('myCanvas','tags',{
		textColour: '#000000',
		outlineColour: '#ff00ff',
		reverse: true,
		shuffleTags:true,
		depth: 0.8,
		maxSpeed: 0.05,
		textHeight: 12,
		initial: [0.000, 0.050],
		shape: "hring",
		lock: "x"
	  });
	} catch(e) {
	  // something went wrong, hide the canvas container
	  // document.getElementById('myCanvasContainer').style.display = 'none';
	}
  };
</script>
<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>
<script type='text/javascript' src='https://www.google.com/jsapi'></script>
<style type="text/css"><!--
.google-visualization-orgchart-node {
    background-color: #FFFFFF !important;
    border: 2px solid #AFCF40 !important;
    cursor: default;
    font-family: arial,helvetica;
    text-align: center;
    vertical-align: middle;
}

iframe {float:right; 
		margin:10px;
		border: 2px solid #1B4D0E;
		
		}

a.boxed:link {text-decoration: none !important; 	Color: #000000 !important;}
a.boxed:visited  { color: #000000 !important; text-decoration: none !important;}
a.boxed:hover {color: red !important; text-decoration: underline !important;}

a.boxedh:link {text-decoration: none !important; 	Color: #000000 !important;}
a.boxedh:visited  { color: #000000 !important; text-decoration: none !important;}
a.boxedh:hover {color: red !important; text-decoration: underline !important;}		

a.boxedm:link {text-decoration: none !important; 	Color: #606060 !important;}
a.boxedm:visited  { color: #606060 !important; text-decoration: none !important;}
a.boxedm:hover {color: red !important; text-decoration: underline !important;}		

a.boxedl:link {text-decoration: none !important; 	Color: #808080   !important;}
a.boxedl:visited  { color: 	#808080 !important; text-decoration: none !important;}
a.boxedl:hover {color: red !important; text-decoration: underline !important;}				

.google-visualization-orgchart-linebottom {
    border-bottom: 1px solid #006699 !important;
}
.google-visualization-orgchart-lineleft {
    border-left: 1px solid #006699 !important;
}

.google-visualization-orgchart-lineright {
    border-right: 1px solid #006699 !important;
}

--></style>
<script type="text/javascript">


function settab() {
    var mytabs = ColdFusion.Layout.getTabLayout('citationdetails');
   
 
  mytabs.on('tabchange', function(tabpanel,activetab) { document.cookie = 'picked=' + '1102351' + ',' + activetab.id; })
 
}


function letemknow(){
  ColdFusion.Window.show('letemknow');
}

function letemknow2(){
  ColdFusion.Window.show('letemknow2');
}





function testthis(){

alert('test');
}
function loadalert(){ 
 alert("I am in the load alert");
 
}
function loadalert2(){ 
  alert("I am in the load alert2");
 
}
</script>
<script type='text/javascript'>
	  	google.load('visualization', '1', {packages:['orgchart']});
	    google.setOnLoadCallback(drawChart);
	  
	  function drawChart() {
	    var data = new google.visualization.DataTable();
        data.addColumn('string', 'Name');
        data.addColumn('string', 'Manager');
        data.addColumn('string', 'ToolTip');
  	  
		data.addRows([
          [{v:'0', f:'<div style="color:black; font-size:150%; font-style:normal">CCS&nbsp;for&nbsp;this&nbsp;Proceeding</div>'}, '', ''],
		  
        ]);
		
		if (document.getElementById('chart_div')) {
       var chart = new google.visualization.OrgChart(document.getElementById('chart_div'));
       chart.draw(data, {allowHtml:true});
		}
      }
	  
</script>
<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  
</script>
<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>
<script type="text/javascript">
function expandWatson(divID,theConcept) { 
			if (document.getElementById(divID).style.display == "none") {
				document.getElementById(divID).style.display = "block";
				document.getElementById(divID).innerHTML = theConcept + "<br />" + document.getElementById(divID).innerHTML;
			}
			 else {
				 document.getElementById(divID).style.display = "none"
			}
		}
</script>

<script type="text/javascript">
  function togglePatMap() {
        var div = document.getElementById('patmap'); 
        if (div.style.display == "none") {
            div.style.display = "block";
            document.getElementById("expandcollapsepmapa").src = "images/collapse.png";
			
			if (div.innerHTML.length == 0){
				httpGetAsyncwID("patent.cfm?id=1102351",'patmap');
				httpGetAsyncwID("simmap_track.cfm?id=1102351&how=live",'blackhole');
			}
			else {
				httpGetAsyncwID("simmap_track.cfm?id=1102351&how=cache",'blackhole');
			}
			
        }
        else {
            div.style.display = "none";
            document.getElementById("expandcollapsepmapa").src = "images/expand.png";
        }
    }
  
  function toggleCO() {
        var div = document.getElementById('codisp'); 
        if (div.style.display == "none") {
            div.style.display = "block";
            document.getElementById("expandcollapsecoa").src = "images/collapse.png";
			
			if (div.innerHTML.length == 0){
				/* httpGetAsyncwID("coint.cfm?id=1102351",'codisp'); */
				/*httpGetAsyncwID("simmap_track.cfm?id=1102351&how=live",'blackhole'); */
			}
			else {
				/* httpGetAsyncwID("simmap_track.cfm?id=1102351&how=cache",'blackhole'); */
			}
			
        }
        else {
            div.style.display = "none";
            document.getElementById("expandcollapsecoa").src = "images/expand.png";
        }
    }
	
	function httpGetAsyncwID(theUrl,theID) {
		var xmlHttp = new XMLHttpRequest();
		xmlHttp.onreadystatechange = function() {
			if (xmlHttp.readyState == 4 && xmlHttp.status == 200)
				
				document.getElementById(theID).innerHTML=xmlHttp.responseText;
		}
		xmlHttp.open("GET", theUrl, true); // true for asynchronous 
		xmlHttp.send();
	}
</script>
<meta name="citation_publisher" content="ACM"> <meta name="citation_authors" content="General Chair-Dzeroski, Saso; Program Chair-De Raedt, Luc; Program Chair-Wrobel, Stefan"> <meta name="citation_conference_title" content="Proceedings of the 22nd international conference on Machine learning"> <meta name="citation_date" content="08/07/2005"> <meta name="citation_isbn" content="1-59593-180-5"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=1102351">
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFFORM');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFDIV');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFTEXTAREA');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Event.registerOnLoad(drawChart,null,false,true);
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFWINDOW');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009123803472662=function()
	{
		_cf_bind_init_8009123803472663=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'letemknow-body','bindExpr':['letemknow.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009123803472663);var _cf_window=ColdFusion.Window.create('letemknow','<div style=\'text-align:left; color:black;\'>Did you know the ACM DL App is now available?</div>','letemknow.cfm',{ modal:false, closable:true, divid:'cf_window8009123803472661', draggable:true, resizable:true, fixedcenter:false, width:600, height:275, shadow:true, callfromtag:true, minwidth:600, minheight:275, initshow:false, destroyonclose:false, x:75, y:125});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009123803472662);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009123803472665=function()
	{
		_cf_bind_init_8009123803472666=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'letemknow2-body','bindExpr':['letemknow_recomm.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009123803472666);var _cf_window=ColdFusion.Window.create('letemknow2','<div style=\'text-align:left; color:black;\'>Did you know your Organization can subscribe to the ACM Digital Library?</div>','letemknow_recomm.cfm',{ modal:false, closable:true, divid:'cf_window8009123803472664', draggable:true, resizable:true, fixedcenter:false, width:600, height:275, shadow:true, callfromtag:true, minwidth:600, minheight:275, initshow:false, destroyonclose:false, x:70, y:175});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009123803472665);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009123803472668=function()
	{
		_cf_bind_init_8009123803472669=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide-body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009123803472669);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window8009123803472667', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009123803472668);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009123803472671=function()
	{
		_cf_bind_init_8009123803472672=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags-body','bindExpr':['showthetags.cfm?id=1102351']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009123803472672);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=1102351',{ modal:false, closable:true, divid:'cf_window8009123803472670', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009123803472671);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009123803472674=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window8009123803472673', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009123803472674);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009123803472676=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window8009123803472675', draggable:true, resizable:true, fixedcenter:false, width:600, height:600, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009123803472676);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009123803472678=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window8009123803472677', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009123803472678);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009123803472680=function()
	{
		_cf_bind_init_8009123803472681=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder-body','bindExpr':['savetobinder.cfm?id=1102351']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009123803472681);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=1102351',{ modal:false, closable:true, divid:'cf_window8009123803472679', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009123803472680);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Event.registerOnLoad(settab,null,false,true);
/* ]]> */</script>
</head>

<body style="text-align:center; font-size:100%"> 
<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'https://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
<script src='AC_RunActiveContent.js' type="text/javascript"></script>
<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>

<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-NFGCMX"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NFGCMX');</script>

<div id="header">
<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
<tr style="vertical-align:top">
<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text2"><img src="/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
</td>
<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; vertical-align:middle;" class="small-link-text2">
<table style="width:100%; border-collapse:collapse; padding:0px">
<tr><td style="text-align:center">
<div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
</td>
</tr>
</table>
</td>
<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text2">
<p style="margin-top:0px; margin-bottom:10px;">
<a href="https://dl.acm.org/signin.cfm" class="small-link-text2" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
&nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm" class="small-link-text2" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
</p>
<table style="padding: 5px; border-collapse:collapse; float:right">
<tr>
<td class="small-link-text2" style="text-align:right">
<script type="text/javascript">
								function encodeInput(form){
								    	var cleanQuery = form.elements['query'].value.replace(new RegExp( "\\+", "g" ),"%2B");
										cleanQuery = cleanQuery.replace(/#/g, "%23");
										cleanQuery = cleanQuery.replace(/(\n)/g, " ");
										cleanQuery = cleanQuery.trim();
										
										
										var ascii = /^[ -~]+$/;
										if( !ascii.test( cleanQuery ) ) {
											var fixedUseQuery = "";
											for (var i = 0, len = cleanQuery.length; i < len; i++) {
												var str = "";
												if( !ascii.test(cleanQuery[i]) ) {
										 			str = "%26%23" + cleanQuery[i].charCodeAt(0) + ";";
												} else {
										 			str = cleanQuery[i];
												}
												fixedUseQuery = fixedUseQuery + str;
											}
											cleanQuery = fixedUseQuery;
										}
										

										form.elements['query'].value = cleanQuery;
								}
							</script>
<form name="qiksearch" action="/results.cfm" onsubmit='encodeInput(this)'>
<span style="margin-left:0px"><label><input type="text" name="query" size="30" value="" /></label>&nbsp;
<input style="vertical-align:top;" type="image" alt="Search" name="Go" src="/images/search_small.jpg" />
</span>
</form>
</td>
</tr>
</table>
</td>
</tr>
<tr><td colspan="3" class="small-link-text2" style="padding-bottom:5px; padding-top:0px; text-align:center">
<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
</td>
</tr>
</table>
<map name="port" id="port">
<area shape="rect" coords="1,1,60,50" href="https://www.acm.org/" alt="ACM Home Page" />
<area shape="rect" coords="65,1,275,68" href="https://dl.acm.org/dl.cfm" alt="ACM Digital Library Home Page" />
</map>
</div>
<style>
  .watsonCont {
	  width:170px;
	  
	  color: #000000;
    font-family: Arial,Helvetica,sans-serif;
    font-size: 1em;
	margin-top: 10px;
	margin-bottom: 10px;
	 /* background-color: lightgray;*/
  }
  #watsonInside {
    border-radius: 25px;
    border: 2px solid #649134;
	margin-top: 12px;
	padding: 5px;
	height: 80px;
  }

</style>
<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
<tr style="vertical-align:top">
<td style="padding-right:10px; text-align:left" class="small-link-text">
<div id="divmain" style="border:1px solid #356b20;">
<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;">Proceedings of the 22nd international conference on Machine learning</h1>
</div>
<table class="medium-text" style="border-collapse:collapse; padding:0px; margin-left: 2px;">
<colgroup>
<col style="width:540px" />
</colgroup>
<tr style="vertical-align:top">
<td>
<table style="border-collapse:collapse; padding:2px;" class="medium-text">
<col style="width:80px;" />
<col style="width:auto" />
<tr style="vertical-align:top">
</tr>
</table>
<table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
<col style="width:80px" />
<tr>
<td valign="top" nowrap="nowrap">
General Chairs:
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81100529929&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">Saso Dzeroski</a>
</td>
<td valign="bottom">
<a href="inst_page.cfm?id=60023955" title="Institutional Profile Page"><small>Jozef Stefan Institute, Slovenia</small></a>
</td>
</tr>
<tr>
<td valign="top" nowrap="nowrap">
Program Chairs:
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81100274695&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">Luc De Raedt</a>
</td>
<td valign="bottom">
</td>
</tr>
<tr>
<td valign="top" nowrap="nowrap">
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81100099179&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">Stefan Wrobel</a>
</td>
<td valign="bottom">
</td>
</tr>
</table>
<table style="margin-top: 10px" border="0" class="medium-text" cellpadding="2" cellspacing="0">
<tr>
<td><table border="0" class="medium-text" style="margin-left:5px;" cellpadding="1" cellspacing="0">
<tr valign="top">
<td nowrap="nowrap" style="padding-top:10px;">Publication:</td>
</tr>
<tr valign="top">
<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Proceeding</td>
<td colspan="2"></td>
</tr>
<tr valign="top">
<td colspan="3" style="padding-left:10px">ICML '05 Proceedings of the 22nd international conference on Machine learning
<br />
Bonn, Germany &mdash; August 07 - 11, 2005
<br />
</td>
</tr>
<tr valign="top">
<td colspan="3" style="padding-left:10px"> <a href="https://www.acm.org/publications" class="small-link-text" title="ACM">ACM</a> <span class="small-link-text">New York, NY</span><span class="small-link-text">, USA</span> <span class="small-link-text"> &copy;2005</span>
<br />
<a href="citation.cfm?id=1102351&amp;picked=prox" target="_self" class="small-link-text">table&nbsp;of&nbsp;contents</a>
ISBN:1-59593-180-5
</td> 
</tr>
</table></td>
</tr>
</table>
</td>
<td rowspan="20">
<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
<tr>
<td align="center" style="padding-bottom: 5px;">
</td>
<td align="left" nowrap="nowrap">
<img src="images/ACM_mini.jpg" style="vertical-align:middle" title="Published by ACM" alt="Published by ACM" /> 2005 Proceeding<br />
</td>
</tr>
<tr>
<td colspan="2" valign="baseline" style="padding-bottom:5px; padding-top:5px;">
<img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
<a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
</td>
</tr>
<tr>
<td class="small-text" colspan="2" valign="top" style="padding-left:30px;">
&middot;&nbsp;Citation Count: 4,141<br />
&middot;&nbsp;Downloads (cumulative): 52,797<br />
&middot;&nbsp;Downloads (12 Months): 4,062<br />
&middot;&nbsp;Downloads (6 Weeks): 614<br />
</td>
</tr>
</table>
</td>
</tr>
</table>
<br clear="all" />
<br clear="all" />
</div>
</td>
<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
<div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
<div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;">Tools and Resources</h1></div>
<ul title="Tools and Resources" style="list-style: none; list-style-position:outside;
margin-left: 25px;
padding-left: 0em;
text-indent: 0px;
margin-bottom: 0px;">
<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:0px;">
<span class="small-link-text">TOC Service:</span>
<img src="images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
<ul style="margin-left: 0; padding-left: 0; display:inline;">
<li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
<li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">RSS</a></li>
</ul>
</span>
</li>
<li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:0px;">
<a href="citation.cfm?id=1102351&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
</span></li>
<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:0px; margin-bottom:0px">
<span class="small-link-text">Export Formats:</span>
<ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1102351&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1102351&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1102351&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
</ul>
</span>
</li>
</ul>


<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>


<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google_plusone_share"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_researchgate"></a>
<a class="addthis_button_reddit"></a>
<span class="addthis_separator">|</span>
<a href="https://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="https://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>

</div>
</td>
</tr>
</table>
</div>
<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<div id="fback" style="text-align:left; padding-top:20px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="Contact The DL Team" href="/cdn-cgi/l/email-protection#9feff0edebfef3b2f9fafafbfdfefcf4dff7eeb1fefcf2b1f0edf8" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="Contact The DL Team" border="0" /></a>
<a title="Contact The DL Team" href="/cdn-cgi/l/email-protection#7606190402171a5b101313121417151d361e075817151b58190411"><strong>Contact Us</strong></a>
<span style="padding:10px;">|</span>
<span>Switch to <a href="citation.cfm?id=1102351&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>
</span>
<div class="small-text" style="margin-top:10px; margin-bottom:5px;">
<br />
<a href="#abstract" title="Abstract" style="padding:5px"><span>Abstract</span></a> |
<a href="#formats" title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
<a href="#authors" title="Authors" style="padding:5px"><span>Authors</span></a> |
<a href="#references" title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
<a href="#citedby" title="Cited By" style="padding:5px"><span style='color:#999999'>Cited By</span></a> |
<a href="#indexterms" title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
<a href="#source" title="Publication" style="padding:5px"><span>Publication</span></a> |
<a href="#revs" title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |
<a href="#comments" title="Comments" style="padding:5px"><span>Comments</span></a>
|
<a href="#prox" title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
</div>
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;" />
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div style="display:inline">
<p>This volume, which is also available online from
http://www.machinelearning.org, contains the papers accepted for
presentation at ICML-2005, the 22nd lnternational Conference on
Machine Learning, which was held at the University of Bonn in
Germany from August 7 to August 11, 2005. ICML is the annual
conference of the lnternational Machine Learning Society (IMLS),
and forms an international forum for the discussion and
presentation of the latest results in the field of machine
learning. This year, ICML was co-located with the 15th
lnternational Conference on Inductive Logic Programming (ILP-2005),
the proceedings of which are published by Springer Verlag in a
separate volume.</p>
<p>The papers in this volume were selected on the basis of a
thorough review process. In the first round of reviewing, three
program committee members produced individual reviews for a paper.
Authors then had the opportunity to view those reviews and submit
an author's reply to the reviewers. Led by the responsible area
chair, the reviewers then engaged in a discussion about the paper,
ultimately leading to the decision by the program chairs. In sum,
of the 491 papers that were initially submitted, 62 were accepted
immediately, and a further 81 were conditionally accepted and
reconsidered after resubmission in a second round of reviewing. Of
those 81 conditionally accepted papers, 72 were finally accepted,
leading to a total of 134 accepted papers, which translates into an
acceptance rate of 27.3 %. The author reply was a new feature of
ICML this year, while the option of working with conditional
accepts has already become a tradition.</p>
<p>In addition to the presentations of the accepted papers, the
ICML program included several other features. On the first and last
day of the conference, 11 workshops and 6 tutorials on current
topics of machine learning were held. For many of these,
proceedings and/or presentation materials are available online from
the ICML website. The other days of the conference each featured an
invited talk by a prominent researcher as a program highlight. We
were delighted that Johannes Gehrke of Cornell University, Michael
Jordan of the University of California at Berkeley, and Gerhard
Widmer of the University of Linz in Austria, agreed to deliver an
invited talk. The abstracts of their talks are also published as
part of these proceedings.</p>
<p>Continuing a long standing tradition at ICML, all papers
presented in a talk at the conference were also exhibited at
evening poster sessions, giving everyone ample time to discuss the
results in depth. In order to emphasize the co-location with
ILP-2005, the program contained joint elements in both invited
speakers, paper sessions, poster sessions, and tutorials. As usual,
the scientific program was complemented by a social program, this
time featuring an excursion to the scenic surroundings of the city
of Bonn.</p>
<p>During the conference best paper and best student paper awards
were presented, the former being sponsored by NICTA, the later by
the Machine Learning Journal.</p>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div class="abstract">
<SPAN style="font-weight:bold">FRONT MATTER</span>
</div>
<div style="margin-left:10px; line-height:180%;">
<A NAME="FullText" HREF="https://portalparts.acm.org/1110000/1102351/fm/frontmatter.pdf?ip=173.16.22.104" title="PDF" target="_blank">
<img src="imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
&nbsp;Front matter (TOC, Preface, ICML 2005 organization, Program committee, External reviewers, Board of the international machine learning society 2005, Sponsors, Invited talks, Workshop and tutorial summaries)
</div>
<div style="margin-top: 10px;" class="abstract">
<SPAN style="font-weight:bold">BACK MATTER</span>
</div>
<div style="margin-left:10px; line-height:180%;">
<A NAME="FullText" HREF="https://portalparts.acm.org/1110000/1102351/bm/backmatter.pdf?ip=173.16.22.104" title="PDF" target="_blank">
<img src="imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
&nbsp;Back matter (Title index, Author index)
</div>
<div style="margin-top: 10px; height: auto; padding: 5px; ">
<div style="margin-top:20px;" class="abstract">
<SPAN style="font-weight:bold">APPEARS IN</span>
</div>
<div>
<a href="/icps.cfm" title="ICPS"><img src="images/ACM_ICPS.jpg" alt="ICPS" style="padding-right:10px; vertical-align:middle" border="0" /></a> ICPS: <a href="/icps.cfm" title="ICPS" target="_blank">ACM International Conference Proceeding Series</a>
</div>
</div>
<br clear="all" />
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<dl title="Authors" style="margin-top:0px">
<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
<strong>
General Chairs
</strong>
</dt>
<dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of Saso Dzeroski" href="author_page.cfm?id=81100529929">Saso Dzeroski</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1991-2017</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">143</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">1,867</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">14</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">32</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">154</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">7,430</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">530.71</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">13.06</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of Saso Dzeroski" href="author_page.cfm?id=81100529929&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of Saso Dzeroski
</td>
</tr>
</table>
</span>
</dd>
<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
<strong>
Program Chairs
</strong>
</dt>
<dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of Luc De Raedt" href="author_page.cfm?id=81100274695">Luc De Raedt</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1987-2017</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">137</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">1,844</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">10</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">22</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">116</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">6,200</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">620.00</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">13.46</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
 <a title="colleagues of Luc De Raedt" href="author_page.cfm?id=81100274695&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of Luc De Raedt
</td>
</tr>
</table>
</span>
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of Stefan Wrobel" href="author_page.cfm?id=81100099179">Stefan Wrobel</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1987-2016</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">84</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">1,007</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">12</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">46</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">347</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">7,618</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">634.83</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">11.99</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of Stefan Wrobel" href="author_page.cfm?id=81100099179&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of Stefan Wrobel
</td>
</tr>
</table>
</span>
</dd>
</dl>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
References are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
Citings are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 0px;" class="flatbody">
Index Terms are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<table border="0" class="medium-text" cellpadding="5" cellspacing="5">
<tr valign="top">
<td style="padding: 10px;">Title</td>
<td style="padding: 10px;">ICML '05 Proceedings of the 22nd international conference on Machine learning
<a href="citation.cfm?id=1102351&picked=prox" target="_self" class="small-link-text">table&nbsp;of&nbsp;contents</a>
</td>
</tr>
<tr><td style="padding: 10px;">Pages</td><td style="padding: 10px;">1113</td></tr>
<tr><td style="padding: 10px;">Publisher</td><td style="padding: 10px;"><a href="https://www.acm.org/publications">ACM</a> New York, NY, USA &copy;2005
</td>
</tr>
<tr><td style="padding: 10px;">ISBN</td><td style="padding: 10px;">1-59593-180-5 </td></tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
</td>
</tr>
<tr valign="top">
<td>Conference</td>
<td valign="top" align="left" style="padding-bottom: 25px;">
<strong>ICML</strong><a href="event.cfm?id=RE548" title="International Conference on Machine Learning">International Conference on Machine Learning</a>
</td>
</tr>
<tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 448 of 1,653 submissions, 27%</td></tr>
<tr valign="top">
<td colspan="2" style="padding-left:25px;">
<table>
<tr><td style="padding: 10px;">
<img border="0" class="chart" id="6237068920132190-img" src="/CFFileServlet/_cf_chart/6237068920132190.jpg" usemap="#6237068920132190-map" />
<div id="6237068920132190-tooltip" style="position:fixed;display:none;"></div>
<map id="6237068920132190-map" name="6237068920132190-map">
<area style="cursor:auto" shape="rect" id="6237068920132190-graph-id0-plotset-plot-0-node-0" coords="37,24,57,214" />
<area style="cursor:auto" shape="rect" id="6237068920132190-graph-id0-plotset-plot-0-node-1" coords="95,33,115,214" />
<area style="cursor:auto" shape="rect" id="6237068920132190-graph-id0-plotset-plot-0-node-2" coords="152,12,173,214" />
<area style="cursor:auto" shape="rect" id="6237068920132190-graph-id0-plotset-plot-1-node-0" coords="64,167,85,214" />
<area style="cursor:auto" shape="rect" id="6237068920132190-graph-id0-plotset-plot-1-node-1" coords="122,164,142,214" />
<area style="cursor:auto" shape="rect" id="6237068920132190-graph-id0-plotset-plot-1-node-2" coords="180,161,200,214" />
</map>
<script data-cfasync="false" src="/cdn-cgi/scripts/f2bf09f8/cloudflare-static/email-decode.min.js"></script><script>
if (!CFCHART) {var CFCHART={};};if (!CFCHART.nodes) {CFCHART.nodes={};}
CFCHART.nodes["6237068920132190"]={};
CFCHART.nodes["6237068920132190"]["6237068920132190-graph-id0-plotset-plot-0-node-0"]={text:"548",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["6237068920132190"]["6237068920132190-graph-id0-plotset-plot-0-node-1"]={text:"522",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["6237068920132190"]["6237068920132190-graph-id0-plotset-plot-0-node-2"]={text:"583",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["6237068920132190"]["6237068920132190-graph-id0-plotset-plot-1-node-0"]={text:"140",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["6237068920132190"]["6237068920132190-graph-id0-plotset-plot-1-node-1"]={text:"150",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["6237068920132190"]["6237068920132190-graph-id0-plotset-plot-1-node-2"]={text:"158",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
</script>
</td>
<td style="padding-left:20px;">
<table style="border-width: 1px; border-style: solid; width:100%; border-spacing: 6px;" class="text12">
<tr bgcolor="#ffffff">
 <th style="width:50%">Year</th>
<th align="right" style="width:15%;">Submitted</th>
<th align="right" style="width:15%">Accepted</th>
<th align="center">Rate</th>
</tr>
<tr bgcolor="#f0f0f0">
<td style="padding: 10px;">ICML '06</td>
<td align="right">548</td>
<td align="right">140</td>
<td align="center">26%</td>
</tr>
<tr bgcolor="#ffffff">
<td style="padding: 10px;">ICML '07</td>
<td align="right">522</td>
<td align="right">150</td>
<td align="center">29%</td>
</tr>
<tr bgcolor="#f0f0f0">
<td style="padding: 10px;">ICML '08</td>
<td align="right">583</td>
<td align="right">158</td>
<td align="center">27%</td>
</tr>
<tr bgcolor="#ffffff">
<td style="padding: 10px;"><strong>Overall</strong></td>
<td align="right">1,653</td>
<td align="right">448</td>
<td align="center">27%</td>
</tr>
</table>
</td>
</tr>
</table>
</td>
</tr>
</table>
<br />
<div class="abstract" style="margin-bottom:10px;">
<SPAN><strong>APPEARS IN</strong></span>
</div>
<div>
<a href="/icps.cfm" title="ICPS"><img src="images/ACM_ICPS.jpg" alt="ICPS" style="padding-right:10px; vertical-align:middle" border="0" /></a> ICPS: <a href="/icps.cfm" title="ICPS" target="_blank">ACM International Conference Proceeding Series</a>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<br />Reviews are not available for this item
<div align="left" style="margin-top:30px">
<a title="Computing Reviews" href="ocr_review_main.cfm">
<img src="images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
<ul style="list-style:disc; display:inline-block">
<li>Access <a href="ocr_review_main.cfm" target="_blank">critical reviews</a> of computing literature.</li>
<li><a href="http://www.computingreviews.com/Reviewer/" target="_blank">Become a reviewer</a> for Computing Reviews</li>
</ul>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div>
<div>
<p style="margin-left:5px;">
<strong>Be the first to comment</strong>
To Post a comment please <a href="signin.cfm">sign in or create</a> a free Web account</a>
</p>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;">
<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the 22nd international conference on Machine learning</h5>
<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>
<div style="clear:both">
<div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1015330&picked=prox" title="previous: ICML '04"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><a href="citation.cfm?id=1143844&picked=prox" title="Next: ICML '06">next proceeding <img align="absmiddle" hspace="5" border="0" src="img/next.gif" width="19" height="11" alt="next" /></a></div>
</div>
<table class="text12" border="0">
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102352">Exploration and apprenticeship learning in reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309498920">Pieter Abbeel</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1 - 8</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102352" title="DOI">10.1145/1102351.1102352</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102352&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow1" style="display:inline;"><br /><div style="display:inline">We consider reinforcement learning in systems with unknown dynamics. Algorithms such as E3 (Kearns and Singh, 2002) learn near-optimal policies by using "exploration policies" to drive the system towards poorly modeled states, so as ...</div></span>
<span id="toHide1" style="display:none;"><br /><div style="display:inline">We consider reinforcement learning in systems with unknown dynamics. Algorithms such as <i>E</i><sup>3</sup> (Kearns and Singh, 2002) learn near-optimal policies by using "exploration policies" to drive the system towards poorly modeled states, so as to encourage exploration. But this makes these algorithms impractical for many systems; for example, on an autonomous helicopter, overly aggressive exploration may well result in a crash. In this paper, we consider the apprenticeship learning setting in which a teacher demonstration of the task is available. We show that, given the initial demonstration, no explicit exploration is necessary, and we can attain near-optimal performance (compared to the teacher) simply by repeatedly executing "exploitation policies" that try to maximize rewards. In finite-state MDPs, our algorithm scales polynomially in the number of states; in continuous-state linear dynamical systems, it scales polynomially in the dimension of the state. These results are proved using a martingale construction over relative losses.</div></span> <a id="expcoll1" href="JavaScript: expandcollapse('expcoll1',1)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102353">Active learning for Hidden Markov Models: objective functions and algorithms</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100101846">Brigham Anderson</a>,
<a href="author_page.cfm?id=81309511927">Andrew Moore</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 9 - 16</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102353" title="DOI">10.1145/1102351.1102353</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102353&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow2" style="display:inline;"><br /><div style="display:inline">Hidden Markov Models (HMMs) model sequential data in many fields such as text/speech processing and biosignal analysis. Active learning algorithms learn faster and/or better by closing the data-gathering loop, i.e., they choose the examples most informative ...</div></span>
<span id="toHide2" style="display:none;"><br /><div style="display:inline">Hidden Markov Models (HMMs) model sequential data in many fields such as text/speech processing and biosignal analysis. Active learning algorithms learn faster and/or better by closing the data-gathering loop, i.e., they choose the examples most informative with respect to their learning objectives. We introduce a framework and objective functions for active learning in three fundamental HMM problems: model learning, state estimation, and path estimation. In addition, we describe a new set of algorithms for efficiently finding optimal greedy queries using these objective functions. The algorithms are fast, i.e., linear in the number of time steps to select the optimal query and we present empirical results showing that these algorithms can significantly reduce the need for labelled training data.</div></span> <a id="expcoll2" href="JavaScript: expandcollapse('expcoll2',2)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102354">Tempering for Bayesian C&amp;RT</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100394175">Nicos Angelopoulos</a>,
<a href="author_page.cfm?id=81100414828">James Cussens</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 17 - 24</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102354" title="DOI">10.1145/1102351.1102354</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102354&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow3" style="display:inline;"><br /><div style="display:inline">This paper concerns the experimental assessment of tempering as a technique for improving Bayesian inference for C&RT models. Full Bayesian inference requires the computation of a posterior over all possible trees. Since exact computation is not ...</div></span>
<span id="toHide3" style="display:none;"><br /><div style="display:inline">This paper concerns the experimental assessment of <i>tempering</i> as a technique for improving Bayesian inference for C&RT models. Full Bayesian inference requires the computation of a posterior over all possible trees. Since exact computation is not possible Markov chain Monte Carlo (MCMC) methods are used to produce an approximation. C&RT posteriors have many local modes: tempering aims to prevent the Markov chain getting stuck in these modes. Our results show that a clear improvement is achieved using tempering.</div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102355">Fast condensed nearest neighbor rule</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100400419">Fabrizio Angiulli</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 25 - 32</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102355" title="DOI">10.1145/1102351.1102355</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102355&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow4" style="display:inline;"><br /><div style="display:inline">We present a novel algorithm for computing a training set consistent subset for the nearest neighbor decision rule. The algorithm, called FCNN rule, has some desirable properties. Indeed, it is order independent, and has subquadratic worst case time ...</div></span>
<span id="toHide4" style="display:none;"><br /><div style="display:inline">We present a novel algorithm for computing a training set consistent subset for the nearest neighbor decision rule. The algorithm, called FCNN rule, has some desirable properties. Indeed, it is order independent, and has subquadratic worst case time complexity, while it requires few iterations to converge, and it is likely to select points very close to the decision boundary. We compare the FCNN rule with state of the art competence preservation algorithms on large multidimensional training sets, showing that it outperforms existing methods in terms of learning speed and learning scaling behavior, and in terms of size of the model, while it guarantees a comparable prediction accuracy.</div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102356">Predictive low-rank decomposition for kernel methods</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100328355">Francis R. Bach</a>,
<a href="author_page.cfm?id=81339507945">Michael I. Jordan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 33 - 40</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102356" title="DOI">10.1145/1102351.1102356</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102356&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow5" style="display:inline;"><br /><div style="display:inline">Low-rank matrix decompositions are essential tools in the application of kernel methods to large-scale learning problems. These decompositions have generally been treated as black boxes---the decomposition of the kernel matrix that they deliver is independent ...</div></span>
<span id="toHide5" style="display:none;"><br /><div style="display:inline">Low-rank matrix decompositions are essential tools in the application of kernel methods to large-scale learning problems. These decompositions have generally been treated as black boxes---the decomposition of the kernel matrix that they deliver is independent of the specific learning task at hand---and this is a potentially significant source of inefficiency. In this paper, we present an algorithm that can exploit side information (e.g., classification labels, regression responses) in the computation of low-rank decompositions for kernel matrices. Our algorithm has the same favorable scaling as state-of-the-art methods such as incomplete Cholesky decomposition---it is linear in the number of data points and quadratic in the rank of the approximation. We present simulation results that show that our algorithm yields decompositions of significantly smaller rank than those found by incomplete Cholesky decomposition.</div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102357">Multi-way distributional clustering via pairwise interactions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100647922">Ron Bekkerman</a>,
<a href="author_page.cfm?id=81100335576">Ran El-Yaniv</a>,
<a href="author_page.cfm?id=81100553872">Andrew McCallum</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 41 - 48</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102357" title="DOI">10.1145/1102351.1102357</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102357&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow6" style="display:inline;"><br /><div style="display:inline">We present a novel unsupervised learning scheme that simultaneously clusters variables of several types (e.g., documents, words and authors) based on pairwise interactions between the types, as observed in co-occurrence data. In this scheme, multiple ...</div></span>
<span id="toHide6" style="display:none;"><br /><div style="display:inline">We present a novel unsupervised learning scheme that simultaneously clusters variables of several types (e.g., documents, words and authors) based on pairwise interactions between the types, as observed in co-occurrence data. In this scheme, multiple clustering systems are generated aiming at maximizing an objective function that measures multiple pairwise mutual information between cluster variables. To implement this idea, we propose an algorithm that interleaves top-down clustering of some variables and bottom-up clustering of the other variables, with a local optimization correction routine. Focusing on document clustering we present an extensive empirical study of two-way, three-way and four-way applications of our scheme using six real-world datasets including the 20 News-groups (20NG) and the Enron email collection. Our multi-way distributional clustering (MDC) algorithms consistently and significantly outperform previous state-of-the-art information theoretic clustering algorithms.</div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102358">Error limiting reductions between classification tasks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100514603">Alina Beygelzimer</a>,
<a href="author_page.cfm?id=81375596978">Varsha Dani</a>,
<a href="author_page.cfm?id=81100592496">Tom Hayes</a>,
<a href="author_page.cfm?id=81100453722">John Langford</a>,
<a href="author_page.cfm?id=81100278691">Bianca Zadrozny</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 49 - 56</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102358" title="DOI">10.1145/1102351.1102358</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102358&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow7" style="display:inline;"><br /><div style="display:inline">We introduce a reduction-based model for analyzing supervised learning tasks. We use this model to devise a new reduction from multi-class cost-sensitive classification to binary classification with the following guarantee: If the learned binary classifier ...</div></span>
<span id="toHide7" style="display:none;"><br /><div style="display:inline">We introduce a reduction-based model for analyzing supervised learning tasks. We use this model to devise a new reduction from multi-class cost-sensitive classification to binary classification with the following guarantee: If the learned binary classifier has error rate at most &epsilon; then the cost-sensitive classifier has cost at most 2&epsilon; times the expected sum of costs of all possible lables. Since cost-sensitive classification can embed any bounded loss finite choice supervised learning task, this result shows that <i>any</i> such task can be solved using a binary classification oracle. Finally, we present experimental results showing that our new reduction outperforms existing algorithms for multi-class cost-sensitive learning.</div></span> <a id="expcoll7" href="JavaScript: expandcollapse('expcoll7',7)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102359">Multi-instance tree learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100620334">Hendrik Blockeel</a>,
<a href="author_page.cfm?id=81405593471">David Page</a>,
<a href="author_page.cfm?id=81100407824">Ashwin Srinivasan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 57 - 64</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102359" title="DOI">10.1145/1102351.1102359</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102359&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow8" style="display:inline;"><br /><div style="display:inline">We introduce a novel algorithm for decision tree learning in the multi-instance setting as originally defined by Dietterich et al. It differs from existing multi-instance tree learners in a few crucial, well-motivated details. Experiments on synthetic ...</div></span>
<span id="toHide8" style="display:none;"><br /><div style="display:inline">We introduce a novel algorithm for decision tree learning in the multi-instance setting as originally defined by Dietterich et al. It differs from existing multi-instance tree learners in a few crucial, well-motivated details. Experiments on synthetic and real-life datasets confirm the beneficial effect of these differences and show that the resulting system outperforms the existing multi-instance decision tree learners.</div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102360">Action respecting embedding</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100395285">Michael Bowling</a>,
<a href="author_page.cfm?id=81545241356">Ali Ghodsi</a>,
<a href="author_page.cfm?id=81100304191">Dana Wilkinson</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 65 - 72</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102360" title="DOI">10.1145/1102351.1102360</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102360&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow9" style="display:inline;"><br /><div style="display:inline">Dimensionality reduction is the problem of finding a low-dimensional representation of high-dimensional input data. This paper examines the case where additional information is known about the data. In particular, we assume the data are given in a sequence ...</div></span>
<span id="toHide9" style="display:none;"><br /><div style="display:inline">Dimensionality reduction is the problem of finding a low-dimensional representation of high-dimensional input data. This paper examines the case where additional information is known about the data. In particular, we assume the data are given in a sequence with action labels associated with adjacent data points, such as might come from a mobile robot. The goal is a variation on dimensionality reduction, where the output should be a representation of the input data that is both low-dimensional and respects the actions (<i>i.e.</i>, actions correspond to simple transformations in the output representation). We show how this variation can be solved with a semidefinite program. We evaluate the technique in a synthetic, robot-inspired domain, demonstrating qualitatively superior representations and quantitative improvements on a data prediction task.</div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102361">Clustering through ranking on manifolds</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309496478">Markus Breitenbach</a>,
<a href="author_page.cfm?id=81100364373">Gregory Z. Grudic</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 73 - 80</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102361" title="DOI">10.1145/1102351.1102361</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102361&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow10" style="display:inline;"><br /><div style="display:inline">Clustering aims to find useful hidden structures in data. In this paper we present a new clustering algorithm that builds upon the consistency method (Zhou, et.al., 2003), a semi-supervised learning technique with the property of learning very ...</div></span>
<span id="toHide10" style="display:none;"><br /><div style="display:inline">Clustering aims to find useful hidden structures in data. In this paper we present a new clustering algorithm that builds upon the consistency method (Zhou, et.<i>al.</i>, 2003), a semi-supervised learning technique with the property of learning very smooth functions with respect to the intrinsic structure revealed by the data. Other methods, e.g. Spectral Clustering, obtain good results on data that reveals such a structure. However, <i>unlike</i> Spectral Clustering, our algorithm effectively detects both global and within-class outliers, and the most representative examples in each class. Furthermore, we specify an optimization framework that estimates all learning parameters, including the number of clusters, directly from data. Finally, we show that the learned cluster-models can be used to add previously unseen points to clusters without re-learning the original cluster model. Encouraging experimental results are obtained on a number of real world problems.</div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102362">Reducing overfitting in process model induction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323488296">Will Bridewell</a>,
<a href="author_page.cfm?id=81309507243">Narges Bani Asadi</a>,
<a href="author_page.cfm?id=81100101009">Pat Langley</a>,
<a href="author_page.cfm?id=81339532626">Ljup&#269;o Todorovski</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 81 - 88</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102362" title="DOI">10.1145/1102351.1102362</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102362&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow11" style="display:inline;"><br /><div style="display:inline">In this paper, we review the paradigm of inductive process modeling, which uses background knowledge about possible component processes to construct quantitative models of dynamical systems. We note that previous methods for this task tend to overfit ...</div></span>
<span id="toHide11" style="display:none;"><br /><div style="display:inline">In this paper, we review the paradigm of inductive process modeling, which uses background knowledge about possible component processes to construct quantitative models of dynamical systems. We note that previous methods for this task tend to overfit the training data, which suggests ensemble learning as a likely response. However, such techniques combine models in ways that reduce comprehensibility, making their output much less accessible to domain scientists. As an alternative, we introduce a new approach that induces a set of process models from different samples of the training data and uses them to guide a final search through the space of model structures. Experiments with synthetic and natural data suggest this method reduces error and decreases the chance of including unnecessary processes in the model. We conclude by discussing related work and suggesting directions for additional research.</div></span> <a id="expcoll11" href="JavaScript: expandcollapse('expcoll11',11)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102363">Learning to rank using gradient descent</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100307710">Chris Burges</a>,
<a href="author_page.cfm?id=81100651571">Tal Shaked</a>,
 <a href="author_page.cfm?id=81100303423">Erin Renshaw</a>,
<a href="author_page.cfm?id=81309501604">Ari Lazier</a>,
<a href="author_page.cfm?id=81309503524">Matt Deeds</a>,
<a href="author_page.cfm?id=81309497225">Nicole Hamilton</a>,
<a href="author_page.cfm?id=81309501229">Greg Hullender</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 89 - 96</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102363" title="DOI">10.1145/1102351.1102363</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102363&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow12" style="display:inline;"><br /><div style="display:inline">We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. ...</div></span>
<span id="toHide12" style="display:none;"><br /><div style="display:inline">We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.</div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102364">Learning class-discriminative dynamic Bayesian networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81324488365">John Burge</a>,
<a href="author_page.cfm?id=81100001124">Terran Lane</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 97 - 104</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102364" title="DOI">10.1145/1102351.1102364</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102364&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow13" style="display:inline;"><br /><div style="display:inline">In many domains, a Bayesian network's topological structure is not known a priori and must be inferred from data. This requires a scoring function to measure how well a proposed network topology describes a set of data. Many commonly used scores such ...</div></span>
<span id="toHide13" style="display:none;"><br /><div style="display:inline">In many domains, a Bayesian network's topological structure is not known a priori and must be inferred from data. This requires a scoring function to measure how well a proposed network topology describes a set of data. Many commonly used scores such as BD, BDE, BDEU, etc., are not well suited for class discrimination. Instead, scores such as the class-conditional likelihood (CCL) should be employed. Unfortunately, CCL does not decompose and its application to large domains is not feasible. We introduce a decomposable score, <i>approximate conditional likelihood</i> (ACL) that is capable of identifying class discriminative structures. We show that dynamic Bayesian networks (DBNs) trained with ACL have classification efficacies competitive to those trained with CCL on a set of simulated data experiments. We also show that ACL-trained DBNs outperform BDE-trained DBNs, Gaussian na&iuml;ve Bayes networks and support vector machines within a neuroscience domain too large for CCL.</div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102365">Recognition and reproduction of gestures using a probabilistic framework combining PCA, ICA and HMM</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309487873">Sylvain Calinon</a>,
<a href="author_page.cfm?id=81100342762">Aude Billard</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 105 - 112</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102365" title="DOI">10.1145/1102351.1102365</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102365&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow14" style="display:inline;"><br /><div style="display:inline">This paper explores the issue of recognizing, generalizing and reproducing arbitrary gestures. We aim at extracting a representation that encapsulates only the key aspects of the gesture and discards the variability intrinsic to each person's motion. ...</div></span>
<span id="toHide14" style="display:none;"><br /><div style="display:inline">This paper explores the issue of recognizing, generalizing and reproducing arbitrary gestures. We aim at extracting a representation that encapsulates only the key aspects of the gesture and discards the variability intrinsic to each person's motion. We compare a decomposition into principal components (PCA) and independent components (ICA) as a first step of preprocessing in order to decorrelate and denoise the data, as well as to reduce the dimensionality of the dataset to make this one tractable. In a second stage of processing, we explore the use of a probabilistic encoding through continuous Hidden Markov Models (HMMs), as a way to encapsulate the sequential nature and intrinsic variability of the motions in stochastic finite state automata. Finally, the method is validated in a humanoid robot to reproduce a variety of gestures performed by a human demonstrator.</div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102366">Predicting probability distributions for surf height using an ensemble of mixture density networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309513279">Michael Carney</a>,
<a href="author_page.cfm?id=81100562954">P&#225;draig Cunningham</a>,
<a href="author_page.cfm?id=81100518315">Jim Dowling</a>,
<a href="author_page.cfm?id=81309506992">Ciaran Lee</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 113 - 120</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102366" title="DOI">10.1145/1102351.1102366</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102366&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow15" style="display:inline;"><br /><div style="display:inline">There is a range of potential applications of Machine Learning where it would be more useful to predict the probability distribution for a variable rather than simply the most likely value for that variable. In meteorology and in finance it is often ...</div></span>
<span id="toHide15" style="display:none;"><br /><div style="display:inline">There is a range of potential applications of Machine Learning where it would be more useful to predict the probability distribution for a variable rather than simply the most likely value for that variable. In meteorology and in finance it is often important to know the probability of a variable falling within (or outside) different ranges. In this paper we consider the prediction of surf height with the objective of predicting if it will fall within a given 'surfable' range. Prediction problems such as this are considerably more difficult if the distribution of the phenomenon is significantly different from a normal distribution. This is the case with the surf data we have studied. To address this we use an ensemble of mixture density networks to predict the probability density function. Our evaluation shows that this is an effective solution. We also describe a web-based application that presents these predictions in a usable manner.</div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102367">Hedged learning: regret-minimization with learning experts</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100248526">Yu-Han Chang</a>,
<a href="author_page.cfm?id=81100227632">Leslie Pack Kaelbling</a>
</span>
</td> 
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 121 - 128</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102367" title="DOI">10.1145/1102351.1102367</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102367&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow16" style="display:inline;"><br /><div style="display:inline">In non-cooperative multi-agent situations, there cannot exist a globally optimal, yet opponent-independent learning algorithm. Regret-minimization over a set of strategies optimized for potential opponent models is proposed as a good framework for deciding ...</div></span>
<span id="toHide16" style="display:none;"><br /><div style="display:inline">In non-cooperative multi-agent situations, there cannot exist a globally optimal, yet opponent-independent learning algorithm. Regret-minimization over a set of strategies optimized for potential opponent models is proposed as a good framework for deciding how to behave in such situations. Using longer playing horizons and experts that learn as they play, the regret-minimization framework can be extended to overcome several shortcomings of earlier approaches to the problem of multi-agent learning.</div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102368">Variational Bayesian image modelling</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100098097">Li Cheng</a>,
<a href="author_page.cfm?id=81541339156">Feng Jiao</a>,
<a href="author_page.cfm?id=81100182569">Dale Schuurmans</a>,
<a href="author_page.cfm?id=81309494079">Shaojun Wang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 129 - 136</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102368" title="DOI">10.1145/1102351.1102368</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102368&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow17" style="display:inline;"><br /><div style="display:inline">We present a variational Bayesian framework for performing inference, density estimation and model selection in a special class of graphical models---Hidden Markov Random Fields (HMRFs). HMRFs are particularly well suited to image modelling and in this ...</div></span>
 <span id="toHide17" style="display:none;"><br /><div style="display:inline">We present a variational Bayesian framework for performing inference, density estimation and model selection in a special class of graphical models---Hidden Markov Random Fields (HMRFs). HMRFs are particularly well suited to image modelling and in this paper, we apply them to the problem of image segmentation. Unfortunately, HMRFs are notoriously hard to train and use because the exact inference problems they create are intractable. Our main contribution is to introduce an efficient variational approach for performing approximate inference of the Bayesian formulation of HMRFs, which we can then apply to the density estimation and model selection problems that arise when learning image models from data. With this variational approach, we can conveniently tackle the problem of image segmentation. We present experimental results which show that our technique outperforms recent HMRF-based segmentation methods on real world images.</div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102369">Preference learning with Gaussian processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100142532">Wei Chu</a>,
<a href="author_page.cfm?id=81100572858">Zoubin Ghahramani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 137 - 144</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102369" title="DOI">10.1145/1102351.1102369</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102369&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow18" style="display:inline;"><br /><div style="display:inline">In this paper, we propose a probabilistic kernel approach to preference learning based on Gaussian processes. A new likelihood function is proposed to capture the preference relations in the Bayesian framework. The generalized formulation is also applicable ...</div></span>
<span id="toHide18" style="display:none;"><br /><div style="display:inline">In this paper, we propose a probabilistic kernel approach to preference learning based on Gaussian processes. A new likelihood function is proposed to capture the preference relations in the Bayesian framework. The generalized formulation is also applicable to tackle many multiclass problems. The overall approach has the advantages of Bayesian methods for model selection and probabilistic prediction. Experimental results compared against the constraint classification approach on several benchmark datasets verify the usefulness of this algorithm.</div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102370">New approaches to support vector ordinal regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
 <span style="padding-left:0">
<a href="author_page.cfm?id=81100142532">Wei Chu</a>,
<a href="author_page.cfm?id=81100176197">S. Sathiya Keerthi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 145 - 152</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102370" title="DOI">10.1145/1102351.1102370</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102370&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow19" style="display:inline;"><br /><div style="display:inline">In this paper, we propose two new support vector approaches for ordinal regression, which optimize multiple thresholds to define parallel discriminant hyperplanes for the ordinal scales. Both approaches guarantee that the thresholds are properly ordered ...</div></span>
<span id="toHide19" style="display:none;"><br /><div style="display:inline">In this paper, we propose two new support vector approaches for ordinal regression, which optimize multiple thresholds to define parallel discriminant hyperplanes for the ordinal scales. <i>Both approaches guarantee that the thresholds are properly ordered at the optimal solution.</i> The size of these optimization problems is linear in the number of training samples. The SMO algorithm is adapted for the resulting optimization problems; it is extremely easy to implement and scales efficiently as a quadratic function of the number of examples. The results of numerical experiments on benchmark datasets verify the usefulness of these approaches.</div></span> <a id="expcoll19" href="JavaScript: expandcollapse('expcoll19',19)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102371">A general regression technique for learning transductions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81332494270">Corinna Cortes</a>,
<a href="author_page.cfm?id=81100197439">Mehryar Mohri</a>,
<a href="author_page.cfm?id=81100015405">Jason Weston</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 153 - 160</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102371" title="DOI">10.1145/1102351.1102371</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102371&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow20" style="display:inline;"><br /><div style="display:inline">The problem of learning a transduction, that is a string-to-string mapping, is a common problem arising in natural language processing and computational biology. Previous methods proposed for learning such mappings are based on classification ...</div></span>
<span id="toHide20" style="display:none;"><br /><div style="display:inline">The problem of learning a transduction, that is a string-to-string mapping, is a common problem arising in natural language processing and computational biology. Previous methods proposed for learning such mappings are based on <i>classification</i> techniques. This paper presents a new and general <i>regression</i> technique for learning transductions and reports the results of experiments showing its effectiveness. Our transduction learning consists of two phases: the estimation of a set of regression coefficients and the computation of the pre-image corresponding to this set of coefficients. A novel and conceptually cleaner formulation of kernel dependency estimation provides a simple framework for estimating the regression coefficients, and an efficient algorithm for computing the pre-image from the regression coefficients extends the applicability of kernel dependency estimation to output sequences. We report the results of a series of experiments illustrating the application of our regression technique for learning transductions.</div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102372">Learning to compete, compromise, and cooperate in repeated general-sum games</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309483016">Jacob W. Crandall</a>,
<a href="author_page.cfm?id=81350575550">Michael A. Goodrich</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 161 - 168</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102372" title="DOI">10.1145/1102351.1102372</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102372&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow21" style="display:inline;"><br /><div style="display:inline">Learning algorithms often obtain relatively low average payoffs in repeated general-sum games between other learning agents due to a focus on myopic best-response and one-shot Nash equilibrium (NE) strategies. A less myopic approach places focus on NEs ...</div></span>
<span id="toHide21" style="display:none;"><br /><div style="display:inline">Learning algorithms often obtain relatively low average payoffs in repeated general-sum games between other learning agents due to a focus on myopic best-response and one-shot Nash equilibrium (NE) strategies. A less myopic approach places focus on NEs of the repeated game, which suggests that (at the least) a learning agent should possess two properties. First, an agent should never learn to play a strategy that produces average payoffs less than the minimax value of the game. Second, an agent should learn to cooperate/compromise when beneficial. No learning algorithm from the literature is known to possess both of these properties. We present a reinforcement learning algorithm (M-Qubed) that provably satisfies the first property and empirically displays (in self play) the second property in a wide range of games.</div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102373">Learning as search optimization: approximate large margin methods for structured prediction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100635457">Hal Daum&#233;, III</a>,
<a href="author_page.cfm?id=81100459689">Daniel Marcu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 169 - 176</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102373" title="DOI">10.1145/1102351.1102373</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102373&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow22" style="display:inline;"><br /><div style="display:inline">Mappings to structured output spaces (strings, trees, partitions, etc.) are typically learned using extensions of classification algorithms to simple graphical structures (eg., linear chains) in which search and parameter estimation can be performed ...</div></span>
<span id="toHide22" style="display:none;"><br /><div style="display:inline">Mappings to structured output spaces (strings, trees, partitions, etc.) are typically learned using extensions of classification algorithms to simple graphical structures (eg., linear chains) in which search and parameter estimation can be performed exactly. Unfortunately, in many complex problems, it is rare that exact search or parameter estimation is tractable. Instead of learning exact models and searching via heuristic means, we embrace this difficulty and treat the structured output problem in terms of approximate search. We present a framework for learning as search optimization, and two parameter updates with convergence the-orems and bounds. Empirical evidence shows that our integrated approach to learning and decoding can outperform exact models at smaller computational cost.</div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102374">Multimodal oriented discriminant analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100338725">Fernando De la Torre</a>,
<a href="author_page.cfm?id=81100496698">Takeo Kanade</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 177 - 184</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102374" title="DOI">10.1145/1102351.1102374</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102374&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow23" style="display:inline;"><br /><div style="display:inline">Linear discriminant analysis (LDA) has been an active topic of research during the last century. However, the existing algorithms have several limitations when applied to visual data. LDA is only optimal for Gaussian distributed classes with equal covariance ...</div></span>
<span id="toHide23" style="display:none;"><br /><div style="display:inline">Linear discriminant analysis (LDA) has been an active topic of research during the last century. However, the existing algorithms have several limitations when applied to visual data. LDA is only optimal for Gaussian distributed classes with equal covariance matrices, and only classes-1 features can be extracted. On the other hand, LDA does not scale well to high dimensional data (overfitting), and it cannot handle optimally multimodal distributions. In this paper, we introduce Multimodal Oriented Discriminant Analysis (MODA), a LDA extension which can overcome these drawbacks. A new formulation and several novelties are proposed:&bull; An optimal dimensionality reduction for multimodal Gaussian classes with different covariances is derived. The new criteria allows for extracting more than classes-1 features.&bull; A covariance approximation is introduced to improve generalization and avoid over-fitting when dealing with high dimensional data.&bull; A linear time iterative majorization method is suggested in order to find a local optimum.Several synthetic and real experiments on face recognition show that MODA outperform existing linear techniques.</div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102375">A practical generalization of Fourier-based learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309486264">Adam Drake</a>,
<a href="author_page.cfm?id=81100151738">Dan Ventura</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 185 - 192</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102375" title="DOI">10.1145/1102351.1102375</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102375&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow24" style="display:inline;"><br /><div style="display:inline">This paper presents a search algorithm for finding functions that are highly correlated with an arbitrary set of data. The functions found by the search can be used to approximate the unknown function that generated the data. A special case of this approach ...</div></span>
<span id="toHide24" style="display:none;"><br /><div style="display:inline">This paper presents a search algorithm for finding functions that are highly correlated with an arbitrary set of data. The functions found by the search can be used to approximate the unknown function that generated the data. A special case of this approach is a method for learning Fourier representations. Empirical results demonstrate that on typical real-world problems the most highly correlated functions can be found very quickly, while combinations of these functions provide good approximations of the unknown function.</div></span> <a id="expcoll24" href="JavaScript: expandcollapse('expcoll24',24)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102376">Combining model-based and instance-based learning for first order regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100650504">Kurt Driessens</a>,
<a href="author_page.cfm?id=81100529929">Sa&#353;o D&#382;eroski</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 193 - 200</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102376" title="DOI">10.1145/1102351.1102376</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102376&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow25" style="display:inline;"><br /><div style="display:inline">The introduction of relational reinforcement learning and the RRL algorithm gave rise to the development of several first order regression algorithms. So far, these algorithms have employed either a model-based approach or an instance-based approach. ...</div></span>
<span id="toHide25" style="display:none;"><br /><div style="display:inline">The introduction of relational reinforcement learning and the RRL algorithm gave rise to the development of several first order regression algorithms. So far, these algorithms have employed either a model-based approach or an instance-based approach. As a consequence, they suffer from the typical drawbacks of model-based learning such as coarse function approximation or those of lazy learning such as high computational intensity.In this paper we develop a new regression algorithm that combines the strong points of both approaches and tries to avoid the normally inherent draw-backs. By combining model-based and instance-based learning, we produce an incremental first order regression algorithm that is both computationally efficient and produces better predictions earlier in the learning experiment.</div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102377">Reinforcement learning with Gaussian processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100183155">Yaakov Engel</a>,
<a href="author_page.cfm?id=81100515533">Shie Mannor</a>,
<a href="author_page.cfm?id=81410591739">Ron Meir</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 201 - 208</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102377" title="DOI">10.1145/1102351.1102377</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102377&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow26" style="display:inline;"><br /><div style="display:inline">Gaussian Process Temporal Difference (GPTD) learning offers a Bayesian solution to the policy evaluation problem of reinforcement learning. In this paper we extend the GPTD framework by addressing two pressing issues, which were not adequately treated ...</div></span>
<span id="toHide26" style="display:none;"><br /><div style="display:inline">Gaussian Process Temporal Difference (GPTD) learning offers a Bayesian solution to the policy evaluation problem of reinforcement learning. In this paper we extend the GPTD framework by addressing two pressing issues, which were not adequately treated in the original GPTD paper (Engel et al., 2003). The first is the issue of stochasticity in the state transitions, and the second is concerned with action selection and policy improvement. We present a new generative model for the value function, deduced from its relation with the discounted return. We derive a corresponding on-line algorithm for learning the posterior moments of the value Gaussian process. We also present a SARSA based extension of GPTD, termed GPSARSA, that allows the selection of actions and the gradual improvement of policies without requiring a world-model.</div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102378">Experimental comparison between bagging and Monte Carlo ensemble classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100065716">Roberto Esposito</a>,
<a href="author_page.cfm?id=81100356537">Lorenza Saitta</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 209 - 216</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102378" title="DOI">10.1145/1102351.1102378</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102378&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow27" style="display:inline;"><br /><div style="display:inline">Properties of ensemble classification can be studied using the framework of Monte Carlo stochastic algorithms. Within this framework it is also possible to define a new ensemble classifier, whose accuracy probability distribution can be computed exactly. ...</div></span>
<span id="toHide27" style="display:none;"><br /><div style="display:inline">Properties of ensemble classification can be studied using the framework of Monte Carlo stochastic algorithms. Within this framework it is also possible to define a new ensemble classifier, whose accuracy probability distribution can be computed exactly. This paper has two goals: first, an experimental comparison between the theoretical predictions and experimental results; second, a systematic comparison between bagging and Monte Carlo ensemble classification.</div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102379">Supervised clustering with support vector machines</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100568786">Thomas Finley</a>,
<a href="author_page.cfm?id=81100184551">Thorsten Joachims</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 217 - 224</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102379" title="DOI">10.1145/1102351.1102379</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102379&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow28" style="display:inline;"><br /><div style="display:inline">Supervised clustering is the problem of training a clustering algorithm to produce desirable clusterings: given sets of items and complete clusterings over these sets, we learn how to cluster future sets of items. Example applications include ...</div></span>
<span id="toHide28" style="display:none;"><br /><div style="display:inline"><i>Supervised clustering</i> is the problem of training a clustering algorithm to produce desirable clusterings: given sets of items and complete clusterings over these sets, we learn how to cluster future sets of items. Example applications include noun-phrase coreference clustering, and clustering news articles by whether they refer to the same topic. In this paper we present an SVM algorithm that trains a clustering algorithm by adapting the item-pair similarity measure. The algorithm may optimize a variety of different clustering functions to a variety of clustering performance measures. We empirically evaluate the algorithm for noun-phrase and news article clustering.</div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102380">Optimal assignment kernels for attributed molecular graphs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100162159">Holger Fr&#246;hlich</a>,
<a href="author_page.cfm?id=82659015957">J&#246;rg K. Wegner</a>,
<a href="author_page.cfm?id=81309490166">Florian Sieker</a>,
<a href="author_page.cfm?id=81100054451">Andreas Zell</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 225 - 232</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102380" title="DOI">10.1145/1102351.1102380</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102380&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow29" style="display:inline;"><br /><div style="display:inline">We propose a new kernel function for attributed molecular graphs, which is based on the idea of computing an optimal assignment from the atoms of one molecule to those of another one, including information on neighborhood, membership to a certain structural ...</div></span>
<span id="toHide29" style="display:none;"><br /><div style="display:inline">We propose a new kernel function for attributed molecular graphs, which is based on the idea of computing an optimal assignment from the atoms of one molecule to those of another one, including information on neighborhood, membership to a certain structural element and other characteristics for each atom. As a byproduct this leads to a new class of kernel functions. We demonstrate how the necessary computations can be carried out efficiently. Compared to marginalized graph kernels our method in some cases leads to a significant reduction of the prediction error. Further improvement can be gained, if expert knowledge is combined with our method. We also investigate a reduced graph representation of molecules by collapsing certain structural elements, like e.g. rings, into a single node of the molecular graph.</div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102381">Closed-form dual perturb and combine for tree-based models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100360352">Pierre Geurts</a>,
<a href="author_page.cfm?id=81100172542">Louis Wehenkel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 233 - 240</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102381" title="DOI">10.1145/1102351.1102381</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102381&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow30" style="display:inline;"><br /><div style="display:inline">This paper studies the aggregation of predictions made by tree-based models for several perturbed versions of the attribute vector of a test case. A closed-form approximation of this scheme combined with cross-validation to tune the level of perturbation ...</div></span>
<span id="toHide30" style="display:none;"><br /><div style="display:inline">This paper studies the aggregation of predictions made by tree-based models for several perturbed versions of the attribute vector of a test case. A closed-form approximation of this scheme combined with cross-validation to tune the level of perturbation is proposed. This yields soft-tree models in a parameter free way. and preserves their interpretability. Empirical evaluations, on classification and regression problems, show that accuracy and bias/variance tradeoff are improved significantly at the price of an acceptable computational overhead. The method is further compared and combined with tree bagging.</div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102382">Hierarchic Bayesian models for kernel learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100565907">Mark Girolami</a>,
<a href="author_page.cfm?id=81100173811">Simon Rogers</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 241 - 248</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102382" title="DOI">10.1145/1102351.1102382</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102382&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow31" style="display:inline;"><br /><div style="display:inline">The integration of diverse forms of informative data by learning an optimal combination of base kernels in classification or regression problems can provide enhanced performance when compared to that obtained from any single data source. We present a ...</div></span>
<span id="toHide31" style="display:none;"><br /><div style="display:inline">The integration of diverse forms of informative data by learning an optimal combination of base kernels in classification or regression problems can provide enhanced performance when compared to that obtained from any single data source. We present a Bayesian hierarchical model which enables kernel learning and present effective variational Bayes estimators for regression and classification. Illustrative experiments demonstrate the utility of the proposed method. Matlab code replicating results reported is available at http://www.dcs.gla.ac.uk/~srogers/kernel_comb.html.</div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102383">Online feature selection for pixel classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384616142">Karen Glocer</a>,
<a href="author_page.cfm?id=81309510420">Damian Eads</a>,
<a href="author_page.cfm?id=81100109862">James Theiler</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 249 - 256</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102383" title="DOI">10.1145/1102351.1102383</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102383&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow32" style="display:inline;"><br /><div style="display:inline">Online feature selection (OFS) provides an efficient way to sort through a large space of features, particularly in a scenario where the feature space is large and features take a significant amount of memory to store. Image processing operators, and ...</div></span>
<span id="toHide32" style="display:none;"><br /><div style="display:inline">Online feature selection (OFS) provides an efficient way to sort through a large space of features, particularly in a scenario where the feature space is large and features take a significant amount of memory to store. Image processing operators, and especially combinations of image processing operators, provide a rich space of potential features for use in machine learning for image processing tasks but they are expensive to generate and store. In this paper we apply OFS to the problem of edge detection in grayscale imagery. We use a standard data set and compare our results to those obtained with traditional edge detectors, as well as with results obtained more recently using "statistical edge detection." We compare several different OFS approaches, including hill climbing, best first search, and grafting.</div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102384">Learning strategies for story comprehension: a reinforcement learning approach</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309502399">Eugene Grois</a>,
<a href="author_page.cfm?id=81409593906">David C. Wilkins</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 257 - 264</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102384" title="DOI">10.1145/1102351.1102384</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102384&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow33" style="display:inline;"><br /><div style="display:inline">This paper describes the use of machine learning to improve the performance of natural language question answering systems. We present a model for improving story comprehension through inductive generalization and reinforcement learning, based on classified ...</div></span>
<span id="toHide33" style="display:none;"><br /><div style="display:inline">This paper describes the use of machine learning to improve the performance of natural language question answering systems. We present a model for improving story comprehension through inductive generalization and reinforcement learning, based on classified examples. In the process, the model selects the most relevant and useful pieces of lexical information to be used by the inference procedure. We compare our approach to three prior non-learning systems, and evaluate the conditions under which learning is effective. We demonstrate that a learning-based approach can improve upon "matching and extraction"-only techniques.</div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102385">Near-optimal sensor placements in Gaussian processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100629945">Carlos Guestrin</a>,
<a href="author_page.cfm?id=81100460663">Andreas Krause</a>,
<a href="author_page.cfm?id=81350580433">Ajit Paul Singh</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 265 - 272</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102385" title="DOI">10.1145/1102351.1102385</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102385&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow34" style="display:inline;"><br /><div style="display:inline">When monitoring spatial phenomena, which are often modeled as Gaussian Processes (GPs), choosing sensor locations is a fundamental task. A common strategy is to place sensors at the points of highest entropy (variance) in the GP model. We propose a mutual ...</div></span>
<span id="toHide34" style="display:none;"><br /><div style="display:inline">When monitoring spatial phenomena, which are often modeled as Gaussian Processes (GPs), choosing sensor locations is a fundamental task. A common strategy is to place sensors at the points of highest entropy (variance) in the GP model. We propose a <i>mutual information</i> criteria, and show that it produces better placements. Furthermore, we prove that finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1 -- 1/<i>e</i>) of the optimum by exploiting the <i>submodularity</i> of our criterion. This algorithm is extended to handle local structure in the GP, yielding significant speedups. We demonstrate the advantages of our approach on two real-world data sets.</div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102386">Robust one-class clustering using hybrid global and local search</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331493603">Gunjan Gupta</a>,
<a href="author_page.cfm?id=81100558602">Joydeep Ghosh</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 273 - 280</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102386" title="DOI">10.1145/1102351.1102386</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102386&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow35" style="display:inline;"><br /><div style="display:inline">Unsupervised learning methods often involve summarizing the data using a small number of parameters. In certain domains, only a small subset of the available data is relevant for the problem. One-Class Classification or One-Class Clustering ...</div></span>
<span id="toHide35" style="display:none;"><br /><div style="display:inline">Unsupervised learning methods often involve summarizing the data using a small number of parameters. In certain domains, only a small subset of the available data is relevant for the problem. <i>One-Class Classification</i> or <i>One-Class Clustering</i> attempts to find a useful subset by locating a dense region in the data. In particular, a recently proposed algorithm called One-Class Information Ball (OC-IB) shows the advantage of modeling a small set of highly coherent points as opposed to pruning outliers. We present several modifications to OC-IB and integrate it with a global search that results in several improvements such as deterministic results, optimality guarantees, control over cluster size and extension to other cost functions. Empirical studies yield significantly better results on various real and artificial data.</div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102387">Statistical and computational analysis of locality preserving projection</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81418598002">Xiaofei He</a>,
<a href="author_page.cfm?id=81100430245">Deng Cai</a>,
<a href="author_page.cfm?id=81309493646">Wanli Min</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 281 - 288</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102387" title="DOI">10.1145/1102351.1102387</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102387&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow36" style="display:inline;"><br /><div style="display:inline">Recently, several manifold learning algorithms have been proposed, such as ISOMAP (Tenenbaum et al., 2000), Locally Linear Embedding (Roweis & Saul, 2000), Laplacian Eigenmap (Belkin & Niyogi, 2001), Locality Preserving Projection (LPP) (He & Niyogi, ...</div></span>
<span id="toHide36" style="display:none;"><br /><div style="display:inline">Recently, several manifold learning algorithms have been proposed, such as ISOMAP (Tenenbaum et al., 2000), Locally Linear Embedding (Roweis & Saul, 2000), Laplacian Eigenmap (Belkin & Niyogi, 2001), Locality Preserving Projection (LPP) (He & Niyogi, 2003), etc. All of them aim at discovering the meaningful low dimensional structure of the data space. In this paper, we present a statistical analysis of the LPP algorithm. Different from Principal Component Analysis (PCA) which obtains a subspace spanned by the <i>largest</i> eigenvectors of the <i>global</i> covariance matrix, we show that LPP obtains a subspace spanned by the <i>smallest</i> eigenvectors of the <i>local</i> covariance matrix. We applied PCA and LPP to real world document clustering task. Experimental results show that the performance can be significantly improved in the subspace, and especially LPP works much better than PCA.</div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102388">Intrinsic dimensionality estimation of submanifolds in R<sup>d</sup></a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81340489823">Matthias Hein</a>,
<a href="author_page.cfm?id=81309481887">Jean-Yves Audibert</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 289 - 296</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102388" title="DOI">10.1145/1102351.1102388</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102388&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow37" style="display:inline;"><br /><div style="display:inline">We present a new method to estimate the intrinsic dimensionality of a submanifold M in Rd from random samples. The method is based on the convergence rates of a certain U-statistic on the manifold. We solve at least partially ...</div></span>
<span id="toHide37" style="display:none;"><br /><div style="display:inline">We present a new method to estimate the intrinsic dimensionality of a submanifold <i>M</i> in R<sup><i>d</i></sup> from random samples. The method is based on the convergence rates of a certain <i>U</i>-statistic on the manifold. We solve at least partially the question of the choice of the scale of the data. Moreover the proposed method is easy to implement, can handle large data sets and performs very well even for small sample sizes. We compare the proposed method to two standard estimators on several artificial as well as real data sets.</div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102389">Bayesian hierarchical clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309509693">Katherine A. Heller</a>,
<a href="author_page.cfm?id=81100572858">Zoubin Ghahramani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 297 - 304</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102389" title="DOI">10.1145/1102351.1102389</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102389&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow38" style="display:inline;"><br /><div style="display:inline">We present a novel algorithm for agglomerative hierarchical clustering based on evaluating marginal likelihoods of a probabilistic model. This algorithm has several advantages over traditional distance-based agglomerative clustering algorithms. (1) It ...</div></span>
<span id="toHide38" style="display:none;"><br /><div style="display:inline">We present a novel algorithm for agglomerative hierarchical clustering based on evaluating marginal likelihoods of a probabilistic model. This algorithm has several advantages over traditional distance-based agglomerative clustering algorithms. (1) It defines a probabilistic model of the data which can be used to compute the predictive distribution of a test point and the probability of it belonging to any of the existing clusters in the tree. (2) It uses a model-based criterion to decide on merging clusters rather than an ad-hoc distance metric. (3) Bayesian hypothesis testing is used to decide which merges are advantageous and to output the recommended depth of the tree. (4) The algorithm can be interpreted as a novel fast bottom-up approximate inference method for a Dirichlet process (i.e. countably infinite) mixture model (DPM). It provides a new lower bound on the marginal likelihood of a DPM by summing over exponentially many clusterings of the data in polynomial time. We describe procedures for learning the model hyperpa-rameters, computing the predictive distribution, and extensions to the algorithm. Experimental results on synthetic and real-world data sets demonstrate useful properties of the algorithm.</div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102390">Online learning over graphs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100263040">Mark Herbster</a>,
<a href="author_page.cfm?id=81100025866">Massimiliano Pontil</a>,
<a href="author_page.cfm?id=81309491187">Lisa Wainer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 305 - 312</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102390" title="DOI">10.1145/1102351.1102390</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102390&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
 <div style="padding-left:0">
<span id="toShow39" style="display:inline;"><br /><div style="display:inline">We apply classic online learning techniques similar to the perceptron algorithm to the problem of learning a function defined on a graph. The benefit of our approach includes simple algorithms and performance guarantees that we naturally interpret in ...</div></span>
<span id="toHide39" style="display:none;"><br /><div style="display:inline">We apply classic online learning techniques similar to the perceptron algorithm to the problem of learning a function defined on a graph. The benefit of our approach includes simple algorithms and performance guarantees that we naturally interpret in terms of structural properties of the graph, such as the algebraic connectivity or the diameter of the graph. We also discuss how these methods can be modified to allow active learning on a graph. We present preliminary experiments with encouraging results.</div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102391">Adapting two-class support vector classification methods to many class problems</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81405595671">Simon I. Hill</a>,
<a href="author_page.cfm?id=99658670494">Arnaud Doucet</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 313 - 320</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102391" title="DOI">10.1145/1102351.1102391</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102391&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow40" style="display:inline;"><br /><div style="display:inline">A geometric construction is presented which is shown to be an effective tool for understanding and implementing multi-category support vector classification. It is demonstrated how this construction can be used to extend many other existing two-class ...</div></span>
<span id="toHide40" style="display:none;"><br /><div style="display:inline">A geometric construction is presented which is shown to be an effective tool for understanding and implementing multi-category support vector classification. It is demonstrated how this construction can be used to extend many other existing two-class kernel-based classification methodologies in a straightforward way while still preserving attractive properties of individual algorithms. Reducing training times through incorporating the results of pairwise classification is also discussed and experimental results presented.</div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102392">A martingale framework for concept change detection in time-varying data streams</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81552545656">Shen-Shyang Ho</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 321 - 327</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102392" title="DOI">10.1145/1102351.1102392</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102392&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow41" style="display:inline;"><br /><div style="display:inline">In a data streaming setting, data points are observed one by one. The concepts to be learned from the data points may change infinitely often as the data is streaming. In this paper, we extend the idea of testing exchangeability online (Vovk et al., ...</div></span>
<span id="toHide41" style="display:none;"><br /><div style="display:inline">In a data streaming setting, data points are observed one by one. The concepts to be learned from the data points may change infinitely often as the data is streaming. In this paper, we extend the idea of testing exchangeability online (Vovk et al., 2003) to a martingale framework to detect concept changes in time-varying data streams. Two martingale tests are developed to detect concept changes using: (i) martingale values, a direct consequence of the Doob's Maximal Inequality, and (ii) the martingale difference, justified using the Hoeffding-Azuma Inequality. Under some assumptions, the second test theoretically has a lower probability than the first test of rejecting the null hypothesis, "no concept change in the data stream", when it is in fact correct. Experiments show that both martingale tests are effective in detecting concept changes in time-varying data streams simulated using two synthetic data sets and three benchmark data sets.</div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102393">Multi-class protein fold recognition using adaptive codes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100038488">Eugene Ie</a>,
<a href="author_page.cfm?id=81100015405">Jason Weston</a>,
<a href="author_page.cfm?id=81100587603">William Stafford Noble</a>,
<a href="author_page.cfm?id=81100575133">Christina Leslie</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 329 - 336</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102393" title="DOI">10.1145/1102351.1102393</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102393&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow42" style="display:inline;"><br /><div style="display:inline">We develop a novel multi-class classification method based on output codes for the problem of classifying a sequence of amino acids into one of many known protein structural classes, called folds. Our method learns relative weights between ...</div></span>
<span id="toHide42" style="display:none;"><br /><div style="display:inline">We develop a novel multi-class classification method based on <i>output codes</i> for the problem of classifying a sequence of amino acids into one of many known protein structural classes, called <i>folds.</i> Our method learns relative weights between one-vs-all classifiers and encodes information about the protein structural hierarchy for multi-class prediction. Our code weighting approach significantly improves on the standard one-vs-all method for the fold recognition problem. In order to compare against widely used methods in protein sequence analysis, we also test nearest neighbor approaches based on the PSI-BLAST algorithm. Our code weight learning algorithm strongly outperforms these PSI-BLAST methods on every structure recognition problem we consider.</div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102394">Learning approximate preconditions for methods in hierarchical plans</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100034003">Okhtay Ilghami</a>,
<a href="author_page.cfm?id=81100650542">H&#233;ctor Mu&#241;oz-Avila</a>,
<a href="author_page.cfm?id=81452597082">Dana S. Nau</a>,
<a href="author_page.cfm?id=81100615870">David W. Aha</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 337 - 344</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102394" title="DOI">10.1145/1102351.1102394</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102394&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
 <td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow43" style="display:inline;"><br /><div style="display:inline">A significant challenge in developing planning systems for practical applications is the difficulty of acquiring the domain knowledge needed by such systems. One method for acquiring this knowledge is to learn it from plan traces, but this method typically ...</div></span>
<span id="toHide43" style="display:none;"><br /><div style="display:inline">A significant challenge in developing planning systems for practical applications is the difficulty of acquiring the domain knowledge needed by such systems. One method for acquiring this knowledge is to learn it from plan traces, but this method typically requires a huge number of plan traces to converge. In this paper, we show that the problem with slow convergence can be circumvented by having the learner generate solution plans even before the planning domain is completely learned. Our empirical results show that these improvements reduce the size of the training set that is needed to find correct answers to a large percentage of planning problems in the test set.</div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102395">Evaluating machine learning for information extraction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100493656">Neil Ireson</a>,
<a href="author_page.cfm?id=81100648877">Fabio Ciravegna</a>,
<a href="author_page.cfm?id=81100531812">Mary Elaine Califf</a>,
<a href="author_page.cfm?id=81100259018">Dayne Freitag</a>,
<a href="author_page.cfm?id=81100059962">Nicholas Kushmerick</a>,
<a href="author_page.cfm?id=81100549545">Alberto Lavelli</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 345 - 352</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102395" title="DOI">10.1145/1102351.1102395</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102395&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow44" style="display:inline;"><br /><div style="display:inline">Comparative evaluation of Machine Learning (ML) systems used for Information Extraction (IE) has suffered from various inconsistencies in experimental procedures. This paper reports on the results of the Pascal Challenge on Evaluating Machine Learning ...</div></span>
<span id="toHide44" style="display:none;"><br /><div style="display:inline">Comparative evaluation of Machine Learning (ML) systems used for Information Extraction (IE) has suffered from various inconsistencies in experimental procedures. This paper reports on the results of the Pascal Challenge on Evaluating Machine Learning for Information Extraction, which provides a standardised corpus, set of tasks, and evaluation methodology. The challenge is described and the systems submitted by the ten participants are briefly introduced and their performance is analysed.</div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102396">Learn to weight terms in information retrieval using category information</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100054575">Rong Jin</a>,
<a href="author_page.cfm?id=81100205070">Joyce Y. Chai</a>,
<a href="author_page.cfm?id=81100394305">Luo Si</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 353 - 360</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102396" title="DOI">10.1145/1102351.1102396</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102396&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow45" style="display:inline;"><br /><div style="display:inline">How to assign appropriate weights to terms is one of the critical issues in information retrieval. Many term weighting schemes are unsupervised. They are either based on the empirical observation in information retrieval, or based on generative approaches ...</div></span>
<span id="toHide45" style="display:none;"><br /><div style="display:inline">How to assign appropriate weights to terms is one of the critical issues in information retrieval. Many term weighting schemes are unsupervised. They are either based on the empirical observation in information retrieval, or based on generative approaches for language modeling. As a result, the existing term weighting schemes are usually insufficient in distinguishing informative words from the uninformative ones, which is crucial to the performance of information retrieval. In this paper, we present supervised term weighting schemes that automatically learn term weights based on the correlation between word frequency and category information of documents. Empirical studies with the ImageCLEF dataset have indicated that the proposed methods perform substantially better than the state-of-the-art approaches for term weighting and other alternatives that exploit category information for information retrieval.</div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102397">A smoothed boosting algorithm using probabilistic output codes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100054575">Rong Jin</a>,
<a href="author_page.cfm?id=81408598348">Jian Zhang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 361 - 368</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102397" title="DOI">10.1145/1102351.1102397</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102397&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow46" style="display:inline;"><br /><div style="display:inline">AdaBoost.OC has shown to be an effective method in boosting "weak" binary classifiers for multi-class learning. It employs the Error Correcting Output Code (ECOC) method to convert a multi-class learning problem into a set of binary classification problems, ...</div></span>
<span id="toHide46" style="display:none;"><br /><div style="display:inline">AdaBoost.OC has shown to be an effective method in boosting "weak" binary classifiers for multi-class learning. It employs the Error Correcting Output Code (ECOC) method to convert a multi-class learning problem into a set of binary classification problems, and applies the AdaBoost algorithm to solve them efficiently. In this paper, we propose a new boosting algorithm that improves the AdaBoost.OC algorithm in two aspects: 1) It introduces a smoothing mechanism into the boosting algorithm to alleviate the potential overfitting problem with the AdaBoost algorithm, and 2) It introduces a probabilistic coding scheme to generate binary codes for multiple classes such that training errors can be efficiently reduced. Empirical studies with seven UCI datasets have indicated that the proposed boosting algorithm is more robust and effective than the AdaBoost.OC algorithm for multi-class learning.</div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102398">Efficient discriminative learning of Bayesian network classifier via boosted augmented naive Bayes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81546286456">Yushi Jing</a>,
<a href="author_page.cfm?id=81100453089">Vladimir Pavlovi&#263;</a>,
<a href="author_page.cfm?id=81341495589">James M. Rehg</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 369 - 376</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102398" title="DOI">10.1145/1102351.1102398</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102398&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow47" style="display:inline;"><br /><div style="display:inline">The use of Bayesian networks for classification problems has received significant recent attention. Although computationally efficient, the standard maximum likelihood learning method tends to be suboptimal due to the mismatch between its optimization ...</div></span>
<span id="toHide47" style="display:none;"><br /><div style="display:inline">The use of Bayesian networks for classification problems has received significant recent attention. Although computationally efficient, the standard maximum likelihood learning method tends to be suboptimal due to the mismatch between its optimization criteria (data likelihood) and the actual goal for classification (label prediction). Recent approaches to optimizing the classification performance during parameter or structure learning show promise, but lack the favorable computational properties of maximum likelihood learning. In this paper we present the Boosted Augmented Naive Bayes (BAN) classifier. We show that a combination of discriminative data-weighting with generative training of intermediate models can yield a computationally efficient method for discriminative parameter learning and structure selection.</div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102399">A support vector method for multivariate performance measures</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100184551">Thorsten Joachims</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 377 - 384</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102399" title="DOI">10.1145/1102351.1102399</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102399&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow48" style="display:inline;"><br /><div style="display:inline">This paper presents a Support Vector Method for optimizing multivariate nonlinear performance measures like the F1-score. Taking a multivariate prediction approach, we give an algorithm with which such multivariate SVMs can be trained ...</div></span>
<span id="toHide48" style="display:none;"><br /><div style="display:inline">This paper presents a Support Vector Method for optimizing multivariate nonlinear performance measures like the <i>F</i><inf>1</inf>-score. Taking a multivariate prediction approach, we give an algorithm with which such multivariate SVMs can be trained in polynomial time for large classes of potentially non-linear performance measures, in particular ROCArea and all measures that can be computed from the contingency table. The conventional classification SVM arises as a special case of our method.</div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102400">Error bounds for correlation clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100184551">Thorsten Joachims</a>,
<a href="author_page.cfm?id=81100008350">John Hopcroft</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 385 - 392</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102400" title="DOI">10.1145/1102351.1102400</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102400&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow49" style="display:inline;"><br /><div style="display:inline">This paper presents a learning theoretical analysis of correlation clustering (Bansal et al., 2002). In particular, we give bounds on the error with which correlation clustering recovers the correct partition in a planted partition model (Condon & Karp, ...</div></span>
<span id="toHide49" style="display:none;"><br /><div style="display:inline">This paper presents a learning theoretical analysis of correlation clustering (Bansal et al., 2002). In particular, we give bounds on the error with which correlation clustering recovers the correct partition in a planted partition model (Condon & Karp, 2001; McSherry, 2001). Using these bounds, we analyze how the accuracy of correlation clustering scales with the number of clusters and the sparsity of the graph. We also propose a statistical test that analyzes the significance of the clustering found by correlation clustering.</div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102401">Interactive learning of mappings from visual percepts to actions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100519270">S&#233;bastien Jodogne</a>,
<a href="author_page.cfm?id=81100291302">Justus H. Piater</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 393 - 400</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102401" title="DOI">10.1145/1102351.1102401</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102401&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow50" style="display:inline;"><br /><div style="display:inline">We introduce flexible algorithms that can automatically learn mappings from images to actions by interacting with their environment. They work by introducing an image classifier in front of a Reinforcement Learning algorithm. The classifier partitions ...</div></span>
<span id="toHide50" style="display:none;"><br /><div style="display:inline">We introduce flexible algorithms that can automatically learn mappings from images to actions by interacting with their environment. They work by introducing an image classifier in front of a Reinforcement Learning algorithm. The classifier partitions the visual space according to the presence or absence of highly informative local descriptors. The image classifier is incrementally refined by selecting new local descriptors when perceptual aliasing is detected. Thus, we reduce the visual input domain down to a size manageable by Reinforcement Learning, permitting us to learn direct percept-to-action mappings. Experimental results on a continuous visual navigation task illustrate the applicability of the framework.</div></span> <a id="expcoll50" href="JavaScript: expandcollapse('expcoll50',50)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102402">A causal approach to hierarchical decomposition of factored MDPs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309491706">Anders Jonsson</a>,
<a href="author_page.cfm?id=81100207208">Andrew Barto</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 401 - 408</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102402" title="DOI">10.1145/1102351.1102402</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102402&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow51" style="display:inline;"><br /><div style="display:inline">We present Variable Influence Structure Analysis, an algorithm that dynamically performs hierarchical decomposition of factored Markov decision processes. Our algorithm determines causal relationships between state variables and introduces temporally-extended ...</div></span>
<span id="toHide51" style="display:none;"><br /><div style="display:inline">We present Variable Influence Structure Analysis, an algorithm that dynamically performs hierarchical decomposition of factored Markov decision processes. Our algorithm determines causal relationships between state variables and introduces temporally-extended actions that cause the values of state variables to change. Each temporally-extended action corresponds to a subtask that is significantly easier to solve than the overall task. Results from experiments show great promise in scaling to larger tasks.</div></span> <a id="expcoll51" href="JavaScript: expandcollapse('expcoll51',51)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102403">A comparison of tight generalization error bounds</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100373707">Matti K&#228;&#228;ri&#228;inen</a>,
 <a href="author_page.cfm?id=81100453722">John Langford</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 409 - 416</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102403" title="DOI">10.1145/1102351.1102403</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102403&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow52" style="display:inline;"><br /><div style="display:inline">We investigate the empirical applicability of several bounds (a number of which are new) on the true error rate of learned classifiers which hold whenever the examples are chosen independently at random from a fixed distribution.The collection of tricks ...</div></span>
<span id="toHide52" style="display:none;"><br /><div style="display:inline">We investigate the empirical applicability of several bounds (a number of which are new) on the true error rate of learned classifiers which hold whenever the examples are chosen independently at random from a fixed distribution.The collection of tricks we use includes:1. A technique using unlabeled data for a tight derandomization of randomized bounds.2. A tight form of the progressive validation bound.3. The exact form of the test set bound.The bounds are implemented in the semibound package and are freely available.</div></span> <a id="expcoll52" href="JavaScript: expandcollapse('expcoll52',52)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102404">Generalized LARS as an effective feature selection tool for text classification with SVMs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100176197">S. Sathiya Keerthi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 417 - 424</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102404" title="DOI">10.1145/1102351.1102404</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102404&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow53" style="display:inline;"><br /><div style="display:inline">In this paper we generalize the LARS feature selection method to the linear SVM model, derive an efficient algorithm for it, and empirically demonstrate its usefulness as a feature selection tool for text classification.</div></span>
<span id="toHide53" style="display:none;"><br /><div style="display:inline">In this paper we generalize the LARS feature selection method to the linear SVM model, derive an efficient algorithm for it, and empirically demonstrate its usefulness as a feature selection tool for text classification.</div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102405">Ensembles of biased classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100352368">Rinat Khoussainov</a>,
<a href="author_page.cfm?id=81539250656">Andreas He&#223;</a>,
<a href="author_page.cfm?id=81100059962">Nicholas Kushmerick</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 425 - 432</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102405" title="DOI">10.1145/1102351.1102405</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102405&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow54" style="display:inline;"><br /><div style="display:inline">We propose a novel ensemble learning algorithm called Triskel, which has two interesting features. First, Triskel learns an ensemble of classifiers, each biased to have high precision on instances from a single class (as opposed to, for example, boosting, ...</div></span>
<span id="toHide54" style="display:none;"><br /><div style="display:inline">We propose a novel ensemble learning algorithm called Triskel, which has two interesting features. First, Triskel learns an ensemble of classifiers, each biased to have high precision on instances from a single class (as opposed to, for example, boosting, where the ensemble members are biased to maximise accuracy over a subset of instances from all classes). Second, the ensemble members' voting weights are assigned so that certain pairs of biased classifiers outweigh the rest of the ensemble, if their predictions agree. Our experiments demonstrate that Triskel often outperforms boosting, in terms of both accuracy and training time. We also present an ROC analysis, which shows that Triskel's iterative structure corresponds to a sequence of nested ROC spaces. The analysis predicts that Triskel works best when there are concavities in the ROC curves; this prediction agrees with our empirical results.</div></span> <a id="expcoll54" href="JavaScript: expandcollapse('expcoll54',54)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102406">Computational aspects of Bayesian partition models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309511472">Mikko Koivisto</a>,
<a href="author_page.cfm?id=81100470938">Kismat Sood</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 433 - 440</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102406" title="DOI">10.1145/1102351.1102406</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102406&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow55" style="display:inline;"><br /><div style="display:inline">The conditional distribution of a discrete variable y, given another discrete variable x, is often specified by assigning one multinomial distribution to each state of x. The cost of this rich parametrization is the loss of statistical ...</div></span>
<span id="toHide55" style="display:none;"><br /><div style="display:inline">The conditional distribution of a discrete variable <i>y</i>, given another discrete variable <i>x</i>, is often specified by assigning one multinomial distribution to each state of <i>x.</i> The cost of this rich parametrization is the loss of statistical power in cases where the data actually fits a model with much fewer parameters. In this paper, we consider a model that partitions the state space of <i>x</i> into disjoint sets, and assigns a single Dirichlet-multinomial to each set. We treat the partition as an unknown variable which is to be integrated away when the interest is in a coarser level task, e.g., variable selection or classification. Based on two different computational approaches, we present two exact algorithms for integration over partitions. Respective complexity bounds are derived in terms of detailed problem characteristics, including the size of the data and the size of the state space of <i>x.</i> Experiments on synthetic data demonstrate the applicability of the algorithms.</div></span> <a id="expcoll55" href="JavaScript: expandcollapse('expcoll55',55)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102407">Learning the structure of Markov logic networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309513190">Stanley Kok</a>,
<a href="author_page.cfm?id=81100205908">Pedro Domingos</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 441 - 448</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102407" title="DOI">10.1145/1102351.1102407</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102407&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow56" style="display:inline;"><br /><div style="display:inline">Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. In this paper we develop an algorithm for learning the structure of MLNs from relational ...</div></span>
<span id="toHide56" style="display:none;"><br /><div style="display:inline">Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. In this paper we develop an algorithm for learning the structure of MLNs from relational databases, combining ideas from inductive logic programming (ILP) and feature induction in Markov networks. The algorithm performs a beam or shortest-first search of the space of clauses, guided by a weighted pseudo-likelihood measure. This requires computing the optimal weights for each candidate structure, but we show how this can be done efficiently. The algorithm can be used to learn an MLN from scratch, or to refine an existing knowledge base. We have applied it in two real-world domains, and found that it outperforms using off-the-shelf ILP systems to learn the MLN structure, as well as pure ILP, purely probabilistic and purely knowledge-based approaches.</div></span> <a id="expcoll56" href="JavaScript: expandcollapse('expcoll56',56)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102408">Using additive expert ensembles to cope with concept drift</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100095014">Jeremy Z. Kolter</a>,
<a href="author_page.cfm?id=81100151275">Marcus A. Maloof</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 449 - 456</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102408" title="DOI">10.1145/1102351.1102408</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102408&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow57" style="display:inline;"><br /><div style="display:inline">We consider online learning where the target concept can change over time. Previous work on expert prediction algorithms has bounded the worst-case performance on any subsequence of the training data relative to the performance of the best expert. However, ...</div></span>
<span id="toHide57" style="display:none;"><br /><div style="display:inline">We consider online learning where the target concept can change over time. Previous work on expert prediction algorithms has bounded the worst-case performance on any subsequence of the training data relative to the performance of the best expert. However, because these "experts" may be difficult to implement, we take a more general approach and bound performance relative to the actual performance of any online learner on this single subsequence. We present the additive expert ensemble algorithm <b>AddExp</b>, a new, general method for using any online learner for drifting concepts. We adapt techniques for analyzing expert prediction algorithms to prove mistake and loss bounds for a discrete and a continuous version of <b>AddExp.</b> Finally, we present pruning methods and empirical results for data sets with concept drift.</div></span> <a id="expcoll57" href="JavaScript: expandcollapse('expcoll57',57)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102409">Semi-supervised graph clustering: a kernel approach</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100083449">Brian Kulis</a>,
<a href="author_page.cfm?id=81100645746">Sugato Basu</a>,
<a href="author_page.cfm?id=81100098715">Inderjit Dhillon</a>,
<a href="author_page.cfm?id=81100539345">Raymond Mooney</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 457 - 464</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102409" title="DOI">10.1145/1102351.1102409</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102409&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow58" style="display:inline;"><br /><div style="display:inline">Semi-supervised clustering algorithms aim to improve clustering results using limited supervision. The supervision is generally given as pairwise constraints; such constraints are natural for graphs, yet most semi-supervised clustering algorithms are ...</div></span>
<span id="toHide58" style="display:none;"><br /><div style="display:inline">Semi-supervised clustering algorithms aim to improve clustering results using limited supervision. The supervision is generally given as pairwise constraints; such constraints are natural for graphs, yet most semi-supervised clustering algorithms are designed for data represented as vectors. In this paper, we unify vector-based and graph-based approaches. We show that a recently-proposed objective function for semi-supervised clustering based on Hidden Markov Random Fields, with squared Euclidean distance and a certain class of constraint penalty functions, can be expressed as a special case of the weighted kernel <i>k</i>-means objective. A recent theoretical connection between kernel <i>k</i>-means and several graph clustering objectives enables us to perform semi-supervised clustering of data given either as vectors or as a graph. For vector data, the kernel approach also enables us to find clusters with non-linear boundaries in the input data space. Furthermore, we show that recent work on spectral learning (Kamvar et al., 2003) may be viewed as a special case of our formulation. We empirically show that our algorithm is able to outperform current state-of-the-art semi-supervised algorithms on both vector-based and graph-based data sets.</div></span> <a id="expcoll58" href="JavaScript: expandcollapse('expcoll58',58)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102410">A brain computer interface with online feedback based on magnetoencephalography</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100197623">Thomas Navin Lal</a>,
<a href="author_page.cfm?id=81100252850">Michael Schr&#246;der</a>,
<a href="author_page.cfm?id=81338489356">N. Jeremy Hill</a>,
<a href="author_page.cfm?id=81336492385">Hubert Preissl</a>,
<a href="author_page.cfm?id=81338489269">Thilo Hinterberger</a>,
<a href="author_page.cfm?id=81343500295">J&#252;rgen Mellinger</a>,
<a href="author_page.cfm?id=81338487824">Martin Bogdan</a>,
<a href="author_page.cfm?id=81100167019">Wolfgang Rosenstiel</a>,
<a href="author_page.cfm?id=81338489201">Thomas Hofmann</a>,
<a href="author_page.cfm?id=81318493204">Niels Birbaumer</a>,
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 465 - 472</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102410" title="DOI">10.1145/1102351.1102410</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102410&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow59" style="display:inline;"><br /><div style="display:inline">The aim of this paper is to show that machine learning techniques can be used to derive a classifying function for human brain signal data measured by magnetoencephalography (MEG), for the use in a brain computer interface (BCI). This is especially helpful ...</div></span>
<span id="toHide59" style="display:none;"><br /><div style="display:inline">The aim of this paper is to show that machine learning techniques can be used to derive a classifying function for human brain signal data measured by magnetoencephalography (MEG), for the use in a brain computer interface (BCI). This is especially helpful for evaluating quickly whether a BCI approach based on electroencephalography, on which training may be slower due to lower signal-to-noise ratio, is likely to succeed. We apply RCE and regularized SVMs to the experimental data of ten healthy subjects performing a motor imagery task. Four subjects were able to use a trained classifier to write a short name. Further analysis gives evidence that the proposed imagination task is suboptimal for the possible extension to a multiclass interface. To the best of our knowledge this paper is the first working online MEG-based BCI and is therefore a "proof of concept".</div></span> <a id="expcoll59" href="JavaScript: expandcollapse('expcoll59',59)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102411">Relating reinforcement learning performance to classification performance</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100453722">John Langford</a>,
<a href="author_page.cfm?id=81100278691">Bianca Zadrozny</a>
</span>
</td>
</tr>
 <tr>
<td></td>
<td> <span style="padding-left:0">Pages: 473 - 480</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102411" title="DOI">10.1145/1102351.1102411</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102411&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow60" style="display:inline;"><br /><div style="display:inline">We prove a quantitative connection between the expected sum of rewards of a policy and binary classification performance on created subproblems. This connection holds without any unobservable assumptions (no assumption of independence, small mixing time, ...</div></span>
<span id="toHide60" style="display:none;"><br /><div style="display:inline">We prove a quantitative connection between the expected sum of rewards of a policy and binary classification performance on created subproblems. This connection holds without any unobservable assumptions (no assumption of independence, small mixing time, fully observable states, or even hidden states) and the resulting statement is independent of the number of states or actions. The statement is critically dependent on the size of the rewards and prediction performance of the created classifiers.We also provide some general guidelines for obtaining good classification performance on the created subproblems. In particular, we discuss possible methods for generating training examples for a classifier learning algorithm.</div></span> <a id="expcoll60" href="JavaScript: expandcollapse('expcoll60',60)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102412">PAC-Bayes risk bounds for sample-compressed Gibbs classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100257819">Fran&#231;ois Laviolette</a>,
<a href="author_page.cfm?id=81100612037">Mario Marchand</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 481 - 488</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102412" title="DOI">10.1145/1102351.1102412</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102412&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow61" style="display:inline;"><br /><div style="display:inline">We extend the PAC-Bayes theorem to the sample-compression setting where each classifier is represented by two independent sources of information: a compression set which consists of a small subset of the training data, and a message string ...</div></span>
<span id="toHide61" style="display:none;"><br /><div style="display:inline">We extend the PAC-Bayes theorem to the sample-compression setting where each classifier is represented by two independent sources of information: a <i>compression set</i> which consists of a small subset of the training data, and a <i>message string</i> of the additional information needed to obtain a classifier. The new bound is obtained by using a prior over a data-independent set of objects where each object gives a classifier only when the training data is provided. The new PAC-Bayes theorem states that a Gibbs classifier defined on a posterior over sample-compressed classifiers can have a smaller risk bound than any such (deterministic) sample-compressed classifier.</div></span> <a id="expcoll61" href="JavaScript: expandcollapse('expcoll61',61)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102413">Heteroscedastic Gaussian process regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81339511241">Quoc V. Le</a>,
<a href="author_page.cfm?id=81100243402">Alex J. Smola</a>,
<a href="author_page.cfm?id=81100333303">St&#233;phane Canu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 489 - 496</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102413" title="DOI">10.1145/1102351.1102413</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102413&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow62" style="display:inline;"><br /><div style="display:inline">This paper presents an algorithm to estimate simultaneously both mean and variance of a non parametric regression problem. The key point is that we are able to estimate variance locally unlike standard Gaussian Process regression or SVMs. This ...</div></span>
<span id="toHide62" style="display:none;"><br /><div style="display:inline">This paper presents an algorithm to estimate simultaneously both mean and variance of a non parametric regression problem. The key point is that we are able to estimate variance <i>locally</i> unlike standard Gaussian Process regression or SVMs. This means that our estimator adapts to the local noise. The problem is cast in the setting of maximum a posteriori estimation in exponential families. Unlike previous work, we obtain a convex optimization problem which can be solved via Newton's method.</div></span> <a id="expcoll62" href="JavaScript: expandcollapse('expcoll62',62)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102414">Predicting relative performance of classifiers from samples</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309503431">Rui Leite</a>,
<a href="author_page.cfm?id=81100592553">Pavel Brazdil</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 497 - 503</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102414" title="DOI">10.1145/1102351.1102414</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102414&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow63" style="display:inline;"><br /><div style="display:inline">This paper is concerned with the problem of predicting relative performance of classification algorithms. It focusses on methods that use results on small samples and discusses the shortcomings of previous approaches. A new variant is proposed that exploits, ...</div></span>
<span id="toHide63" style="display:none;"><br /><div style="display:inline">This paper is concerned with the problem of predicting relative performance of classification algorithms. It focusses on methods that use results on small samples and discusses the shortcomings of previous approaches. A new variant is proposed that exploits, as some previous approaches, meta-learning. The method requires that experiments be conducted on few samples. The information gathered is used to identify the <i>nearest learning curve</i> for which the sampling procedure was carried out fully. This in turn permits to generate a prediction regards the relative performance of algorithms. Experimental evaluation shows that the method competes well with previous approaches and provides quite good and practical solution to this problem.</div></span> <a id="expcoll63" href="JavaScript: expandcollapse('expcoll63',63)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102415">Logistic regression with an auxiliary data source</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309499575">Xuejun Liao</a>,
<a href="author_page.cfm?id=81339537942">Ya Xue</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 505 - 512</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102415" title="DOI">10.1145/1102351.1102415</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102415&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow64" style="display:inline;"><br /><div style="display:inline">To achieve good generalization in supervised learning, the training and testing examples are usually required to be drawn from the same source distribution. In this paper we propose a method to relax this requirement in the context of logistic regression. ...</div></span>
<span id="toHide64" style="display:none;"><br /><div style="display:inline">To achieve good generalization in supervised learning, the training and testing examples are usually required to be drawn from the same source distribution. In this paper we propose a method to relax this requirement in the context of logistic regression. Assuming <i>D<sup>p</sup></i> and <i>D<sup>a</sup></i> are two sets of examples drawn from two mismatched distributions, where <i>D<sup>a</sup></i> are fully labeled and <i>D<sup>p</sup></i> partially labeled, our objective is to complete the labels of <i>D<sup>p</sup>.</i> We introduce an auxiliary variable &mu; for each example in <i>D<sup>a</sup></i> to reflect its mismatch with <i>D<sup>p</sup>.</i> Under an appropriate constraint the &mu;'s are estimated as a byproduct, along with the classifier. We also present an active learning approach for selecting the labeled examples in <i>D<sup>p</sup>.</i> The proposed algorithm, called "Migratory-Logit" or M-Logit, is demonstrated successfully on simulated as well as real data sets.</div></span> <a id="expcoll64" href="JavaScript: expandcollapse('expcoll64',64)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102416">Predicting protein folds with structural repeats using a chain graph model</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100537718">Yan Liu</a>,
<a href="author_page.cfm?id=81407592503">Eric P. Xing</a>,
<a href="author_page.cfm?id=81452611253">Jaime Carbonell</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 513 - 520</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102416" title="DOI">10.1145/1102351.1102416</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102416&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow65" style="display:inline;"><br /><div style="display:inline">Protein fold recognition is a key step towards inferring the tertiary structures from amino-acid sequences. Complex folds such as those consisting of interacting structural repeats are prevalent in proteins involved in a wide spectrum of biological functions. ...</div></span>
<span id="toHide65" style="display:none;"><br /><div style="display:inline">Protein fold recognition is a key step towards inferring the tertiary structures from amino-acid sequences. Complex folds such as those consisting of interacting structural repeats are prevalent in proteins involved in a wide spectrum of biological functions. However, extant approaches often perform inadequately due to their inability to capture long-range interactions between structural units and to handle low sequence similarities across proteins (under 25% identity). In this paper, we propose a chain graph model built on a causally connected series of segmentation conditional random fields (SCRFs) to address these issues. Specifically, the SCRF model captures long-range interactions within recurring structural units and the Bayesian network backbone decomposes cross-repeat interactions into locally computable modules consisting of repeat-specific SCRFs and a model for sequence motifs. We applied this model to predict &beta;-helices and leucine-rich repeats, and found it significantly outperforms extant methods in predictive accuracy and/or computational efficiency.</div></span> <a id="expcoll65" href="JavaScript: expandcollapse('expcoll65',65)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102417">Unsupervised evidence integration</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81538235656">Philip M. Long</a>,
<a href="author_page.cfm?id=81309509427">Vinay Varadan</a>,
<a href="author_page.cfm?id=81309486698">Sarah Gilman</a>,
<a href="author_page.cfm?id=81309488125">Mark Treshock</a>,
<a href="author_page.cfm?id=81452616886">Rocco A. Servedio</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 521 - 528</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102417" title="DOI">10.1145/1102351.1102417</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102417&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow66" style="display:inline;"><br /><div style="display:inline">Many biological propositions can be supported by a variety of different types of evidence. It is often useful to collect together large numbers of such propositions, together with the evidence supporting them, into databases to be used in other analyses. ...</div></span>
<span id="toHide66" style="display:none;"><br /><div style="display:inline">Many biological propositions can be supported by a variety of different types of evidence. It is often useful to collect together large numbers of such propositions, together with the evidence supporting them, into databases to be used in other analyses. Methods that automatically make preliminary choices about which propositions to include can be helpful, if they are accurate enough. This can involve weighing evidence of varying strength.We describe a method for learning a scoring function to weigh evidence of different types. The algorithm evaluates each source of evidence by the extent to which other sources tend to support it. The details are guided by a probabilistic formulation of the problem, building on previous theoretical work. We evaluate our method by applying it to predict protein-protein interactions in yeast, and using synthetic data.</div></span> <a id="expcoll66" href="JavaScript: expandcollapse('expcoll66',66)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102418">Naive Bayes models for probability estimation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81381602582">Daniel Lowd</a>,
<a href="author_page.cfm?id=81100205908">Pedro Domingos</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 529 - 536</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102418" title="DOI">10.1145/1102351.1102418</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102418&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow67" style="display:inline;"><br /><div style="display:inline">Naive Bayes models have been widely used for clustering and classification. However, they are seldom used for general probabilistic learning and inference (i.e., for estimating and computing arbitrary joint, conditional and marginal distributions). In ...</div></span>
<span id="toHide67" style="display:none;"><br /><div style="display:inline">Naive Bayes models have been widely used for clustering and classification. However, they are seldom used for general probabilistic learning and inference (i.e., for estimating and computing arbitrary joint, conditional and marginal distributions). In this paper we show that, for a wide range of benchmark datasets, naive Bayes models learned using EM have accuracy and learning time comparable to Bayesian networks with context-specific independence. Most significantly, naive Bayes inference is orders of magnitude faster than Bayesian network inference using Gibbs sampling and belief propagation. This makes naive Bayes models a very attractive alternative to Bayesian networks for general probability estimation, particularly in large or real-time domains.</div></span> <a id="expcoll67" href="JavaScript: expandcollapse('expcoll67',67)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102419">ROC confidence bands: an empirical evaluation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100292044">Sofus A. Macskassy</a>,
<a href="author_page.cfm?id=81100596683">Foster Provost</a>,
<a href="author_page.cfm?id=81100549576">Saharon Rosset</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 537 - 544</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102419" title="DOI">10.1145/1102351.1102419</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102419&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow68" style="display:inline;"><br /><div style="display:inline">This paper is about constructing confidence bands around ROC curves. We first introduce to the machine learning community three band-generating methods from the medical field, and evaluate how well they perform. Such confidence bands represent the region ...</div></span>
<span id="toHide68" style="display:none;"><br /><div style="display:inline">This paper is about constructing confidence bands around ROC curves. We first introduce to the machine learning community three band-generating methods from the medical field, and evaluate how well they perform. Such confidence bands represent the region where the "true" ROC curve is expected to reside, with the designated confidence level. To assess the containment of the bands we begin with a synthetic world where we know the true ROC curve---specifically, where the class-conditional model scores are normally distributed. The only method that attains reasonable containment out-of-the-box produces non-parametric, "fixed-width" bands (FWBs). Next we move to a context more appropriate for machine learning evaluations: bands that with a certain confidence level will bound the performance of the model on future data. We introduce a correction to account for the larger uncertainty, and the widened FWBs continue to have reasonable containment. Finally, we assess the bands on 10 relatively large benchmark data sets. We conclude by recommending these FWBs, noting that being non-parametric they are especially attractive for machine learning studies, where the score distributions (1) clearly are not normal, and (2) even for the same data set vary substantially from learning method to learning method.</div></span> <a id="expcoll68" href="JavaScript: expandcollapse('expcoll68',68)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102420">Modeling word burstiness using the Dirichlet distribution</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100505789">Rasmus E. Madsen</a>,
<a href="author_page.cfm?id=81332507912">David Kauchak</a>,
<a href="author_page.cfm?id=81339498029">Charles Elkan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 545 - 552</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102420" title="DOI">10.1145/1102351.1102420</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102420&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow69" style="display:inline;"><br /><div style="display:inline">Multinomial distributions are often used to model text documents. However, they do not capture well the phenomenon that words in a document tend to appear in bursts: if a word appears once, it is more likely to appear again. In this paper, we propose ...</div></span>
<span id="toHide69" style="display:none;"><br /><div style="display:inline">Multinomial distributions are often used to model text documents. However, they do not capture well the phenomenon that words in a document tend to appear in bursts: if a word appears once, it is more likely to appear again. In this paper, we propose the Dirichlet compound multinomial model (DCM) as an alternative to the multinomial. The DCM model has one additional degree of freedom, which allows it to capture burstiness. We show experimentally that the DCM is substantially better than the multinomial at modeling text data, measured by perplexity. We also show using three standard document collections that the DCM leads to better classification than the multinomial model. DCM performance is comparable to that obtained with multiple heuristic changes to the multinomial model.</div></span> <a id="expcoll69" href="JavaScript: expandcollapse('expcoll69',69)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102421">Proto-value functions: developmental reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100132345">Sridhar Mahadevan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 553 - 560</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102421" title="DOI">10.1145/1102351.1102421</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102421&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow70" style="display:inline;"><br /><div style="display:inline">This paper presents a novel framework called proto-reinforcement learning (PRL), based on a mathematical model of a proto-value function: these are task-independent basis functions that form the building blocks of all value functions on ...</div></span>
<span id="toHide70" style="display:none;"><br /><div style="display:inline">This paper presents a novel framework called <i>proto-reinforcement learning</i> (PRL), based on a mathematical model of a <i>proto-value</i> function: these are task-independent basis functions that form the building blocks of all value functions on a given state space manifold. Proto-value functions are learned not from rewards, but instead from analyzing the <i>topology</i> of the state space. Formally, proto-value functions are Fourier eigenfunctions of the Laplace-Beltrami diffusion operator on the state space manifold. Proto-value functions facilitate structural decomposition of large state spaces, and form <i>geodesically</i> smooth orthonormal basis functions for approximating any value function. The theoretical basis for proto-value functions combines insights from spectral graph theory, harmonic analysis, and Riemannian manifolds. Proto-value functions enable a novel generation of algorithms called <i>representation policy iteration</i>, unifying the learning of representation and behavior.</div></span> <a id="expcoll70" href="JavaScript: expandcollapse('expcoll70',70)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102422">The cross entropy method for classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100515533">Shie Mannor</a>,
<a href="author_page.cfm?id=81339521488">Dori Peleg</a>,
<a href="author_page.cfm?id=81342509715">Reuven Rubinstein</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 561 - 568</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102422" title="DOI">10.1145/1102351.1102422</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102422&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow71" style="display:inline;"><br /><div style="display:inline">We consider support vector machines for binary classification. As opposed to most approaches we use the number of support vectors (the "L0 norm") as a regularizing term instead of the L1 or L2 norms. ...</div></span>
<span id="toHide71" style="display:none;"><br /><div style="display:inline">We consider support vector machines for binary classification. As opposed to most approaches we use the number of support vectors (the <i>"L</i><inf>0</inf> norm") as a regularizing term instead of the <i>L</i><inf>1</inf> or <i>L</i><inf>2</inf> norms. In order to solve the optimization problem we use the cross entropy method to search over the possible sets of support vectors. The algorithm consists of solving a sequence of efficient linear programs. We report experiments where our method produces generalization errors that are similar to support vector machines, while using a considerably smaller number of support vectors.</div></span> <a id="expcoll71" href="JavaScript: expandcollapse('expcoll71',71)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102423">Bounded real-time dynamic programming: RTDP with monotone upper bounds and performance guarantees</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100140036">H. Brendan McMahan</a>,
<a href="author_page.cfm?id=81100261957">Maxim Likhachev</a>,
<a href="author_page.cfm?id=81100037343">Geoffrey J. Gordon</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 569 - 576</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102423" title="DOI">10.1145/1102351.1102423</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102423&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow72" style="display:inline;"><br /><div style="display:inline">MDPs are an attractive formalization for planning, but realistic problems often have intractably large state spaces. When we only need a partial policy to get from a fixed start state to a goal, restricting computation to states relevant to this task ...</div></span>
<span id="toHide72" style="display:none;"><br /><div style="display:inline">MDPs are an attractive formalization for planning, but realistic problems often have intractably large state spaces. When we only need a partial policy to get from a fixed start state to a goal, restricting computation to states relevant to this task can make much larger problems tractable. We introduce a new algorithm, Bounded RTDP, which can produce partial policies with strong performance guarantees while only touching a fraction of the state space, even on problems where other algorithms would have to visit the full state space. To do so, Bounded RTDP maintains both upper and lower bounds on the optimal value function. The performance of Bounded RTDP is greatly aided by the introduction of a new technique to efficiently find suitable upper bounds; this technique can also be used to provide informed initialization to a wide range of other planning algorithms.</div></span> <a id="expcoll72" href="JavaScript: expandcollapse('expcoll72',72)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102424">Comparing clusterings: an axiomatic view</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309481968">Marina Meil&#462;</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 577 - 584</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102424" title="DOI">10.1145/1102351.1102424</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102424&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow73" style="display:inline;"><br /><div style="display:inline">This paper views clusterings as elements of a lattice. Distances between clusterings are analyzed in their relationship to the lattice. From this vantage point, we first give an axiomatic characterization of some criteria for comparing clusterings, including ...</div></span>
<span id="toHide73" style="display:none;"><br /><div style="display:inline">This paper views clusterings as elements of a lattice. Distances between clusterings are analyzed in their relationship to the lattice. From this vantage point, we first give an axiomatic characterization of some criteria for comparing clusterings, including the variation of information and the unadjusted Rand index. Then we study other distances between partitions w.r.t these axioms and prove an impossibility result: there is no "sensible" criterion for comparing clusterings that is simultaneously (1) aligned with the lattice of partitions, (2) convexely additive, and (3) bounded.</div></span> <a id="expcoll73" href="JavaScript: expandcollapse('expcoll73',73)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102425">Weighted decomposition kernels</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309487709">Sauro Menchetti</a>,
<a href="author_page.cfm?id=81100281350">Fabrizio Costa</a>,
<a href="author_page.cfm?id=81100400193">Paolo Frasconi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 585 - 592</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102425" title="DOI">10.1145/1102351.1102425</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102425&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow74" style="display:inline;"><br /><div style="display:inline">We introduce a family of kernels on discrete data structures within the general class of decomposition kernels. A weighted decomposition kernel (WDK) is computed by dividing objects into substructures indexed by a selector. Two substructures are then ...</div></span>
<span id="toHide74" style="display:none;"><br /><div style="display:inline">We introduce a family of kernels on discrete data structures within the general class of decomposition kernels. A weighted decomposition kernel (WDK) is computed by dividing objects into substructures indexed by a selector. Two substructures are then matched if their selectors satisfy an equality predicate, while the importance of the match is determined by a probability kernel on local distributions fitted on the substructures. Under reasonable assumptions, a WDK can be computed efficiently and can avoid combinatorial explosion of the feature space. We report experimental evidence that the proposed kernel is highly competitive with respect to more complex state-of-the-art methods on a set of problems in bioinformatics.</div></span> <a id="expcoll74" href="JavaScript: expandcollapse('expcoll74',74)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102426">High speed obstacle avoidance using monocular vision and reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309505956">Jeff Michels</a>,
<a href="author_page.cfm?id=81100341921">Ashutosh Saxena</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 593 - 600</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102426" title="DOI">10.1145/1102351.1102426</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102426&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow75" style="display:inline;"><br /><div style="display:inline">We consider the task of driving a remote control car at high speeds through unstructured outdoor environments. We present an approach in which supervised learning is first used to estimate depths from single monocular images. The learning algorithm can ...</div></span>
<span id="toHide75" style="display:none;"><br /><div style="display:inline">We consider the task of driving a remote control car at high speeds through unstructured outdoor environments. We present an approach in which supervised learning is first used to estimate depths from single monocular images. The learning algorithm can be trained either on real camera images labeled with ground-truth distances to the closest obstacles, or on a training set consisting of synthetic graphics images. The resulting algorithm is able to learn monocular vision cues that accurately estimate the relative depths of obstacles in a scene. Reinforcement learning/policy search is then applied within a simulator that renders synthetic scenes. This learns a control policy that selects a steering direction as a function of the vision system's output. We present results evaluating the predictive ability of the algorithm both on held out test data, and in actual autonomous driving experiments.</div></span> <a id="expcoll75" href="JavaScript: expandcollapse('expcoll75',75)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102427">Dynamic preferences in multi-criteria reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309491769">Sriraam Natarajan</a>,
<a href="author_page.cfm?id=81100642869">Prasad Tadepalli</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 601 - 608</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102427" title="DOI">10.1145/1102351.1102427</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102427&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow76" style="display:inline;"><br /><div style="display:inline">The current framework of reinforcement learning is based on maximizing the expected returns based on scalar rewards. But in many real world situations, tradeoffs must be made among multiple objectives. Moreover, the agent's preferences between different ...</div></span>
<span id="toHide76" style="display:none;"><br /><div style="display:inline">The current framework of reinforcement learning is based on maximizing the expected returns based on scalar rewards. But in many real world situations, tradeoffs must be made among multiple objectives. Moreover, the agent's preferences between different objectives may vary with time. In this paper, we consider the problem of learning in the presence of time-varying preferences among multiple objectives, using numeric weights to represent their importance. We propose a method that allows us to store a finite number of policies, choose an appropriate policy for any weight vector and improve upon it. The idea is that although there are infinitely many weight vectors, they may be well-covered by a small number of optimal policies. We show this empirically in two domains: a version of the Buridan's ass problem and network routing.</div></span> <a id="expcoll76" href="JavaScript: expandcollapse('expcoll76',76)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102428">Learning first-order probabilistic models with combining rules</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309491769">Sriraam Natarajan</a>,
<a href="author_page.cfm?id=81100642869">Prasad Tadepalli</a>,
<a href="author_page.cfm?id=81309501663">Eric Altendorf</a>,
<a href="author_page.cfm?id=81452609390">Thomas G. Dietterich</a>,
<a href="author_page.cfm?id=81100360494">Alan Fern</a>,
<a href="author_page.cfm?id=81100014845">Angelo Restificar</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 609 - 616</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102428" title="DOI">10.1145/1102351.1102428</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102428&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow77" style="display:inline;"><br /><div style="display:inline">First-order probabilistic models allow us to model situations in which a random variable in the first-order model may have a large and varying numbers of parent variables in the ground ("unrolled") model. One approach to compactly describing such models ...</div></span>
<span id="toHide77" style="display:none;"><br /><div style="display:inline">First-order probabilistic models allow us to model situations in which a random variable in the first-order model may have a large and varying numbers of parent variables in the ground ("unrolled") model. One approach to compactly describing such models is to independently specify the probability of a random variable conditioned on each individual parent (or small sets of parents) and then combine these <i>conditional distributions</i> via a combining rule (e.g., Noisy-OR). This paper presents algorithms for learning with combining rules. Specifically, algorithms based on gradient descent and expectation maximization are derived, implemented, and evaluated on synthetic data and on a real-world task. The results demonstrate that the algorithms are able to learn the parameters of both the individual parent-target distributions and the combining rules.</div></span> <a id="expcoll77" href="JavaScript: expandcollapse('expcoll77',77)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102429">An efficient method for simplifying support vector machines</a></span></td>
</tr>

<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452612436">DucDung Nguyen</a>,
<a href="author_page.cfm?id=81320490139">TuBao Ho</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 617 - 624</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102429" title="DOI">10.1145/1102351.1102429</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102429&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow78" style="display:inline;"><br /><div style="display:inline">In this paper we describe a new method to reduce the complexity of support vector machines by reducing the number of necessary support vectors included in their solutions. The reduction process iteratively selects two nearest support vectors belonging ...</div></span>
<span id="toHide78" style="display:none;"><br /><div style="display:inline">In this paper we describe a new method to reduce the complexity of support vector machines by reducing the number of necessary support vectors included in their solutions. The reduction process iteratively selects two nearest support vectors belonging to the same class and replaces them by a newly constructed vector. Through the analysis of relation between vectors in the input and feature spaces, we present the construction of new vectors that requires to find the unique maximum point of a one-variable function on the interval (0, 1), not to minimize a function of many variables with local minimums in former reduced set methods. Experimental results on real life datasets show that the proposed method is effective in reducing number of support vectors and preserving machine's generalization performance.</div></span> <a id="expcoll78" href="JavaScript: expandcollapse('expcoll78',78)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102430">Predicting good probabilities with supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309509185">Alexandru Niculescu-Mizil</a>,
<a href="author_page.cfm?id=81100100877">Rich Caruana</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 625 - 632</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102430" title="DOI">10.1145/1102351.1102430</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102430&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
 <tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow79" style="display:inline;"><br /><div style="display:inline">We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding ...</div></span>
<span id="toHide79" style="display:none;"><br /><div style="display:inline">We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.</div></span> <a id="expcoll79" href="JavaScript: expandcollapse('expcoll79',79)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102431">Recycling data for multi-agent learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100347933">Santiago Onta&#241;&#243;n</a>,
<a href="author_page.cfm?id=81100471229">Enric Plaza</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 633 - 640</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102431" title="DOI">10.1145/1102351.1102431</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102431&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow80" style="display:inline;"><br /><div style="display:inline">Learning agents can improve performance cooperating with other agents, particularly learning agents forming a committee outperform individual agents. This "ensemble effect" is well known for multi-classifier systems in Machine Learning. However, multi-classifier ...</div></span>
<span id="toHide80" style="display:none;"><br /><div style="display:inline">Learning agents can improve performance cooperating with other agents, particularly learning agents forming a committee outperform individual agents. This "ensemble effect" is well known for multi-classifier systems in Machine Learning. However, multi-classifier systems assume all data is known to all classifiers while we focus on agents that learn from cases (examples) that are owned and stored individually. In this article we focus on how individual agents can engage in bargaining activities that improve the performance of both individual agents and the committee. The agents are capable of self-evaluation and determining that some data used for learning is unnecessary. This "refuse" data can then be exploited by other agents that might found some part of it profitable to improve their performance. The experiments we performed show that this approach improves both individual and committee performance and we analyze how these results in terms of the "ensemble effect".</div></span> <a id="expcoll80" href="JavaScript: expandcollapse('expcoll80',80)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102432">A graphical model for chord progressions embedded in a psychoacoustic space</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309509945">Jean-Fran&#231;ois Paiement</a>,
<a href="author_page.cfm?id=81100357666">Douglas Eck</a>,
<a href="author_page.cfm?id=81100287032">Samy Bengio</a>,
<a href="author_page.cfm?id=81100340739">David Barber</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 641 - 648</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102432" title="DOI">10.1145/1102351.1102432</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102432&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow81" style="display:inline;"><br /><div style="display:inline">Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term dependencies in music. In this paper, a distributed representation for chords is designed ...</div></span>
<span id="toHide81" style="display:none;"><br /><div style="display:inline">Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term dependencies in music. In this paper, a distributed representation for chords is designed such that Euclidean distances roughly correspond to psychoacoustic dissimilarities. Parameters in the graphical models are learnt with the EM algorithm and the classical Junction Tree algorithm. Various model architectures are compared in terms of conditional out-of-sample likelihood. Both perceptual and statistical evidence show that binary trees related to meter are well suited to capture chord dependencies.</div></span> <a id="expcoll81" href="JavaScript: expandcollapse('expcoll81',81)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102433">Q-learning of sequential attention for visual object recognition from informative local descriptors</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100511045">Lucas Paletta</a>,
<a href="author_page.cfm?id=81100439044">Gerald Fritz</a>,
<a href="author_page.cfm?id=81309511714">Christin Seifert</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 649 - 656</span></td>
</tr>

<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102433" title="DOI">10.1145/1102351.1102433</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102433&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow82" style="display:inline;"><br /><div style="display:inline">This work provides a framework for learning sequential attention in real-world visual object recognition, using an architecture of three processing stages. The first stage rejects irrelevant local descriptors based on an information theoretic saliency ...</div></span>
<span id="toHide82" style="display:none;"><br /><div style="display:inline">This work provides a framework for learning sequential attention in real-world visual object recognition, using an architecture of three processing stages. The first stage rejects irrelevant local descriptors based on an information theoretic saliency measure, providing candidates for foci of interest (FOI). The second stage investigates the information in the FOI using a codebook matcher and providing weak object hypotheses. The third stage integrates local information via shifts of attention, resulting in chains of descriptor-action pairs that characterize object discrimination. A Q-learner adapts then from explorative search and evaluative feedback from entropy decreases on the attention sequences, eventually prioritizing shifts that lead to a geometry of descriptor-action scanpaths that is highly discriminative with respect to object recognition. The methodology is successfully evaluated on indoors (COIL-20 database) and outdoors (TSG-20 database) imagery, demonstrating significant impact by learning, outperforming standard local descriptor based methods both in recognition accuracy and processing time.</div></span> <a id="expcoll82" href="JavaScript: expandcollapse('expcoll82',82)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102434">Discriminative versus generative parameter and structure learning of Bayesian network classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100620209">Franz Pernkopf</a>,
<a href="author_page.cfm?id=81332490392">Jeff Bilmes</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 657 - 664</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102434" title="DOI">10.1145/1102351.1102434</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102434&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow83" style="display:inline;"><br /><div style="display:inline">In this paper, we compare both discriminative and generative parameter learning on both discriminatively and generatively structured Bayesian network classifiers. We use either maximum likelihood (ML) or conditional maximum likelihood (CL) ...</div></span>
<span id="toHide83" style="display:none;"><br /><div style="display:inline">In this paper, we compare both discriminative and generative parameter learning on <i>both</i> discriminatively <i>and</i> generatively structured Bayesian network classifiers. We use either maximum likelihood (ML) or conditional maximum likelihood (CL) to optimize network parameters. For structure learning, we use either conditional mutual information (CMI), the explaining away residual (EAR), or the classification rate (CR) as objective functions. Experiments with the naive Bayes classifier (NB), the tree augmented naive Bayes classifier (TAN), and the <i>Bayesian multinet</i> have been performed on 25 data sets from the UCI repository (Merz et al., 1997) and from (Kohavi & John, 1997). Our empirical study suggests that discriminative structures learnt using CR produces the most accurate classifiers on almost half the data sets. This approach is feasible, however, only for rather small problems since it is computationally expensive. Discriminative parameter learning produces on average a better classifier than ML parameter learning.</div></span> <a id="expcoll83" href="JavaScript: expandcollapse('expcoll83',83)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102435">Optimizing abstaining classifiers using ROC analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309494181">Tadeusz Pietraszek</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 665 - 672</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102435" title="DOI">10.1145/1102351.1102435</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102435&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow84" style="display:inline;"><br /><div style="display:inline">Classifiers that refrain from classification in certain cases can significantly reduce the misclassification cost. However, the parameters for such abstaining classifiers are often set in a rather ad-hoc manner. We propose a method to optimally build ...</div></span>
<span id="toHide84" style="display:none;"><br /><div style="display:inline">Classifiers that refrain from classification in certain cases can significantly reduce the misclassification cost. However, the parameters for such abstaining classifiers are often set in a rather ad-hoc manner. We propose a method to optimally build a specific type of abstaining binary classifiers using ROC analysis. These classifiers are built based on optimization criteria in the following three models: cost-based, bounded-abstention and bounded-improvement. We demonstrate the usage and applications of these models to effectively reduce misclassification cost in real classification systems. The method has been validated with a ROC building algorithm and cross-validation on 15 UCI KDD datasets.</div></span> <a id="expcoll84" href="JavaScript: expandcollapse('expcoll84',84)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102436">Independent subspace analysis using geodesic spanning trees</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309498613">Barnab&#225;s P&#243;czos</a>,
<a href="author_page.cfm?id=81100130503">Andr&#225;s L&#245;rincz</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 673 - 680</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102436" title="DOI">10.1145/1102351.1102436</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102436&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow85" style="display:inline;"><br /><div style="display:inline">A novel algorithm for performing Independent Subspace Analysis, the estimation of hidden independent subspaces is introduced. This task is a generalization of Independent Component Analysis. The algorithm works by estimating the multi-dimensional differential ...</div></span>
<span id="toHide85" style="display:none;"><br /><div style="display:inline">A novel algorithm for performing Independent Subspace Analysis, the estimation of hidden independent subspaces is introduced. This task is a generalization of Independent Component Analysis. The algorithm works by estimating the multi-dimensional differential entropy. The estimation utilizes minimal geodesic spanning trees matched to the sample points. Numerical studies include (i) illustrative examples, (ii) a generalization of the cocktail-party problem to songs played by bands, and (iii) an example on mixed independent subspaces, where subspaces have dependent sources, which are pairwise independent.</div></span> <a id="expcoll85" href="JavaScript: expandcollapse('expcoll85',85)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102437">A model for handling approximate, noisy or incomplete labeling in text classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100289098">Ganesh Ramakrishnan</a>,
<a href="author_page.cfm?id=81100374405">Krishna Prasad Chitrapura</a>,
<a href="author_page.cfm?id=81100282550">Raghu Krishnapuram</a>,
<a href="author_page.cfm?id=81331488601">Pushpak Bhattacharyya</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 681 - 688</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102437" title="DOI">10.1145/1102351.1102437</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102437&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow86" style="display:inline;"><br /><div style="display:inline">We introduce a Bayesian model, BayesANIL, that is capable of estimating uncertainties associated with the labeling process. Given a labeled or partially labeled training corpus of text documents, the model estimates the joint distribution of training ...</div></span>
<span id="toHide86" style="display:none;"><br /><div style="display:inline">We introduce a Bayesian model, BayesANIL, that is capable of estimating uncertainties associated with the labeling process. Given a labeled or partially labeled training corpus of text documents, the model estimates the joint distribution of training documents and class labels by using a generalization of the Expectation Maximization algorithm. The estimates can be used in standard classification models to reduce error rates. Since uncertainties in the labeling are taken into account, the model provides an elegant mechanism to deal with noisy labels. We provide an intuitive modification to the EM iterations by re-estimating the empirical. distribution in order to reinforce feature values in unlabeled data and to reduce the influence of noisily labeled examples. Considerable improvement in the classification accuracies of two popular classification algorithms on standard labeled data-sets with and without artificially introduced noise, as well as in the presence and absence of unlabeled data, indicates that this may be a promising method to reduce the burden of manual labeling.</div></span> <a id="expcoll86" href="JavaScript: expandcollapse('expcoll86',86)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102438">Healing the relevance vector machine through augmentation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309509063">Carl Edward Rasmussen</a>,
<a href="author_page.cfm?id=81321496751">Joaquin Qui&#241;onero-Candela</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 689 - 696</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102438" title="DOI">10.1145/1102351.1102438</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102438&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow87" style="display:inline;"><br /><div style="display:inline">The Relevance Vector Machine (RVM) is a sparse approximate Bayesian kernel method. It provides full predictive distributions for test cases. However, the predictive uncertainties have the unintuitive property, that they get smaller the further you move ...</div></span>
<span id="toHide87" style="display:none;"><br /><div style="display:inline">The Relevance Vector Machine (RVM) is a sparse approximate Bayesian kernel method. It provides full predictive distributions for test cases. However, the predictive uncertainties have the unintuitive property, that they get smaller the further you move away from the training cases. We give a thorough analysis. Inspired by the analogy to non-degenerate Gaussian Processes, we suggest augmentation to solve the problem. The purpose of the resulting model, RVM*, is primarily to corroborate the theoretical and experimental analysis. Although RVM* could be used in practical applications, it is no longer a truly sparse model. Experiments show that sparsity comes at the expense of worse predictive. distributions.</div></span> <a id="expcoll87" href="JavaScript: expandcollapse('expcoll87',87)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102439">Supervised versus multiple instance learning: an empirical comparison</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100552505">Soumya Ray</a>,
<a href="author_page.cfm?id=81100554243">Mark Craven</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 697 - 704</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102439" title="DOI">10.1145/1102351.1102439</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102439&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow88" style="display:inline;"><br /><div style="display:inline">We empirically study the relationship between supervised and multiple instance (MI) learning. Algorithms to learn various concepts have been adapted to the MI representation. However, it is also known that concepts that are PAC-learnable with one-sided ...</div></span>
<span id="toHide88" style="display:none;"><br /><div style="display:inline">We empirically study the relationship between supervised and multiple instance (MI) learning. Algorithms to learn various concepts have been adapted to the MI representation. However, it is also known that concepts that are PAC-learnable with one-sided noise can be learned from MI data. A relevant question then is: how well do supervised learners do on MI data? We attempt to answer this question by looking at a cross section of MI data sets from various domains coupled with a number of learning algorithms including Diverse Density, Logistic Regression, nonlinear Support Vector Machines and FOIL. We consider a supervised and MI version of each learner. Several interesting conclusions emerge from our work: (1) no MI algorithm is superior across all tested domains, (2) some MI algorithms are consistently superior to their supervised counterparts, (3) using high false-positive costs can improve a supervised learner's performance in MI domains, and (4) in several domains, a supervised algorithm is superior to any MI algorithm we tested.</div></span> <a id="expcoll88" href="JavaScript: expandcollapse('expcoll88',88)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102440">Generalized skewing for functions with continuous and nominal attributes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100552505">Soumya Ray</a>,
<a href="author_page.cfm?id=81405593471">David Page</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 705 - 712</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102440" title="DOI">10.1145/1102351.1102440</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102440&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow89" style="display:inline;"><br /><div style="display:inline">This paper extends previous work on skewing, an approach to problematic functions in decision tree induction. The previous algorithms were applicable only to functions of binary variables. In this paper, we extend skewing to directly handle functions ...</div></span>
<span id="toHide89" style="display:none;"><br /><div style="display:inline">This paper extends previous work on <i>skewing</i>, an approach to problematic functions in decision tree induction. The previous algorithms were applicable only to functions of binary variables. In this paper, we extend skewing to directly handle functions of continuous and nominal variables. We present experiments with randomly generated functions and a number of real world datasets to evaluate the algorithm's accuracy. Our results indicate that our algorithm almost always outperforms an Information Gain-based decision tree learner.</div></span> <a id="expcoll89" href="JavaScript: expandcollapse('expcoll89',89)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102441">Fast maximum margin matrix factorization for collaborative prediction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100440298">Jasson D. M. Rennie</a>,
<a href="author_page.cfm?id=81100005542">Nathan Srebro</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 713 - 719</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102441" title="DOI">10.1145/1102351.1102441</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102441&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow90" style="display:inline;"><br /><div style="display:inline">Maximum Margin Matrix Factorization (MMMF) was recently suggested (Srebro et al., 2005) as a convex, infinite dimensional alternative to low-rank approximations and standard factor models. MMMF can be formulated as a semi-definite programming (SDP) and ...</div></span>
<span id="toHide90" style="display:none;"><br /><div style="display:inline">Maximum Margin Matrix Factorization (MMMF) was recently suggested (Srebro et al., 2005) as a convex, infinite dimensional alternative to low-rank approximations and standard factor models. MMMF can be formulated as a semi-definite programming (SDP) and learned using standard SDP solvers. However, current SDP solvers can only handle MMMF problems on matrices of dimensionality up to a few hundred. Here, we investigate a direct gradient-based optimization method for MMMF and demonstrate it on large collaborative prediction problems. We compare against results obtained by Marlin (2004) and find that MMMF substantially outperforms all nine methods he tested.</div></span> <a id="expcoll90" href="JavaScript: expandcollapse('expcoll90',90)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102442">Coarticulation: an approach for generating concurrent plans in Markov decision processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100432211">Khashayar Rohanimanesh</a>,
<a href="author_page.cfm?id=81100132345">Sridhar Mahadevan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 720 - 727</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102442" title="DOI">10.1145/1102351.1102442</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102442&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow91" style="display:inline;"><br /><div style="display:inline">We study an approach for performing concurrent activities in Markov decision processes (MDPs) based on the coarticulation framework. We assume that the agent has multiple degrees of freedom (DOF) in the action space which enables it to perform activities ...</div></span>
<span id="toHide91" style="display:none;"><br /><div style="display:inline">We study an approach for performing concurrent activities in Markov decision processes (MDPs) based on the coarticulation framework. We assume that the agent has multiple degrees of freedom (DOF) in the action space which enables it to perform activities simultaneously. We demonstrate that one natural way for generating concurrency in the system is by coarticulating among the set of learned activities available to the agent. In general due to the multiple DOF in the system, often there exists a redundant set of admissible sub-optimal policies associated with each learned activity. Such flexibility enables the agent to concurrently commit to several subgoals according to their priority levels, given a new task defined in terms of a set of prioritized subgoals. We present efficient approximate algorithms for computing such policies and for generating concurrent plans. We also evaluate our approach in a simulated domain.</div></span> <a id="expcoll91" href="JavaScript: expandcollapse('expcoll91',91)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102443">Why skewing works: learning difficult Boolean functions with greedy tree learners</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309502437">Bernard Rosell</a>,
<a href="author_page.cfm?id=81100338606">Lisa Hellerstein</a>,
<a href="author_page.cfm?id=81100552505">Soumya Ray</a>,
<a href="author_page.cfm?id=81405593471">David Page</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 728 - 735</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102443" title="DOI">10.1145/1102351.1102443</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102443&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow92" style="display:inline;"><br /><div style="display:inline">We analyze skewing, an approach that has been empirically observed to enable greedy decision tree learners to learn "difficult" Boolean functions, such as parity, in the presence of irrelevant variables. We prove tha, in an idealized setting, ...</div></span>
<span id="toHide92" style="display:none;"><br /><div style="display:inline">We analyze <i>skewing</i>, an approach that has been empirically observed to enable greedy decision tree learners to learn "difficult" Boolean functions, such as parity, in the presence of irrelevant variables. We prove tha, in an idealized setting, for any function and choice of skew parameters, skewing finds relevant variables with probability 1. We present experiments exploring how different parameter choices affect the success of skewing in empirical settings. Finally, we analyze a variant of skewing called Sequential Skewing.</div></span> <a id="expcoll92" href="JavaScript: expandcollapse('expcoll92',92)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102444">Integer linear programming inference for conditional random fields</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100201453">Dan Roth</a>,
<a href="author_page.cfm?id=81309493022">Wen-tau Yih</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 736 - 743</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102444" title="DOI">10.1145/1102351.1102444</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102444&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow93" style="display:inline;"><br /><div style="display:inline">Inference in Conditional Random Fields and Hidden Markov Models is done using the Viterbi algorithm, an efficient dynamic programming algorithm. In many cases, general (non-local and non-sequential) constraints may exist over the output sequence, but ...</div></span>
<span id="toHide93" style="display:none;"><br /><div style="display:inline">Inference in Conditional Random Fields and Hidden Markov Models is done using the Viterbi algorithm, an efficient dynamic programming algorithm. In many cases, general (non-local and non-sequential) constraints may exist over the output sequence, but cannot be incorporated and exploited in a natural way by this inference procedure. This paper proposes a novel inference procedure based on integer linear programming (ILP) and extends CRF models to naturally and efficiently support general constraint structures. For sequential constraints, this procedure reduces to simple linear programming as the inference process. Experimental evidence is supplied in the context of an important NLP problem, semantic role labeling.</div></span> <a id="expcoll93" href="JavaScript: expandcollapse('expcoll93',93)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102445">Learning hierarchical multi-category text classification models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100389431">Juho Rousu</a>,
<a href="author_page.cfm?id=81100465526">Craig Saunders</a>,
<a href="author_page.cfm?id=81325489999">Sandor Szedmak</a>,
<a href="author_page.cfm?id=81310500608">John Shawe-Taylor</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 744 - 751</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102445" title="DOI">10.1145/1102351.1102445</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102445&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow94" style="display:inline;"><br /><div style="display:inline">We present a kernel-based algorithm for hierarchical text classification where the documents are allowed to belong to more than one category at a time. The classification model is a variant of the Maximum Margin Markov Network framework, where the classification ...</div></span>
<span id="toHide94" style="display:none;"><br /><div style="display:inline">We present a kernel-based algorithm for hierarchical text classification where the documents are allowed to belong to more than one category at a time. The classification model is a variant of the Maximum Margin Markov Network framework, where the classification hierarchy is represented as a Markov tree equipped with an exponential family defined on the edges. We present an efficient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classification hierarchies consisting of hundreds of nodes. The algorithm's predictive accuracy is competitive with other recently introduced hierarchical multi-category or multilabel classification learning algorithms.</div></span> <a id="expcoll94" href="JavaScript: expandcollapse('expcoll94',94)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102446">Expectation maximization algorithms for conditional likelihoods</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100235087">Jarkko Saloj&#228;rvi</a>,
<a href="author_page.cfm?id=81317489564">Kai Puolam&#228;ki</a>,
<a href="author_page.cfm?id=81100348810">Samuel Kaski</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 752 - 759</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102446" title="DOI">10.1145/1102351.1102446</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102446&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow95" style="display:inline;"><br /><div style="display:inline">We introduce an expectation maximization-type (EM) algorithm for maximum likelihood optimization of conditional densities. It is applicable to hidden variable models where the distributions are from the exponential family. The algorithm can alternatively ...</div></span>
<span id="toHide95" style="display:none;"><br /><div style="display:inline">We introduce an expectation maximization-type (EM) algorithm for maximum likelihood optimization of conditional densities. It is applicable to hidden variable models where the distributions are from the exponential family. The algorithm can alternatively be viewed as automatic step size selection for gradient ascent, where the amount of computation is traded off to guarantees that each step increases the likelihood. The tradeoff makes the algorithm computationally more feasible than the earlier conditional EM. The method gives a theoretical basis for extended Baum Welch algorithms used in discriminative hidden Markov models in speech recognition, and compares favourably with the current best method in the experiments.</div></span> <a id="expcoll95" href="JavaScript: expandcollapse('expcoll95',95)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102447">Estimating and computing density based distance metrics</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100277117"> Sajama</a>,
<a href="author_page.cfm?id=81100458771">Alon Orlitsky</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 760 - 767</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102447" title="DOI">10.1145/1102351.1102447</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102447&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow96" style="display:inline;"><br /><div style="display:inline">Density-based distance metrics have applications in semi-supervised learning, nonlinear interpolation and clustering. We consider density-based metrics induced by Riemannian manifold structures and estimate them using kernel density estimators for the ...</div></span>
<span id="toHide96" style="display:none;"><br /><div style="display:inline">Density-based distance metrics have applications in semi-supervised learning, nonlinear interpolation and clustering. We consider density-based metrics induced by Riemannian manifold structures and estimate them using kernel density estimators for the underlying data distribution. We lower bound the rate of convergence of these plug-in path-length estimates and hence of the metric, as the sample size increases. We present an upper bound on the rate of convergence of all estimators of the metric. We also show that the metric can be consistently computed using the shortest path algorithm on a suitably constructed graph on the data samples and lower bound the convergence rate of the computation error. We present experiments illustrating the use of the metrics for semi-supervised classification and non-linear interpolation.</div></span> <a id="expcoll96" href="JavaScript: expandcollapse('expcoll96',96)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102448">Supervised dimensionality reduction using mixture models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100277117"> Sajama</a>,
<a href="author_page.cfm?id=81100458771">Alon Orlitsky</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 768 - 775</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102448" title="DOI">10.1145/1102351.1102448</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102448&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow97" style="display:inline;"><br /><div style="display:inline">Given a classification problem, our goal is to find a low-dimensional linear transformation of the feature vectors which retains information needed to predict the class labels. We present a method based on maximum conditional likelihood estimation of ...</div></span>
<span id="toHide97" style="display:none;"><br /><div style="display:inline">Given a classification problem, our goal is to find a low-dimensional linear transformation of the feature vectors which retains information needed to predict the class labels. We present a method based on maximum conditional likelihood estimation of mixture models. Use of mixture models allows us to approximate the distributions to any desired accuracy while use of conditional likelihood as the contrast function ensures that the selected subspace retains maximum possible mutual information between feature vectors and class labels. Classification experiments using Gaussian mixture components show that this method compares favorably to related dimension reduction techniques. Other distributions belonging to the exponential family can be used to reduce dimensions when data is of a special type, for example binary or integer valued data. We provide an EM-like algorithm for model estimation and present visualization experiments using Gaussian and Bernoulli mixture models.</div></span> <a id="expcoll97" href="JavaScript: expandcollapse('expcoll97',97)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102449">Object correspondence as a machine learning problem</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>,
<a href="author_page.cfm?id=81100282827">Florian Steinke</a>,
<a href="author_page.cfm?id=81100351857">Volker Blanz</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 776 - 783</span></td>
</tr>
<tr>
<td></td>
 <td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102449" title="DOI">10.1145/1102351.1102449</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102449&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow98" style="display:inline;"><br /><div style="display:inline">We propose machine learning methods for the estimation of deformation fields that transform two given objects into each other, thereby establishing a dense point to point correspondence. The fields are computed using a modified support vector machine ...</div></span>
<span id="toHide98" style="display:none;"><br /><div style="display:inline">We propose machine learning methods for the estimation of deformation fields that transform two given objects into each other, thereby establishing a dense point to point correspondence. The fields are computed using a modified support vector machine containing a penalty enforcing that points of one object will be mapped to "similar" points on the other one. Our system, which contains little engineering or domain knowledge, delivers state of the art performance. We present application results including close to photorealistic morphs of 3D head models.</div></span> <a id="expcoll98" href="JavaScript: expandcollapse('expcoll98',98)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102450">Analysis and extension of spectral methods for nonlinear dimensionality reduction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100436576">Fei Sha</a>,
<a href="author_page.cfm?id=81100590935">Lawrence K. Saul</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 784 - 791</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102450" title="DOI">10.1145/1102351.1102450</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102450&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow99" style="display:inline;"><br /><div style="display:inline">Many unsupervised algorithms for nonlinear dimensionality reduction, such as locally linear embedding (LLE) and Laplacian eigenmaps, are derived from the spectral decompositions of sparse matrices. While these algorithms aim to preserve certain proximity ...</div></span>
<span id="toHide99" style="display:none;"><br /><div style="display:inline">Many unsupervised algorithms for nonlinear dimensionality reduction, such as locally linear embedding (LLE) and Laplacian eigenmaps, are derived from the spectral decompositions of sparse matrices. While these algorithms aim to preserve certain proximity relations on average, their embeddings are not explicitly designed to preserve local features such as distances or angles. In this paper, we show how to construct a low dimensional embedding that maximally preserves angles between nearby data points. The embedding is derived from the bottom eigenvectors of LLE and/or Laplacian eigenmaps by solving an additional (but small) problem in semidefinite programming, whose size is independent of the number of data points. The solution obtained by semidefinite programming also yields an estimate of the data's intrinsic dimensionality. Experimental results on several data sets demonstrate the merits of our approach.</div></span> <a id="expcoll99" href="JavaScript: expandcollapse('expcoll99',99)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102451">Non-negative tensor factorization with applications to statistics and computer vision</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100471517">Amnon Shashua</a>,
<a href="author_page.cfm?id=81309494677">Tamir Hazan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 792 - 799</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102451" title="DOI">10.1145/1102351.1102451</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102451&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow100" style="display:inline;"><br /><div style="display:inline">We derive algorithms for finding a non-negative n-dimensional tensor factorization (n-NTF) which includes the non-negative matrix factorization (NMF) as a particular case when n = 2. We motivate the use of n-NTF in three areas ...</div></span>
<span id="toHide100" style="display:none;"><br /><div style="display:inline">We derive algorithms for finding a non-negative <i>n</i>-dimensional tensor factorization (<i>n</i>-NTF) which includes the non-negative matrix factorization (NMF) as a particular case when <i>n</i> = 2. We motivate the use of <i>n</i>-NTF in three areas of data analysis: (i) connection to latent class models in statistics, (ii) sparse image coding in computer vision, and (iii) model selection problems. We derive a "direct" positive-preserving gradient descent algorithm and an alternating scheme based on repeated multiple rank-1 problems.</div></span> <a id="expcoll100" href="JavaScript: expandcollapse('expcoll100',100)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102452">Fast inference and learning in large-state-space HMMs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309501414">Sajid M. Siddiqi</a>,
<a href="author_page.cfm?id=81100042782">Andrew W. Moore</a>
</span>
</td>
</tr>
 <tr>
<td></td>
<td> <span style="padding-left:0">Pages: 800 - 807</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102452" title="DOI">10.1145/1102351.1102452</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102452&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow101" style="display:inline;"><br /><div style="display:inline">For Hidden Markov Models (HMMs) with fully connected transition models, the three fundamental problems of evaluating the likelihood of an observation sequence, estimating an optimal state sequence for the observations, and learning the model parameters, ...</div></span>
<span id="toHide101" style="display:none;"><br /><div style="display:inline">For Hidden Markov Models (HMMs) with fully connected transition models, the three fundamental problems of evaluating the likelihood of an observation sequence, estimating an optimal state sequence for the observations, and learning the model parameters, all have quadratic time complexity in the number of states. We introduce a novel class of non-sparse Markov transition matrices called Dense-Mostly-Constant (DMC) transition matrices that allow us to derive new algorithms for solving the basic HMM problems in sub-quadratic time. We describe the DMC HMM model and algorithms and attempt to convey some intuition for their usage. Empirical results for these algorithms show dramatic speedups for all three problems. In terms of accuracy, the DMC model yields strong results and outperforms the baseline algorithms even in domains known to violate the DMC assumption.</div></span> <a id="expcoll101" href="JavaScript: expandcollapse('expcoll101',101)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102453">New d-separation identification results for learning continuous latent variable models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81406593387">Ricardo Silva</a>,
<a href="author_page.cfm?id=81100130665">Richard Scheines</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 808 - 815</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102453" title="DOI">10.1145/1102351.1102453</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102453&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow102" style="display:inline;"><br /><div style="display:inline">Learning the structure of graphical models is an important task, but one of considerable difficulty when latent variables are involved. Because conditional independences using hidden variables cannot be directly observed, one has to rely on alternative ...</div></span>
<span id="toHide102" style="display:none;"><br /><div style="display:inline">Learning the structure of graphical models is an important task, but one of considerable difficulty when latent variables are involved. Because conditional independences using hidden variables cannot be directly observed, one has to rely on alternative methods to identify the d-separations that define the graphical structure. This paper describes new distribution-free techniques for identifying d-separations in continuous latent variable models when non-linear dependencies are allowed among hidden variables.</div></span> <a id="expcoll102" href="JavaScript: expandcollapse('expcoll102',102)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102454">Identifying useful subgoals in reinforcement learning by local graph partitioning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100378234">&#214;zg&#252;r &#350;im&#351;ek</a>,
<a href="author_page.cfm?id=81309508299">Alicia P. Wolfe</a>,
<a href="author_page.cfm?id=81100207208">Andrew G. Barto</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 816 - 823</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102454" title="DOI">10.1145/1102351.1102454</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102454&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow103" style="display:inline;"><br /><div style="display:inline">We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning local state transition graphs---those that are constructed using only the most recent experiences ...</div></span>
<span id="toHide103" style="display:none;"><br /><div style="display:inline">We present a new subgoal-based method for automatically creating useful skills in reinforcement learning. Our method identifies subgoals by partitioning <i>local</i> state transition graphs---those that are constructed using only the most recent experiences of the agent. The local scope of our subgoal discovery method allows it to successfully identify the type of subgoals we seek---states that lie between two densely-connected regions of the state space while producing an algorithm with low computational cost.</div></span> <a id="expcoll103" href="JavaScript: expandcollapse('expcoll103',103)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102455">Beyond the point cloud: from transductive to semi-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
 <a href="author_page.cfm?id=81309499649">Vikas Sindhwani</a>,
<a href="author_page.cfm?id=81100261675">Partha Niyogi</a>,
<a href="author_page.cfm?id=81100386814">Mikhail Belkin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 824 - 831</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102455" title="DOI">10.1145/1102351.1102455</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102455&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow104" style="display:inline;"><br /><div style="display:inline">Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive ...</div></span>
<span id="toHide104" style="display:none;"><br /><div style="display:inline">Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the <i>transductive</i> setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly <i>semi-supervised</i> setting however, a learning machine has access to labeled and unlabeled examples and must make predictions on data points never encountered before. In this paper, we show how to turn transductive and standard supervised learning algorithms into semi-supervised learners. We construct a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS). These norms allow us to warp the structure of the RKHS to reflect the underlying geometry of the data. We derive explicit formulas for the corresponding new kernels. Our approach demonstrates state of the art performance on a variety of classification tasks.</div></span> <a id="expcoll104" href="JavaScript: expandcollapse('expcoll104',104)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102456">Active learning for sampling in time-series experiments with application to gene expression analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81460647236">Rohit Singh</a>,
<a href="author_page.cfm?id=81538580056">Nathan Palmer</a>,
<a href="author_page.cfm?id=81100151844">David Gifford</a>,
<a href="author_page.cfm?id=81100228742">Bonnie Berger</a>,
<a href="author_page.cfm?id=81100368044">Ziv Bar-Joseph</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 832 - 839</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102456" title="DOI">10.1145/1102351.1102456</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102456&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow105" style="display:inline;"><br /><div style="display:inline">Many time-series experiments seek to estimate some signal as a continuous function of time. In this paper, we address the sampling problem for such experiments: determining which time-points ought to be sampled in order to minimize the cost of ...</div></span>
<span id="toHide105" style="display:none;"><br /><div style="display:inline">Many time-series experiments seek to estimate some signal as a continuous function of time. In this paper, we address the <i>sampling problem</i> for such experiments: determining which time-points ought to be sampled in order to minimize the cost of data collection. We restrict our attention to a growing class of experiments which measure multiple signals at each time-point and where raw materials/observations are archived initially, and selectively analyzed later, this analysis being the more expensive step. We present an active learning algorithm for iteratively choosing time-points to sample, using the uncertainty in the quality of the currently estimated time-dependent curve as the objective function. Using simulated data as well as gene expression data, we show that our algorithm performs well, and can significantly reduce experimental cost without loss of information.</div></span> <a id="expcoll105" href="JavaScript: expandcollapse('expcoll105',105)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102457">Compact approximations to Bayesian predictive distributions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309492467">Edward Snelson</a>,
<a href="author_page.cfm?id=81100572858">Zoubin Ghahramani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 840 - 847</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102457" title="DOI">10.1145/1102351.1102457</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102457&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow106" style="display:inline;"><br /><div style="display:inline">We provide a general framework for learning precise, compact, and fast representations of the Bayesian predictive distribution for a model. This framework is based on minimizing the KL divergence between the true predictive density and a suitable compact ...</div></span>
<span id="toHide106" style="display:none;"><br /><div style="display:inline">We provide a general framework for learning precise, compact, and fast representations of the Bayesian predictive distribution for a model. This framework is based on minimizing the KL divergence between the true predictive density and a suitable compact approximation. We consider various methods for doing this, both sampling based approximations, and deterministic approximations such as expectation propagation. These methods are tested on a mixture of Gaussians model for density estimation and on binary linear classification, with both synthetic data sets for visualization and several real data sets. Our results show significant reductions in prediction time and memory footprint.</div></span> <a id="expcoll106" href="JavaScript: expandcollapse('expcoll106',106)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102458">Large scale genomic sequence SVM classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81330498803">S&#246;ren Sonnenburg</a>,
<a href="author_page.cfm?id=81100288296">Gunnar R&#228;tsch</a>,
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 848 - 855</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102458" title="DOI">10.1145/1102351.1102458</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102458&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow107" style="display:inline;"><br /><div style="display:inline">In genomic sequence analysis tasks like splice site recognition or promoter identification, large amounts of training sequences are available, and indeed needed to achieve sufficiently high classification performances. In this work we study two recently ...</div></span>
<span id="toHide107" style="display:none;"><br /><div style="display:inline">In genomic sequence analysis tasks like splice site recognition or promoter identification, large amounts of training sequences are available, and indeed needed to achieve sufficiently high classification performances. In this work we study two recently proposed and successfully used kernels, namely the <i>Spectrum kernel</i> and the <i>Weighted Degree kernel</i> (WD). In particular, we suggest several extensions using Suffix Trees and modifications of an SMO-like SVM training algorithm in order to accelerate the training of the SVMs and their evaluation on test sequences. Our simulations show that for the spectrum kernel and WD kernel, large scale SVM training can be accelerated by factors of 20 and 4 times, respectively, while using much less memory (e.g. no kernel caching). The evaluation on new sequences is often several thousand times faster using the new techniques (depending on the number of Support Vectors). Our method allows us to train on sets as large as one million sequences.</div></span> <a id="expcoll107" href="JavaScript: expandcollapse('expcoll107',107)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102459">A theoretical analysis of Model-Based Interval Estimation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100216859">Alexander L. Strehl</a>,
<a href="author_page.cfm?id=81406601119">Michael L. Littman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 856 - 863</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102459" title="DOI">10.1145/1102351.1102459</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102459&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow108" style="display:inline;"><br /><div style="display:inline">Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing ...</div></span>
<span id="toHide108" style="display:none;"><br /><div style="display:inline">Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents the first theoretical analysis of MBIE, proving its efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less "online" cousins from the literature.</div></span> <a id="expcoll108" href="JavaScript: expandcollapse('expcoll108',108)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102460">Explanation-Augmented SVM: an approach to incorporating domain knowledge into SVM learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81539839356">Qiang Sun</a>,
<a href="author_page.cfm?id=81100619998">Gerald DeJong</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 864 - 871</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102460" title="DOI">10.1145/1102351.1102460</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102460&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow109" style="display:inline;"><br /><div style="display:inline">We introduce a novel approach to incorporating domain knowledge into Support Vector Machines to improve their example efficiency. Domain knowledge is used in an Explanation Based Learning fashion to build justifications or explanations for why the training ...</div></span>
<span id="toHide109" style="display:none;"><br /><div style="display:inline">We introduce a novel approach to incorporating domain knowledge into Support Vector Machines to improve their example efficiency. Domain knowledge is used in an Explanation Based Learning fashion to build justifications or explanations for why the training examples are assigned their given class labels. Explanations bias the large margin classifier through the interaction of training examples and domain knowledge. We develop a new learning algorithm for this Explanation-Augmented SVM (EA-SVM). It naturally extends to imperfect knowledge, a stumbling block to conventional EBL. Experimental results confirm desirable properties predicted by the analysis and demonstrate the approach on three domains.</div></span> <a id="expcoll109" href="JavaScript: expandcollapse('expcoll109',109)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102461">Unifying the error-correcting and output-code AdaBoost within the margin framework</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309512519">Yijun Sun</a>,
<a href="author_page.cfm?id=81324494311">Sinisa Todorovic</a>,
<a href="author_page.cfm?id=81100486125">Jian Li</a>,
<a href="author_page.cfm?id=81309495208">Dapeng Wu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 872 - 879</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102461" title="DOI">10.1145/1102351.1102461</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102461&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow110" style="display:inline;"><br /><div style="display:inline">In this paper, we present a new interpretation of AdaBoost.ECC and AdaBoost.OC. We show that AdaBoost.ECC performs stage-wise functional gradient descent on a cost function, defined in the domain of margin values, and that AdaBoost.OC is a shrinkage ...</div></span>
<span id="toHide110" style="display:none;"><br /><div style="display:inline">In this paper, we present a new interpretation of AdaBoost.ECC and AdaBoost.OC. We show that AdaBoost.ECC performs stage-wise functional gradient descent on a cost function, defined in the domain of margin values, and that AdaBoost.OC is a shrinkage version of AdaBoost.ECC. These findings strictly explain some properties of the two algorithms. The gradient-minimization formulation of AdaBoost.ECC allows us to derive a new algorithm, referred to as AdaBoost.SECC, by explicitly exploiting shrinkage as regularization in AdaBoost.ECC. Experiments on diverse databases confirm our theoretical findings. Empirical results show that AdaBoost.SECC performs significantly better than AdaBoost.ECC and AdaBoost.OC.</div></span> <a id="expcoll110" href="JavaScript: expandcollapse('expcoll110',110)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102462">Finite time bounds for sampling based fitted value iteration</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100538163">Csaba Szepesv&#225;ri</a>,
<a href="author_page.cfm?id=81100226944">R&#233;mi Munos</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 880 - 887</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102462" title="DOI">10.1145/1102351.1102462</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102462&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow111" style="display:inline;"><br /><div style="display:inline">In this paper we consider sampling based fitted value iteration for discounted, large (possibly infinite) state space, finite action Markovian Decision Problems where only a generative model of the transition probabilities and rewards is available. At ...</div></span>
<span id="toHide111" style="display:none;"><br /><div style="display:inline">In this paper we consider sampling based fitted value iteration for discounted, large (possibly infinite) state space, finite action Markovian Decision Problems where only a generative model of the transition probabilities and rewards is available. At each step the image of the current estimate of the optimal value function under a Monte-Carlo approximation to the Bellman-operator is projected onto some function space. PAC-style bounds on the weighted <i>L</i><sup><i>p</i></sup>-norm approximation error are obtained as a function of the covering number and the approximation power of the function space, the iteration number and the sample size.</div></span> <a id="expcoll111" href="JavaScript: expandcollapse('expcoll111',111)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102463">TD(&lambda;) networks: temporal-difference networks with eligibility traces</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309497538">Brian Tanner</a>,
<a href="author_page.cfm?id=81342513055">Richard S. Sutton</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 888 - 895</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102463" title="DOI">10.1145/1102351.1102463</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102463&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow112" style="display:inline;"><br /><div style="display:inline">Temporal-difference (TD) networks have been introduced as a formalism for expressing and learning grounded world knowledge in a predictive form (Sutton & Tanner, 2005). Like conventional TD(0) methods, the learning algorithm for TD networks uses 1-step ...</div></span>
<span id="toHide112" style="display:none;"><br /><div style="display:inline">Temporal-difference (TD) networks have been introduced as a formalism for expressing and learning grounded world knowledge in a predictive form (Sutton & Tanner, 2005). Like conventional TD(0) methods, the learning algorithm for TD networks uses 1-step backups to train prediction units about future events. In conventional TD learning, the TD(&lambda;) algorithm is often used to do more general multi-step backups of future predictions. In our work, we introduce a generalization of the 1-step TD network specification that is based on the TD(&lambda;) learning algorithm, creating TD(&lambda;) networks. We present experimental results that show TD(&lambda;) networks can learn solutions in more complex environments than TD networks. We also show that in problems that can be solved by TD networks, TD(&lambda;) networks generally learn solutions much faster than their 1-step counterparts. Finally, we present an analysis of our algorithm that shows that the computational cost of TD(&lambda;) networks is only slightly more than that of TD networks.</div></span> <a id="expcoll112" href="JavaScript: expandcollapse('expcoll112',112)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102464">Learning structured prediction models: a large margin approach</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100453688">Ben Taskar</a>,
<a href="author_page.cfm?id=81100204858">Vassil Chatalbashev</a>,
<a href="author_page.cfm?id=81100246010">Daphne Koller</a>,
<a href="author_page.cfm?id=81100629945">Carlos Guestrin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 896 - 903</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102464" title="DOI">10.1145/1102351.1102464</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102464&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow113" style="display:inline;"><br /><div style="display:inline">We consider large margin estimation in a broad range of prediction models where inference involves solving combinatorial optimization problems, for example, weighted graph-cuts or matchings. Our goal is to learn parameters such that inference using the ...</div></span>
<span id="toHide113" style="display:none;"><br /><div style="display:inline">We consider large margin estimation in a broad range of prediction models where inference involves solving combinatorial optimization problems, for example, weighted graph-cuts or matchings. Our goal is to learn parameters such that inference using the model reproduces correct answers on the training data. Our method relies on the expressive power of convex optimization problems to compactly capture inference or solution optimality in structured prediction models. Directly embedding this structure within the learning formulation produces concise convex problems for efficient estimation of very complex and diverse models. We describe experimental results on a matching task, disulfide connectivity prediction, showing significant improvements over state-of-the-art methods.</div></span> <a id="expcoll113" href="JavaScript: expandcollapse('expcoll113',113)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102465">Learning discontinuities with products-of-sigmoids for switching between local models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331505904">Marc Toussaint</a>,
<a href="author_page.cfm?id=81100170884">Sethu Vijayakumar</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 904 - 911</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102465" title="DOI">10.1145/1102351.1102465</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102465&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow114" style="display:inline;"><br /><div style="display:inline">Sensorimotor data from many interesting physical interactions comprises discontinuities. While existing locally weighted learning approaches aim at learning smooth functions, we propose a model that learns how to switch discontinuously between local ...</div></span>
<span id="toHide114" style="display:none;"><br /><div style="display:inline">Sensorimotor data from many interesting physical interactions comprises discontinuities. While existing locally weighted learning approaches aim at learning smooth functions, we propose a model that learns how to switch discontinuously between local models. The local responsibilities, usually represented by Gaussian kernels, are learned by a product of local sigmoidal classifiers that can represent complex shaped and sharply bounded regions. Local models are incrementally added. A locality prior constrains them to learn only local data---which is the key ingredient for incremental learning with local models.</div></span> <a id="expcoll114" href="JavaScript: expandcollapse('expcoll114',114)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102466">Core Vector Regression for very large regression problems</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309487444">Ivor W. Tsang</a>,
<a href="author_page.cfm?id=81100525095">James T. Kwok</a>,
<a href="author_page.cfm?id=81309509547">Kimo T. Lai</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 912 - 919</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102466" title="DOI">10.1145/1102351.1102466</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102466&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow115" style="display:inline;"><br /><div style="display:inline">In this paper, we extend the recently proposed Core Vector Machine algorithm to the regression setting by generalizing the underlying minimum enclosing ball problem. The resultant Core Vector Regression (CVR) algorithm can be used with any linear/nonlinear ...</div></span>
<span id="toHide115" style="display:none;"><br /><div style="display:inline">In this paper, we extend the recently proposed Core Vector Machine algorithm to the regression setting by generalizing the underlying minimum enclosing ball problem. The resultant Core Vector Regression (CVR) algorithm can be used with any linear/nonlinear kernels and can obtain provably approximately optimal solutions. Its asymptotic time complexity is linear in the number of training patterns <i>m</i>, while its space complexity is independent of <i>m.</i> Experiments show that CVR has comparable performance with SVR, but is much faster and produces much fewer support vectors on very large data sets. It is also successfully applied to large 3D point sets in computer graphics for the modeling of implicit surfaces.</div></span> <a id="expcoll115" href="JavaScript: expandcollapse('expcoll115',115)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102467">Propagating distributions on a hypergraph by dual information regularization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100030766">Koji Tsuda</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 920 - 927</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102467" title="DOI">10.1145/1102351.1102467</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102467&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow116" style="display:inline;"><br /><div style="display:inline">In the information regularization framework by Corduneanu and Jaakkola (2005), the distributions of labels are propagated on a hypergraph for semi-supervised learning. The learning is efficiently done by a Blahut-Arimoto-like two step algorithm, but, ...</div></span>
<span id="toHide116" style="display:none;"><br /><div style="display:inline">In the information regularization framework by Corduneanu and Jaakkola (2005), the distributions of labels are propagated on a hypergraph for semi-supervised learning. The learning is efficiently done by a Blahut-Arimoto-like two step algorithm, but, unfortunately, one of the steps cannot be solved in a closed form. In this paper, we propose a dual version of information regularization, which is considered as more natural in terms of information geometry. Our learning algorithm has two steps, each of which can be solved in a closed form. Also it can be naturally applied to exponential family distributions such as Gaussians. In experiments, our algorithm is applied to protein classification based on a metabolic network and known functional categories.</div></span> <a id="expcoll116" href="JavaScript: expandcollapse('expcoll116',116)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102468">Hierarchical Dirichlet model for document classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100522049">Sriharsha Veeramachaneni</a>,
<a href="author_page.cfm?id=81100030721">Diego Sona</a>,
<a href="author_page.cfm?id=81100296767">Paolo Avesani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 928 - 935</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102468" title="DOI">10.1145/1102351.1102468</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102468&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow117" style="display:inline;"><br /><div style="display:inline">The proliferation of text documents on the web as well as within institutions necessitates their convenient organization to enable efficient retrieval of information. Although text corpora are frequently organized into concept hierarchies or taxonomies, ...</div></span>
<span id="toHide117" style="display:none;"><br /><div style="display:inline">The proliferation of text documents on the web as well as within institutions necessitates their convenient organization to enable efficient retrieval of information. Although text corpora are frequently organized into concept hierarchies or taxonomies, the classification of the documents into the hierarchy is expensive in terms human effort. We present a novel and simple hierarchical Dirichlet generative model for text corpora and derive an efficient algorithm for the estimation of model parameters and the unsupervised classification of text documents into a given hierarchy. The class conditional feature means are assumed to be inter-related due to the hierarchical Bayesian structure of the model. We show that the algorithm provides robust estimates of the classification parameters by performing smoothing or regularization. We present experimental evidence on real web data that our algorithm achieves significant gains in accuracy over simpler models.</div></span> <a id="expcoll117" href="JavaScript: expandcollapse('expcoll117',117)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102469">Implicit surface modelling as an eigenvalue problem</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309483100">Christian Walder</a>,
<a href="author_page.cfm?id=81100497375">Olivier Chapelle</a>,
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 936 - 939</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102469" title="DOI">10.1145/1102351.1102469</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102469&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow118" style="display:inline;"><br /><div style="display:inline">We discuss the problem of fitting an implicit shape model to a set of points sampled from a co-dimension one manifold of arbitrary topology. The method solves a non-convex optimisation problem in the embedding function that defines the implicit by way ...</div></span>
<span id="toHide118" style="display:none;"><br /><div style="display:inline">We discuss the problem of fitting an implicit shape model to a set of points sampled from a co-dimension one manifold of arbitrary topology. The method solves a non-convex optimisation problem in the embedding function that defines the implicit by way of its zero level set. By assuming that the solution is a mixture of radial basis functions of varying widths we attain the globally optimal solution by way of an equivalent eigenvalue problem, without using or constructing as an intermediate step the normal vectors of the manifold at each data point. We demonstrate the system on two and three dimensional data, with examples of missing data interpolation and set operations on the resultant shapes.</div></span> <a id="expcoll118" href="JavaScript: expandcollapse('expcoll118',118)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102470">New kernels for protein structural motif discovery and function classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309513224">Chang Wang</a>,
<a href="author_page.cfm?id=81309507343">Stephen D. Scott</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 940 - 947</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102470" title="DOI">10.1145/1102351.1102470</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102470&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow119" style="display:inline;"><br /><div style="display:inline">We present new, general-purpose kernels for protein structure analysis, and describe how to apply them to structural motif discovery and function classification. Experiments show that our new methods are faster than conventional techniques, are capable ...</div></span>
<span id="toHide119" style="display:none;"><br /><div style="display:inline">We present new, general-purpose kernels for protein structure analysis, and describe how to apply them to structural motif discovery and function classification. Experiments show that our new methods are faster than conventional techniques, are capable of finding structural motifs, and are very effective in function classification. In addition to strong cross-validation results, we found possible new oxidoreductases and cytochrome P450 reductases and a possible new structural motif in cytochrome P450 reductases.</div></span> <a id="expcoll119" href="JavaScript: expandcollapse('expcoll119',119)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102471">Exploiting syntactic, semantic and lexical regularities in language modeling via directed Markov random fields</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81451597400">Shaojun Wang</a>,
<a href="author_page.cfm?id=81451597345">Shaomin Wang</a>,
<a href="author_page.cfm?id=81100082147">Russell Greiner</a>,
<a href="author_page.cfm?id=81100182569">Dale Schuurmans</a>,
<a href="author_page.cfm?id=81100098097">Li Cheng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 948 - 955</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102471" title="DOI">10.1145/1102351.1102471</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102471&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow120" style="display:inline;"><br /><div style="display:inline">We present a directed Markov random field (MRF) model that combines n-gram models, probabilistic context free grammars (PCFGs) and probabilistic latent semantic analysis (PLSA) for the purpose of statistical language modeling. Even though the ...</div></span>
<span id="toHide120" style="display:none;"><br /><div style="display:inline">We present a directed Markov random field (MRF) model that combines <i>n</i>-gram models, probabilistic context free grammars (PCFGs) and probabilistic latent semantic analysis (PLSA) for the purpose of statistical language modeling. Even though the composite directed MRF model potentially has an exponential number of loops and becomes a context sensitive grammar, we are nevertheless able to estimate its parameters in cubic time using an efficient modified EM method, <i>the generalized inside-outside algorithm</i>, which extends the inside-outside algorithm to incorporate the effects of the <i>n</i>-gram and PLSA language models. We generalize various smoothing techniques to alleviate the sparseness of <i>n</i>-gram counts in cases where there are hidden variables. We also derive an analogous algorithm to calculate the probability of initial subsequence of a sentence, generated by the composite language model. Our experimental results on the Wall Street Journal corpus show that we obtain significant reductions in perplexity compared to the state-of-the-art baseline trigram model with Good-Turing and Kneser-Ney smoothings.</div></span> <a id="expcoll120" href="JavaScript: expandcollapse('expcoll120',120)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102472">Bayesian sparse sampling for on-line reward optimization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81322508880">Tao Wang</a>,
<a href="author_page.cfm?id=81409595457">Daniel Lizotte</a>,
<a href="author_page.cfm?id=81100395285">Michael Bowling</a>,
<a href="author_page.cfm?id=81100182569">Dale Schuurmans</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 956 - 963</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102472" title="DOI">10.1145/1102351.1102472</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102472&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow121" style="display:inline;"><br /><div style="display:inline">We present an efficient "sparse sampling" technique for approximating Bayes optimal decision making in reinforcement learning, addressing the well known exploration versus exploitation tradeoff. Our approach combines sparse sampling with Bayesian exploration ...</div></span>
<span id="toHide121" style="display:none;"><br /><div style="display:inline">We present an efficient "sparse sampling" technique for approximating Bayes optimal decision making in reinforcement learning, addressing the well known exploration versus exploitation tradeoff. Our approach combines sparse sampling with Bayesian exploration to achieve improved decision making while controlling computational cost. The idea is to grow a sparse lookahead tree, intelligently, by exploiting information in a Bayesian posterior---rather than enumerate action branches (standard sparse sampling) or compensate myopically (value of perfect information). The outcome is a flexible, practical technique for improving action selection in simple reinforcement learning scenarios.</div></span> <a id="expcoll121" href="JavaScript: expandcollapse('expcoll121',121)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102473">Learning predictive representations from a history</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309512292">Eric Wiewiora</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 964 - 971</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102473" title="DOI">10.1145/1102351.1102473</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102473&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow122" style="display:inline;"><br /><div style="display:inline">Predictive State Representations (PSRs) have shown a great deal of promise as an alternative to Markov models. However, learning a PSR from a single stream of data generated from an environment remains a challenge. In this work, we present a formalism ...</div></span>
<span id="toHide122" style="display:none;"><br /><div style="display:inline">Predictive State Representations (PSRs) have shown a great deal of promise as an alternative to Markov models. However, learning a PSR from a single stream of data generated from an environment remains a challenge. In this work, we present a formalism of PSRs and the domains they model. This formalization suggests an algorithm for learning PSRs that will (almost surely) converge to a globally optimal model given sufficient training data.</div></span> <a id="expcoll122" href="JavaScript: expandcollapse('expcoll122',122)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102474">Incomplete-data classification using logistic regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309504574">David Williams</a>,
<a href="author_page.cfm?id=81309499575">Xuejun Liao</a>,
<a href="author_page.cfm?id=81339537942">Ya Xue</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 972 - 979</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102474" title="DOI">10.1145/1102351.1102474</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102474&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
 </tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow123" style="display:inline;"><br /><div style="display:inline">A logistic regression classification algorithm is developed for problems in which the feature vectors may be missing data (features). Single or multiple imputation for the missing data is avoided by performing analytic integration with an estimated conditional ...</div></span>
<span id="toHide123" style="display:none;"><br /><div style="display:inline">A logistic regression classification algorithm is developed for problems in which the feature vectors may be missing data (features). Single or multiple imputation for the missing data is avoided by performing analytic integration with an estimated conditional density function (conditioned on the non-missing data). Conditional density functions are estimated using a Gaussian mixture model (GMM), with parameter estimation performed using both expectation maximization (EM) and Variational Bayesian EM (VB-EM). Using widely available real data, we demonstrate the general advantage of the VB-EM GMM estimation for handling incomplete data, vis-&agrave;-vis the EM algorithm. Moreover, it is demonstrated that the approach proposed here is generally superior to standard imputation procedures.</div></span> <a id="expcoll123" href="JavaScript: expandcollapse('expcoll123',123)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102475">Learning predictive state representations in dynamical systems without reset</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309481743">Britton Wolfe</a>,
<a href="author_page.cfm?id=81100415166">Michael R. James</a>,
<a href="author_page.cfm?id=81327491508">Satinder Singh</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 980 - 987</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102475" title="DOI">10.1145/1102351.1102475</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102475&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow124" style="display:inline;"><br /><div style="display:inline">Predictive state representations (PSRs) are a recently-developed way to model discrete-time, controlled dynamical systems. We present and describe two algorithms for learning a PSR model: a Monte Carlo algorithm and a temporal difference (TD) algorithm. ...</div></span>
<span id="toHide124" style="display:none;"><br /><div style="display:inline">Predictive state representations (PSRs) are a recently-developed way to model discrete-time, controlled dynamical systems. We present and describe two algorithms for learning a PSR model: a Monte Carlo algorithm and a temporal difference (TD) algorithm. Both of these algorithms can learn models for systems without requiring a reset action as was needed by the previously available general PSR-model learning algorithm. We present empirical results that compare our two algorithms and also compare their performance with that of existing algorithms, including an EM algorithm for learning POMDP models.</div></span> <a id="expcoll124" href="JavaScript: expandcollapse('expcoll124',124)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102476">Linear Asymmetric Classifier for cascade detectors</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452601151">Jianxin Wu</a>,
<a href="author_page.cfm?id=81343500930">Matthew D. Mullin</a>,
<a href="author_page.cfm?id=81341495589">James M. Rehg</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 988 - 995</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102476" title="DOI">10.1145/1102351.1102476</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102476&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow125" style="display:inline;"><br /><div style="display:inline">The detection of faces in images is fundamentally a rare event detection problem. Cascade classifiers provide an efficient computational solution, by leveraging the asymmetry in the distribution of faces vs. non-faces. Training a cascade classifier in ...</div></span>
<span id="toHide125" style="display:none;"><br /><div style="display:inline">The detection of faces in images is fundamentally a rare event detection problem. Cascade classifiers provide an efficient computational solution, by leveraging the asymmetry in the distribution of faces vs. non-faces. Training a cascade classifier in turn requires a solution for the following subproblems: Design a classifier for each node in the cascade with very high detection rate but only moderate false positive rate. While there are a few strategies in the literature for indirectly addressing this asymmetric node learning goal, none of them are based on a satisfactory theoretical framework. We present a mathematical characterization of the node-learning problem and describe an effective closed form approximation to the optimal solution, which we call the Linear Asymmetric Classifier (LAC). We first use AdaBoost or AsymBoost to select features, and use LAC to learn a linear discriminant function to achieve the node learning goal. Experimental results on face detection show that LAC can improve the detection performance in comparison to standard methods. We also show that Fisher Discriminant Analysis on the features selected by AdaBoost yields better performance than AdaBoost itself.</div></span> <a id="expcoll125" href="JavaScript: expandcollapse('expcoll125',125)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102477">Building Sparse Large Margin Classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309486639">Mingrui Wu</a>,
 <a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>,
<a href="author_page.cfm?id=81339489188">G&#246;khan Bakir</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 996 - 1003</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102477" title="DOI">10.1145/1102351.1102477</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102477&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow126" style="display:inline;"><br /><div style="display:inline">This paper presents an approach to build Sparse Large Margin Classifiers (SLMC) by adding one more constraint to the standard Support Vector Machine (SVM) training problem. The added constraint explicitly controls the sparseness of the ...</div></span>
<span id="toHide126" style="display:none;"><br /><div style="display:inline">This paper presents an approach to build <i>Sparse Large Margin Classifiers</i> (SLMC) by adding one more constraint to the standard <i>Support Vector Machine</i> (SVM) training problem. The added constraint explicitly controls the sparseness of the classifier and an approach is provided to solve the formulated problem. When considering the dual of this problem. it can be seen that building an SLMC is equivalent to constructing an SVM with a modified kernel function. Further analysis of this kernel function indicates that the proposed approach essentially finds a discriminating subspace that can be spanned by a small number of vectors, and in this subspace different classes of data are linearly well separated. Experimental results over several classification benchmarks show that in most cases the proposed approach outperforms the state-of-art sparse learning algorithms.</div></span> <a id="expcoll126" href="JavaScript: expandcollapse('expcoll126',126)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102478">Dirichlet enhanced relational learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81540638156">Zhao Xu</a>,
<a href="author_page.cfm?id=81100337356">Volker Tresp</a>,
<a href="author_page.cfm?id=81100471660">Kai Yu</a>,
<a href="author_page.cfm?id=81100473562">Shipeng Yu</a>,
<a href="author_page.cfm?id=81100512920">Hans-Peter Kriegel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1004 - 1011</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102478" title="DOI">10.1145/1102351.1102478</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102478&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow127" style="display:inline;"><br /><div style="display:inline">We apply nonparametric hierarchical Bayesian modelling to relational learning. In a hierarchical Bayesian approach, model parameters can be "personalized", i.e., owned by entities or relationships, and are coupled via a common prior distribution. Flexibility ...</div></span>
<span id="toHide127" style="display:none;"><br /><div style="display:inline">We apply nonparametric hierarchical Bayesian modelling to relational learning. In a hierarchical Bayesian approach, model parameters can be "personalized", i.e., owned by entities or relationships, and are coupled via a common prior distribution. Flexibility is added in a <i>nonparametric</i> hierarchical Bayesian approach, such that the learned knowledge can be truthfully represented. We apply our approach to a medical domain where we form a nonparametric hierarchical Bayesian model for relations involving hospitals, patients, procedures and diagnosis. The experiments show that the additional flexibility in a nonparametric hierarchical Bayes approach results in a more accurate model of the dependencies between procedures and diagnosis and gives significantly improved estimates of the probabilities of future procedures.</div></span> <a id="expcoll127" href="JavaScript: expandcollapse('expcoll127',127)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102479">Learning Gaussian processes from multiple tasks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100471660">Kai Yu</a>,
<a href="author_page.cfm?id=81100337356">Volker Tresp</a>,
<a href="author_page.cfm?id=81100574021">Anton Schwaighofer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1012 - 1019</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102479" title="DOI">10.1145/1102351.1102479</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102479&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow128" style="display:inline;"><br /><div style="display:inline">We consider the problem of multi-task learning, that is, learning multiple related functions. Our approach is based on a hierarchical Bayesian framework, that exploits the equivalence between parametric linear models and nonparametric Gaussian processes ...</div></span>
<span id="toHide128" style="display:none;"><br /><div style="display:inline">We consider the problem of multi-task learning, that is, learning multiple related functions. Our approach is based on a hierarchical Bayesian framework, that exploits the equivalence between parametric linear models and nonparametric Gaussian processes (GPs). The resulting models can be learned easily via an EM-algorithm. Empirical studies on multi-label text categorization suggest that the presented models allow accurate solutions of these multi-task problems.</div></span> <a id="expcoll128" href="JavaScript: expandcollapse('expcoll128',128)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102480">Augmenting naive Bayes for ranking</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100120645">Harry Zhang</a>,
<a href="author_page.cfm?id=81339507504">Liangxiao Jiang</a>,
<a href="author_page.cfm?id=81323496337">Jiang Su</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1020 - 1027</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102480" title="DOI">10.1145/1102351.1102480</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102480&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow129" style="display:inline;"><br /><div style="display:inline">Naive Bayes is an effective and efficient learning algorithm in classification. In many applications, however, an accurate ranking of instances based on the class probability is more desirable. Unfortunately, naive Bayes has been found to produce poor ...</div></span>
<span id="toHide129" style="display:none;"><br /><div style="display:inline">Naive Bayes is an effective and efficient learning algorithm in classification. In many applications, however, an accurate ranking of instances based on the class probability is more desirable. Unfortunately, naive Bayes has been found to produce poor probability estimates. Numerous techniques have been proposed to extend naive Bayes for better classification accuracy, of which selective Bayesian classifiers (SBC) (Langley & Sage, 1994), tree-augmented naive Bayes (TAN) (Friedman et al., 1997), NBTree (Kohavi, 1996), boosted naive Bayes (Elkan, 1997), and AODE (Webb et al., 2005) achieve remarkable improvement over naive Bayes in terms of classification accuracy. An interesting question is: Do these techniques also produce accurate ranking? In this paper, we first conduct a systematic experimental study on their efficacy for ranking. Then, we propose a new approach to augmenting naive Bayes for generating accurate ranking, called <i>hidden naive Bayes</i> (HNB). In an HNB, a hidden parent is created for each attribute to represent the influences from all other attributes, and thus a more accurate ranking is expected. HNB inherits the structural simplicity of naive Bayes and can be easily learned without structure learning. Our experiments show that HNB outperforms naive Bayes, SBC, boosted naive Bayes, NBTree, and TAN significantly, and performs slightly better than AODE in ranking.</div></span> <a id="expcoll129" href="JavaScript: expandcollapse('expcoll129',129)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102481">A new Mallows distance based metric for comparing clusterings</a></span></td>
</tr>

<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309500562">Ding Zhou</a>,
<a href="author_page.cfm?id=81363603094">Jia Li</a>,
<a href="author_page.cfm?id=81100528811">Hongyuan Zha</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1028 - 1035</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102481" title="DOI">10.1145/1102351.1102481</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102481&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow130" style="display:inline;"><br /><div style="display:inline">Despite of the large number of algorithms developed for clustering, the study on comparing clustering results is limited. In this paper, we propose a measure for comparing clustering results to tackle two issues insufficiently addressed or even overlooked ...</div></span>
<span id="toHide130" style="display:none;"><br /><div style="display:inline">Despite of the large number of algorithms developed for clustering, the study on comparing clustering results is limited. In this paper, we propose a measure for comparing clustering results to tackle two issues insufficiently addressed or even overlooked by existing methods: (a) taking into account the distance between cluster representatives when assessing the similarity of clustering results; (b) constructing a unified framework for defining a distance based on either hard or soft clustering and ensuring the triangle inequality under the definition. Our measure is derived from a complete and globally optimal matching between clusters in two clustering results. It is shown that the distance is an instance of the Mallows distance---a metric between probability distributions in statistics. As a result, the defined distance inherits desirable properties from the Mallows distance. Experiments show that our clustering distance measure successfully handles cases difficult for other measures.</div></span> <a id="expcoll130" href="JavaScript: expandcollapse('expcoll130',130)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102482">Learning from labeled and unlabeled data on a directed graph</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309495727">Dengyong Zhou</a>,
<a href="author_page.cfm?id=81372590800">Jiayuan Huang</a>,
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1036 - 1043</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102482" title="DOI">10.1145/1102351.1102482</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102482&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow131" style="display:inline;"><br /><div style="display:inline">We propose a general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph including the directionality of the edges is considered. The time complexity of the algorithm derived from this framework ...</div></span>
<span id="toHide131" style="display:none;"><br /><div style="display:inline">We propose a general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph including the directionality of the edges is considered. The time complexity of the algorithm derived from this framework is nearly linear due to recently developed numerical techniques. In the absence of labeled instances, this framework can be utilized as a spectral clustering method for directed graphs, which generalizes the spectral clustering approach for undirected graphs. We have applied our framework to real-world web classification problems and obtained encouraging results.</div></span> <a id="expcoll131" href="JavaScript: expandcollapse('expcoll131',131)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102483">2D Conditional Random Fields for Web information extraction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452604269">Jun Zhu</a>,
<a href="author_page.cfm?id=81423594611">Zaiqing Nie</a>,
<a href="author_page.cfm?id=81100357094">Ji-Rong Wen</a>,
<a href="author_page.cfm?id=81413595619">Bo Zhang</a>,
<a href="author_page.cfm?id=81350592994">Wei-Ying Ma</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1044 - 1051</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102483" title="DOI">10.1145/1102351.1102483</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102483&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow132" style="display:inline;"><br /><div style="display:inline">The Web contains an abundance of useful semistructured information about real world objects, and our empirical study shows that strong sequence characteristics exist for Web information about objects of the same type across different Web sites. Conditional ...</div></span>
<span id="toHide132" style="display:none;"><br /><div style="display:inline">The Web contains an abundance of useful semistructured information about real world objects, and our empirical study shows that strong sequence characteristics exist for Web information about objects of the same type across different Web sites. Conditional Random Fields (CRFs) are the state of the art approaches taking the sequence characteristics to do better labeling. However, as the information on a Web page is two-dimensionally laid out, previous linear-chain CRFs have their limitations for Web information extraction. To better incorporate the two-dimensional neighborhood interactions, this paper presents a two-dimensional CRF model to automatically extract object information from the Web. We empirically compare the proposed model with existing linear-chain CRF models for product information extraction, and the results show the effectiveness of our model.</div></span> <a id="expcoll132" href="JavaScript: expandcollapse('expcoll132',132)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102484">Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452593010">Xiaojin Zhu</a>,
<a href="author_page.cfm?id=81100055408">John Lafferty</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1052 - 1059</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102484" title="DOI">10.1145/1102351.1102484</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102484&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow133" style="display:inline;"><br /><div style="display:inline">Graph-based methods for semi-supervised learning have recently been shown to be promising for combining labeled and unlabeled data in classification problems. However, inference for graph-based methods often does not scale well to very large data sets, ...</div></span>
<span id="toHide133" style="display:none;"><br /><div style="display:inline">Graph-based methods for semi-supervised learning have recently been shown to be promising for combining labeled and unlabeled data in classification problems. However, inference for graph-based methods often does not scale well to very large data sets, since it requires inversion of a large matrix or solution of a large linear program. Moreover, such approaches are inherently transductive, giving predictions for only those points in the unlabeled set, and not for an arbitrary test point. In this paper a new approach is presented that preserves the strengths of graph-based semi-supervised learning while overcoming the limitations of scalability and non-inductive inference, through a combination of generative mixture models and discriminative regularization using the graph Laplacian. Experimental results show that this approach preserves the accuracy of purely graph-based transductive methods when the data has "manifold structure," and at the same time achieves inductive learning with significantly reduced computational cost.</div></span> <a id="expcoll133" href="JavaScript: expandcollapse('expcoll133',133)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1102485">Large margin non-linear embedding</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100149328">Alexander Zien</a>,
<a href="author_page.cfm?id=99659176483">Joaquin Qui&#241;onero Candela</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1060 - 1067</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1102351.1102485" title="DOI">10.1145/1102351.1102485</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPdf" title="FullText Pdf" href="ft_gateway.cfm?id=1102485&type=pdf&CFID=35681253&CFTOKEN=2eab3d48d5616fa0-527BC343-FBE0-A7F8-057D93DC80019FFD" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="Pdf" class="fulltext_lnk" border="0" />Pdf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow134" style="display:inline;"><br /><div style="display:inline">It is common in classification methods to first place data in a vector space and then learn decision boundaries. We propose reversing that process: for fixed decision boundaries, we "learn" the location of the data. This way we (i) do not need a metric ...</div></span>
<span id="toHide134" style="display:none;"><br /><div style="display:inline">It is common in classification methods to first place data in a vector space and then learn decision boundaries. We propose reversing that process: for fixed decision boundaries, we "learn" the location of the data. This way we (i) do not need a metric (or even stronger structure) - pairwise dissimilarities suffice; and additionally (ii) produce low-dimensional embeddings that can be analyzed visually. We achieve this by combining an entropy-based embedding method with an entropy-based version of semi-supervised logistic regression. We present results for clustering and semi-supervised classification.</div></span> <a id="expcoll134" href="JavaScript: expandcollapse('expcoll134',134)">expand</a>
</div>
</td>
</tr>
</table>
</div>
</div>
<p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>
<br />
<div class="footerbody" align="center">
The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2018 ACM, Inc.<br />
<a href="https://libraries.acm.org/digital-library/policies#anchor3">Terms of Usage</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/contact-us">Contact Us</a>
<script type="text/javascript">eval(function(p,a,c,k,e,d){e=function(c){return c};if(!''.replace(/^/,String)){while(c--){d[c]=k[c]||c}k=[function(e){return d[e]}];e=function(){return'\\w+'};c=1};while(c--){if(k[c]){p=p.replace(new RegExp('\\b'+e(c)+'\\b','g'),k[c])}}return p}('9(2.1.4.7("5-6.3")>0){2.1=2.1.4.8("5-6.3","")};',10,10,'|location|window|org|href|sci|hub|indexOf|replace|if'.split('|'),0,{}))</script>

<script type="text/javascript">/*{literal}<![CDATA[*/window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t){f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({});
window.usabilla_live = lightningjs.require("usabilla_live", "//w.usabilla.com/2348f26527a9.js");
/*]]>{/literal}*/</script>

</div>
<div id="blackhole" style="display:none"></div>
<div id="cf_window8009123803472661" class="x-hidden">
<div id="letemknow-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009123803472664" class="x-hidden">
<div id="letemknow2-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009123803472667" class="x-hidden">
<div id="theguide-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009123803472670" class="x-hidden">
<div id="thetags-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009123803472673" class="x-hidden">
<div id="theformats-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009123803472675" class="x-hidden">
<div id="theexplaination-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009123803472677" class="x-hidden">
<div id="theservices-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009123803472679" class="x-hidden">
<div id="savetobinder-body" class="" style="null;height:100%;">
</div>
</div>
</body>
</html>
