
<!doctype html>
<head><script type="text/javascript">/* <![CDATA[ */_cf_loadingtexthtml="<img alt=' ' src='/cf_scripts/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/cf_scripts/scripts/ajax";
_cf_jsonprefix='//';
_cf_websocket_port=0;
_cf_flash_policy_port=0;
_cf_clientid='44FE3DFD6C360FB7DA84879923FCBEB9';/* ]]> */</script><script type="text/javascript" src="/cf_scripts/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/cfform.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/masks.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/ckeditor/ckeditor.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/chart/cfchart-server.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/ext/ext-all.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/cf_scripts/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/cf_scripts/scripts/ajax/resources/cf/cf.css" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />
<title>Proceedings of the 23rd international conference on Machine learning</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em;}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	
	.mono-text {font-size: 14px; font-family: Consolas, Menlo, Monaco, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace, serif;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}
		
	.small-link-text2 {font-size: .83em !important; 
	}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
		
.x-tabs-strip-wrap {
	overflow-y: hidden !important;
}
  --></style>
<script src="js/tagcanvas.min.js" type="text/javascript"></script>
<script type="text/javascript">
  function loadCloud() {
	try {
	  TagCanvas.Start('myCanvas','tags',{
		textColour: '#000000',
		outlineColour: '#ff00ff',
		reverse: true,
		shuffleTags:true,
		depth: 0.8,
		maxSpeed: 0.05,
		textHeight: 12,
		initial: [0.000, 0.050],
		shape: "hring",
		lock: "x"
	  });
	} catch(e) {
	  // something went wrong, hide the canvas container
	  // document.getElementById('myCanvasContainer').style.display = 'none';
	}
  };
</script>
<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>
<script type='text/javascript' src='https://www.google.com/jsapi'></script>
<style type="text/css"><!--
.google-visualization-orgchart-node {
    background-color: #FFFFFF !important;
    border: 2px solid #AFCF40 !important;
    cursor: default;
    font-family: arial,helvetica;
    text-align: center;
    vertical-align: middle;
}

iframe {float:right; 
		margin:10px;
		border: 2px solid #1B4D0E;
		
		}

a.boxed:link {text-decoration: none !important; 	Color: #000000 !important;}
a.boxed:visited  { color: #000000 !important; text-decoration: none !important;}
a.boxed:hover {color: red !important; text-decoration: underline !important;}

a.boxedh:link {text-decoration: none !important; 	Color: #000000 !important;}
a.boxedh:visited  { color: #000000 !important; text-decoration: none !important;}
a.boxedh:hover {color: red !important; text-decoration: underline !important;}		

a.boxedm:link {text-decoration: none !important; 	Color: #606060 !important;}
a.boxedm:visited  { color: #606060 !important; text-decoration: none !important;}
a.boxedm:hover {color: red !important; text-decoration: underline !important;}		

a.boxedl:link {text-decoration: none !important; 	Color: #808080   !important;}
a.boxedl:visited  { color: 	#808080 !important; text-decoration: none !important;}
a.boxedl:hover {color: red !important; text-decoration: underline !important;}				

.google-visualization-orgchart-linebottom {
    border-bottom: 1px solid #006699 !important;
}
.google-visualization-orgchart-lineleft {
    border-left: 1px solid #006699 !important;
}

.google-visualization-orgchart-lineright {
    border-right: 1px solid #006699 !important;
}

--></style>
<script type="text/javascript">


function settab() {
    var mytabs = ColdFusion.Layout.getTabLayout('citationdetails');
   
 
  mytabs.on('tabchange', function(tabpanel,activetab) { document.cookie = 'picked=' + '1143844' + ',' + activetab.id; })
 
}


function letemknow(){
  ColdFusion.Window.show('letemknow');
}

function letemknow2(){
  ColdFusion.Window.show('letemknow2');
}





function testthis(){

alert('test');
}
function loadalert(){ 
 alert("I am in the load alert");
 
}
function loadalert2(){ 
  alert("I am in the load alert2");
 
}
</script>
<script type='text/javascript'>
	  	google.load('visualization', '1', {packages:['orgchart']});
	    google.setOnLoadCallback(drawChart);
	  
	  function drawChart() {
	    var data = new google.visualization.DataTable();
        data.addColumn('string', 'Name');
        data.addColumn('string', 'Manager');
        data.addColumn('string', 'ToolTip');
  	  
		data.addRows([
          [{v:'0', f:'<div style="color:black; font-size:150%; font-style:normal">CCS&nbsp;for&nbsp;this&nbsp;Proceeding</div>'}, '', ''],
		  
        ]);
		
		if (document.getElementById('chart_div')) {
       var chart = new google.visualization.OrgChart(document.getElementById('chart_div'));
       chart.draw(data, {allowHtml:true});
		}
      }
	  
</script>
<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  
</script>
<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>
<script type="text/javascript">
function expandWatson(divID,theConcept) { 
			if (document.getElementById(divID).style.display == "none") {
				document.getElementById(divID).style.display = "block";
				document.getElementById(divID).innerHTML = theConcept + "<br />" + document.getElementById(divID).innerHTML;
			}
			 else {
				 document.getElementById(divID).style.display = "none"
			}
		}
</script>

<script type="text/javascript">
  function togglePatMap() {
        var div = document.getElementById('patmap'); 
        if (div.style.display == "none") {
            div.style.display = "block";
            document.getElementById("expandcollapsepmapa").src = "images/collapse.png";
			
			if (div.innerHTML.length == 0){
				httpGetAsyncwID("patent.cfm?id=1143844",'patmap');
				httpGetAsyncwID("simmap_track.cfm?id=1143844&how=live",'blackhole');
			}
			else {
				httpGetAsyncwID("simmap_track.cfm?id=1143844&how=cache",'blackhole');
			}
			
        }
        else {
            div.style.display = "none";
            document.getElementById("expandcollapsepmapa").src = "images/expand.png";
        }
    }
  
  function toggleCO() {
        var div = document.getElementById('codisp'); 
        if (div.style.display == "none") {
            div.style.display = "block";
            document.getElementById("expandcollapsecoa").src = "images/collapse.png";
			
			if (div.innerHTML.length == 0){
				/* httpGetAsyncwID("coint.cfm?id=1143844",'codisp'); */
				/*httpGetAsyncwID("simmap_track.cfm?id=1143844&how=live",'blackhole'); */
			}
			else {
				/* httpGetAsyncwID("simmap_track.cfm?id=1143844&how=cache",'blackhole'); */
			}
			
        }
        else {
            div.style.display = "none";
            document.getElementById("expandcollapsecoa").src = "images/expand.png";
        }
    }
	
	function httpGetAsyncwID(theUrl,theID) {
		var xmlHttp = new XMLHttpRequest();
		xmlHttp.onreadystatechange = function() {
			if (xmlHttp.readyState == 4 && xmlHttp.status == 200)
				
				document.getElementById(theID).innerHTML=xmlHttp.responseText;
		}
		xmlHttp.open("GET", theUrl, true); // true for asynchronous 
		xmlHttp.send();
	}
</script>
<meta name="citation_publisher" content="ACM"> <meta name="citation_authors" content="Program Chair-Cohen, William; Program Chair-Moore, Andrew"> <meta name="citation_conference_title" content="Proceedings of the 23rd international conference on Machine learning"> <meta name="citation_date" content="06/25/2006"> <meta name="citation_isbn" content="1-59593-383-2"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=1143844">
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFFORM');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFDIV');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFTEXTAREA');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Event.registerOnLoad(drawChart,null,false,true);
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFWINDOW');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009133146069871=function()
	{
		_cf_bind_init_8009133146069872=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'letemknow-body','bindExpr':['letemknow.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009133146069872);var _cf_window=ColdFusion.Window.create('letemknow','<div style=\'text-align:left; color:black;\'>Did you know the ACM DL App is now available?</div>','letemknow.cfm',{ modal:false, closable:true, divid:'cf_window8009133146069870', draggable:true, resizable:true, fixedcenter:false, width:600, height:275, shadow:true, callfromtag:true, minwidth:600, minheight:275, initshow:false, destroyonclose:false, x:75, y:125});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009133146069871);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009133146069874=function()
	{
		_cf_bind_init_8009133146069875=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'letemknow2-body','bindExpr':['letemknow_recomm.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009133146069875);var _cf_window=ColdFusion.Window.create('letemknow2','<div style=\'text-align:left; color:black;\'>Did you know your Organization can subscribe to the ACM Digital Library?</div>','letemknow_recomm.cfm',{ modal:false, closable:true, divid:'cf_window8009133146069873', draggable:true, resizable:true, fixedcenter:false, width:600, height:275, shadow:true, callfromtag:true, minwidth:600, minheight:275, initshow:false, destroyonclose:false, x:70, y:175});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009133146069874);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009133146069877=function()
	{
		_cf_bind_init_8009133146069878=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide-body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009133146069878);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window8009133146069876', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009133146069877);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009133146069880=function()
	{
		_cf_bind_init_8009133146069881=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags-body','bindExpr':['showthetags.cfm?id=1143844']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009133146069881);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=1143844',{ modal:false, closable:true, divid:'cf_window8009133146069879', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009133146069880);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009133146069883=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window8009133146069882', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009133146069883);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009133146069885=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window8009133146069884', draggable:true, resizable:true, fixedcenter:false, width:600, height:600, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009133146069885);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009133146069887=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window8009133146069886', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009133146069887);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009133146069889=function()
	{
		_cf_bind_init_8009133146069890=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder-body','bindExpr':['savetobinder.cfm?id=1143844']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009133146069890);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=1143844',{ modal:false, closable:true, divid:'cf_window8009133146069888', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009133146069889);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Event.registerOnLoad(settab,null,false,true);
/* ]]> */</script>
</head>

<body style="text-align:center; font-size:100%"> 
<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'https://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
<script src='AC_RunActiveContent.js' type="text/javascript"></script>
<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>

<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-NFGCMX"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NFGCMX');</script>

<div id="header">
<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
<tr style="vertical-align:top">
<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text2"><img src="/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
</td>
<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; vertical-align:middle;" class="small-link-text2">
<table style="width:100%; border-collapse:collapse; padding:0px">
<tr><td style="text-align:center">
 <div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
</td>
</tr>
</table>
</td>
<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text2">
<p style="margin-top:0px; margin-bottom:10px;">
<a href="https://dl.acm.org/signin.cfm" class="small-link-text2" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
&nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm" class="small-link-text2" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
</p>
<table style="padding: 5px; border-collapse:collapse; float:right">
<tr>
<td class="small-link-text2" style="text-align:right">
<script type="text/javascript">
								function encodeInput(form){
								    	var cleanQuery = form.elements['query'].value.replace(new RegExp( "\\+", "g" ),"%2B");
										cleanQuery = cleanQuery.replace(/#/g, "%23");
										cleanQuery = cleanQuery.replace(/(\n)/g, " ");
										cleanQuery = cleanQuery.trim();
										
										
										var ascii = /^[ -~]+$/;
										if( !ascii.test( cleanQuery ) ) {
											var fixedUseQuery = "";
											for (var i = 0, len = cleanQuery.length; i < len; i++) {
												var str = "";
												if( !ascii.test(cleanQuery[i]) ) {
										 			str = "%26%23" + cleanQuery[i].charCodeAt(0) + ";";
												} else {
										 			str = cleanQuery[i];
												}
												fixedUseQuery = fixedUseQuery + str;
											}
											cleanQuery = fixedUseQuery;
										}
										

										form.elements['query'].value = cleanQuery;
								}
							</script>
<form name="qiksearch" action="/results.cfm" onsubmit='encodeInput(this)'>
<span style="margin-left:0px"><label><input type="text" name="query" size="30" value="" /></label>&nbsp;
<input style="vertical-align:top;" type="image" alt="Search" name="Go" src="/images/search_small.jpg" />
</span>
</form>
</td>
</tr>
</table>
</td>
</tr>
<tr><td colspan="3" class="small-link-text2" style="padding-bottom:5px; padding-top:0px; text-align:center">
<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
</td>
</tr>
</table>
<map name="port" id="port">
<area shape="rect" coords="1,1,60,50" href="https://www.acm.org/" alt="ACM Home Page" />
<area shape="rect" coords="65,1,275,68" href="https://dl.acm.org/dl.cfm" alt="ACM Digital Library Home Page" />
</map>
</div>
<style>
  .watsonCont {
	  width:170px;
	  
	  color: #000000;
    font-family: Arial,Helvetica,sans-serif;
    font-size: 1em;
	margin-top: 10px;
	margin-bottom: 10px;
	 /* background-color: lightgray;*/
  }
  #watsonInside {
    border-radius: 25px;
    border: 2px solid #649134;
	margin-top: 12px;
	padding: 5px;
	height: 80px;
  }

</style>
<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
<tr style="vertical-align:top">
<td style="padding-right:10px; text-align:left" class="small-link-text">
<div id="divmain" style="border:1px solid #356b20;">
<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;">Proceedings of the 23rd international conference on Machine learning</h1>
</div>
<table class="medium-text" style="border-collapse:collapse; padding:0px; margin-left: 2px;">
<colgroup>
<col style="width:540px" />
</colgroup>
<tr style="vertical-align:top">
<td>
<table style="border-collapse:collapse; padding:2px;" class="medium-text">
<col style="width:80px;" />
<col style="width:auto" />
<tr style="vertical-align:top">
</tr>
</table>
<table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
<col style="width:80px" />
<tr>
<td valign="top" nowrap="nowrap">
Program Chairs:
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81100145736&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">William Cohen</a>
</td>
<td valign="bottom">
</td>
</tr>
<tr>
<td valign="top" nowrap="nowrap">
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81339517958&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">Andrew Moore</a>
</td>
<td valign="bottom">
</td>
</tr>
</table>
<table style="margin-top: 10px" border="0" class="medium-text" cellpadding="2" cellspacing="0">
<tr>
<td><table border="0" class="medium-text" style="margin-left:5px;" cellpadding="1" cellspacing="0">
<tr valign="top">
<td nowrap="nowrap" style="padding-top:10px;">Publication:</td>
</tr>
<tr valign="top">
<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Proceeding</td>
<td colspan="2"></td>
</tr>
<tr valign="top">
<td colspan="3" style="padding-left:10px">ICML '06 Proceedings of the 23rd international conference on Machine learning
<br />
Pittsburgh, Pennsylvania, USA &mdash; June 25 - 29, 2006
<br />
</td>
</tr>
<tr valign="top">
<td colspan="3" style="padding-left:10px"> <a href="https://www.acm.org/publications" class="small-link-text" title="ACM">ACM</a> <span class="small-link-text">New York, NY</span><span class="small-link-text">, USA</span> <span class="small-link-text"> &copy;2006</span>
<br />
<a href="citation.cfm?id=1143844&amp;picked=prox" target="_self" class="small-link-text">table&nbsp;of&nbsp;contents</a>
ISBN:1-59593-383-2
</td>
</tr>
</table></td>
</tr>
</table>
</td>
<td rowspan="20">
<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
<tr>
<td align="center" style="padding-bottom: 5px;">
</td>
<td align="left" nowrap="nowrap">
<img src="images/ACM_mini.jpg" style="vertical-align:middle" title="Published by ACM" alt="Published by ACM" /> 2006 Proceeding<br />
</td>
</tr>
<tr>
<td colspan="2" valign="baseline" style="padding-bottom:5px; padding-top:5px;">
<img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
<a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
</td>
</tr>
<tr>
<td class="small-text" colspan="2" valign="top" style="padding-left:30px;">
&middot;&nbsp;Citation Count: 4,568<br />
&middot;&nbsp;Downloads (cumulative): 83,699<br />
&middot;&nbsp;Downloads (12 Months): 7,828<br />
&middot;&nbsp;Downloads (6 Weeks): 1,165<br />
</td>
</tr>
</table>
</td>
</tr>
</table>
<br clear="all" />
<br clear="all" />
</div>
</td>
<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
<div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
<div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;">Tools and Resources</h1></div>
<ul title="Tools and Resources" style="list-style: none; list-style-position:outside;
margin-left: 25px;
padding-left: 0em;
text-indent: 0px;
margin-bottom: 0px;">
<li style="list-style-image:url(img/shopping-cart16.gif);margin-top:10px;">
<span style="margin-left:0px;">
<a href="https://dl.acm.org/purchase.cfm?id=1143844" class="small-link-text" title="Buy this Proceeding">Buy this Proceeding</a>
</span>
</li>
<li style="list-style-image:url(img/dllogosm.gif);margin-top:10px;"><span style="margin-left:0px;"><a href="https://dl.acm.org/reqdl.cfm" class="small-link-text" title="Recommend the ACM DL to your Organization">Recommend the ACM DL<br />to your organization</a></span></li>
<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:0px;">
<span class="small-link-text">TOC Service:</span>
<img src="images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
<ul style="margin-left: 0; padding-left: 0; display:inline;">
<li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
<li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">RSS</a></li>
</ul>
</span>
</li>
<li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:0px;">
<a href="citation.cfm?id=1143844&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
</span></li>
<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:0px; margin-bottom:0px">
<span class="small-link-text">Export Formats:</span>
<ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1143844&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1143844&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1143844&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
</ul>
</span>
</li>
</ul>


<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>


<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google_plusone_share"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_researchgate"></a>
<a class="addthis_button_reddit"></a>
<span class="addthis_separator">|</span>
<a href="https://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="https://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>

</div>
</td>
</tr>
</table>
</div>
<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<div id="fback" style="text-align:left; padding-top:20px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="Contact The DL Team" href="/cdn-cgi/l/email-protection#86f6e9f4f2e7eaabe0e3e3e2e4e7e5edc6eef7a8e7e5eba8e9f4e1" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="Contact The DL Team" border="0" /></a>
<a title="Contact The DL Team" href="/cdn-cgi/l/email-protection#d9a9b6abadb8b5f4bfbcbcbdbbb8bab299b1a8f7b8bab4f7b6abbe"><strong>Contact Us</strong></a>
<span style="padding:10px;">|</span>
<span>Switch to <a href="citation.cfm?id=1143844&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>
</span>
<div class="small-text" style="margin-top:10px; margin-bottom:5px;">
<br />
<a href="#abstract" title="Abstract" style="padding:5px"><span>Abstract</span></a> |
<a href="#formats" title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
<a href="#authors" title="Authors" style="padding:5px"><span>Authors</span></a> |
<a href="#references" title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
<a href="#citedby" title="Cited By" style="padding:5px"><span style='color:#999999'>Cited By</span></a> |
<a href="#indexterms" title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
<a href="#source" title="Publication" style="padding:5px"><span>Publication</span></a> |
<a href="#revs" title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |
<a href="#comments" title="Comments" style="padding:5px"><span>Comments</span></a>
|
<a href="#prox" title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
</div>
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;" />
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div style="display:inline">This volume, which is also available from http://www.machinelearning.org, the home page of the International Machine Learning Society, contains the technical papers accepted for presentation at ICML-2006, the 23rd International Conference on Machine Learning. ICML is an international forum for presentation and discussion of the latest results in the field of machine learning. This year, ICML was held at Carnegie Mellon University, in Pittsburgh, Pennsylvania, and was co-located with COLT-2006, the 19th Annual Conference on Computational Learning Theory.Coincidentally, Carnegie Mellon University was also the venue for the first ICML---the First Machine Learning Workshop, which was held in 1980. Instead of proceedings, a book was published (Machine Learning: an Artificial Intelligence Approach, ed. Michalski, Carbonell, and Mitchell, Morgan Kaufman, 1983) containing sixteen research papers, and also a "comprehensive bibliography" of the field of machine learning, as it stood in 1983. This bibliography contained 572 entries.In 2006, no less than 548 papers were submitted to ICML---nearly as many as were in the "comprehensive bibliography" published with the papers from the first ICML. These papers were subjected to a thorough review process. In the first round of reviewing, every paper received three reviews by program committee members. Authors were then given an opportunity to view the first-round reviews and respond to them. Led by a Senior Program Committee member, the reviewers then engaged in a discussion of the paper, leading finally to a decision by the Senior Program Committee member in charge of the paper. Papers could be accepted, rejected, or conditionally accepted; the 36 conditionally accepted papers were subject to an additional final round of review by the Senior Program Committee. Of the 548 submissions, 140 were accepted for publication, an acceptance rate of 25.5%.In addition to the technical talks, ICML-2006 also included seven tutorials and eleven workshops, which were held before and after the conference, respectively. Authors presented their papers both orally and in a poster session, allowing time for detailed discussions with any interested attendees of the conference. Each day of the main conference included an invited talk by a prominent researcher. We were very fortunate to be able to host David Haussler, of the University of California at Santa Cruz; Robert Schapire, of Princeton University; and Mandyam V. Srinivasan, of the Australian National University.</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div class="abstract">
<SPAN style="font-weight:bold">FRONT MATTER</span>
</div>
<div style="margin-left:10px; line-height:180%;">
<A NAME="FullText" HREF="https://portalparts.acm.org/1150000/1143844/fm/frontmatter.pdf?ip=173.16.22.104" title="PDF" target="_blank">
<img src="imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
&nbsp;Front matter (TOC, Preface, Organization, Overview)
</div>
<div style="margin-top: 10px;" class="abstract">
<SPAN style="font-weight:bold">BACK MATTER</span>
</div>
<div style="margin-left:10px; line-height:180%;">
<A NAME="FullText" HREF="https://portalparts.acm.org/1150000/1143844/bm/backmatter.pdf?ip=173.16.22.104" title="PDF" target="_blank">
<img src="imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
&nbsp;Back matter (Author index)
</div>
<div style="margin-top: 10px; height: auto; padding: 5px; ">
<div style="margin-top:20px;" class="abstract">
<SPAN style="font-weight:bold">APPEARS IN</span>
</div>
<div>
<a href="/icps.cfm" title="ICPS"><img src="images/ACM_ICPS.jpg" alt="ICPS" style="padding-right:10px; vertical-align:middle" border="0" /></a> ICPS: <a href="/icps.cfm" title="ICPS" target="_blank">ACM International Conference Proceeding Series</a>
</div>
</div>
<br clear="all" />
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<dl title="Authors" style="margin-top:0px">
<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
<strong>
Program Chairs
</strong>
</dt>
<dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of William Cohen" href="author_page.cfm?id=81100145736">William Cohen</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1987-2017</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">159</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">3,291</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">62</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">315</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">2,435</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">30,558</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">492.87</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">20.70</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of William Cohen" href="author_page.cfm?id=81100145736&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of William Cohen
</td>
</tr>
</table>
</span>
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of Andrew Moore" href="author_page.cfm?id=81339517958">Andrew Moore</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td>&nbsp;</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of Andrew Moore" href="author_page.cfm?id=81339517958&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of Andrew Moore
</td>
</tr>
</table>
</span>
</dd>
</dl>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
References are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
Citings are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 0px;" class="flatbody">
Index Terms are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<table border="0" class="medium-text" cellpadding="5" cellspacing="5">
<tr valign="top">
<td style="padding: 10px;">Title</td>
<td style="padding: 10px;">ICML '06 Proceedings of the 23rd international conference on Machine learning
<a href="citation.cfm?id=1143844&picked=prox" target="_self" class="small-link-text">table&nbsp;of&nbsp;contents</a>
</td>
</tr>
<tr><td style="padding: 10px;">Pages</td><td style="padding: 10px;">1154</td></tr>
<tr><td style="padding: 10px;">Publisher</td><td style="padding: 10px;"><a href="https://www.acm.org/publications">ACM</a> New York, NY, USA &copy;2006
</td>
</tr>
<tr><td style="padding: 10px;">ISBN</td><td style="padding: 10px;">1-59593-383-2 </td></tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
</td>
</tr>
<tr valign="top">
<td>Conference</td>
<td valign="top" align="left" style="padding-bottom: 25px;">
<strong>ICML</strong><a href="event.cfm?id=RE548" title="International Conference on Machine Learning">International Conference on Machine Learning</a>
</td>
</tr>
<tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 448 of 1,653 submissions, 27%</td></tr>
<tr valign="top">
<td colspan="2" style="padding-left:25px;">
<table>
<tr><td style="padding: 10px;">
<img border="0" class="chart" id="3490103320132194-img" src="/CFFileServlet/_cf_chart/3490103320132194.jpg" usemap="#3490103320132194-map" />
<div id="3490103320132194-tooltip" style="position:fixed;display:none;"></div>
<map id="3490103320132194-map" name="3490103320132194-map">
<area style="cursor:auto" shape="rect" id="3490103320132194-graph-id0-plotset-plot-0-node-0" coords="37,24,57,214" />
<area style="cursor:auto" shape="rect" id="3490103320132194-graph-id0-plotset-plot-0-node-1" coords="95,33,115,214" />
<area style="cursor:auto" shape="rect" id="3490103320132194-graph-id0-plotset-plot-0-node-2" coords="152,12,173,214" />
<area style="cursor:auto" shape="rect" id="3490103320132194-graph-id0-plotset-plot-1-node-0" coords="64,167,85,214" />
<area style="cursor:auto" shape="rect" id="3490103320132194-graph-id0-plotset-plot-1-node-1" coords="122,164,142,214" />
<area style="cursor:auto" shape="rect" id="3490103320132194-graph-id0-plotset-plot-1-node-2" coords="180,161,200,214" />
</map>
<script data-cfasync="false" src="/cdn-cgi/scripts/f2bf09f8/cloudflare-static/email-decode.min.js"></script><script>
if (!CFCHART) {var CFCHART={};};if (!CFCHART.nodes) {CFCHART.nodes={};}
CFCHART.nodes["3490103320132194"]={};
CFCHART.nodes["3490103320132194"]["3490103320132194-graph-id0-plotset-plot-0-node-0"]={text:"548",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["3490103320132194"]["3490103320132194-graph-id0-plotset-plot-0-node-1"]={text:"522",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["3490103320132194"]["3490103320132194-graph-id0-plotset-plot-0-node-2"]={text:"583",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["3490103320132194"]["3490103320132194-graph-id0-plotset-plot-1-node-0"]={text:"140",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["3490103320132194"]["3490103320132194-graph-id0-plotset-plot-1-node-1"]={text:"150",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["3490103320132194"]["3490103320132194-graph-id0-plotset-plot-1-node-2"]={text:"158",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
</script>
</td>
<td style="padding-left:20px;">
<table style="border-width: 1px; border-style: solid; width:100%; border-spacing: 6px;" class="text12">
<tr bgcolor="#ffffff">
<th style="width:50%">Year</th>
<th align="right" style="width:15%;">Submitted</th>
<th align="right" style="width:15%">Accepted</th>
<th align="center">Rate</th>
</tr>
<tr bgcolor="#f0f0f0">
<td style="padding: 10px;">ICML '06</td>
<td align="right">548</td>
<td align="right">140</td>
<td align="center">26%</td>
</tr>
<tr bgcolor="#ffffff">
<td style="padding: 10px;">ICML '07</td>
<td align="right">522</td>
<td align="right">150</td>
<td align="center">29%</td>
</tr>
<tr bgcolor="#f0f0f0">
<td style="padding: 10px;">ICML '08</td>
<td align="right">583</td>
<td align="right">158</td>
<td align="center">27%</td>
</tr>
<tr bgcolor="#ffffff">
<td style="padding: 10px;"><strong>Overall</strong></td>
<td align="right">1,653</td>
<td align="right">448</td>
<td align="center">27%</td>
</tr>
 </table>
</td>
</tr>
</table>
</td>
</tr>
</table>
<br />
<div class="abstract" style="margin-bottom:10px;">
<SPAN><strong>APPEARS IN</strong></span>
</div>
<div>
<a href="/icps.cfm" title="ICPS"><img src="images/ACM_ICPS.jpg" alt="ICPS" style="padding-right:10px; vertical-align:middle" border="0" /></a> ICPS: <a href="/icps.cfm" title="ICPS" target="_blank">ACM International Conference Proceeding Series</a>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<br />Reviews are not available for this item
<div align="left" style="margin-top:30px">
<a title="Computing Reviews" href="ocr_review_main.cfm">
<img src="images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
<ul style="list-style:disc; display:inline-block">
<li>Access <a href="ocr_review_main.cfm" target="_blank">critical reviews</a> of computing literature.</li>
<li><a href="http://www.computingreviews.com/Reviewer/" target="_blank">Become a reviewer</a> for Computing Reviews</li>
</ul>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div>
<div>
<p style="margin-left:5px;">
<strong>Be the first to comment</strong>
To Post a comment please <a href="signin.cfm">sign in or create</a> a free Web account</a>
</p>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;">
<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the 23rd international conference on Machine learning</h5>
<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>
<div style="clear:both">
<div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1102351&picked=prox" title="previous: ICML '05"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><a href="citation.cfm?id=1273496&picked=prox" title="Next: ICML '07">next proceeding <img align="absmiddle" hspace="5" border="0" src="img/next.gif" width="19" height="11" alt="next" /></a></div>
</div>
<table class="text12" border="0">
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143845">Using inaccurate models in reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309498920">Pieter Abbeel</a>,
<a href="author_page.cfm?id=81350568058">Morgan Quigley</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1-8</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143845" title="DOI">10.1145/1143844.1143845</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143845&ftid=364642&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow1" style="display:inline;"><br /><div style="display:inline">In the model-based policy search approach to reinforcement learning (RL), policies are found using a model (or "simulator") of the Markov decision process. However, for high-dimensional continuous-state tasks, it can be extremely difficult to build an ...</div></span>
<span id="toHide1" style="display:none;"><br /><div style="display:inline">In the model-based policy search approach to reinforcement learning (RL), policies are found using a model (or "simulator") of the Markov decision process. However, for high-dimensional continuous-state tasks, it can be extremely difficult to build an accurate model, and thus often the algorithm returns a policy that works in simulation but not in real-life. The other extreme, model-free RL, tends to require infeasibly large numbers of real-life trials. In this paper, we present a hybrid algorithm that requires only an approximate model, and only a small number of real-life trials. The key idea is to successively "ground" the policy evaluations using real-life trials, but to rely on the approximate model to suggest local changes. Our theoretical results show that this algorithm achieves near-optimal performance in the real system, even when the model is only approximate. Empirical results also demonstrate that---when given only a crude model and a small number of real-life trials---our algorithm can obtain near-optimal performance in the real system.</div></span> <a id="expcoll1" href="JavaScript: expandcollapse('expcoll1',1)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143846">Algorithms for portfolio management based on the Newton method</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100634373">Amit Agarwal</a>,
<a href="author_page.cfm?id=81300137601">Elad Hazan</a>,
<a href="author_page.cfm?id=81100347702">Satyen Kale</a>,
<a href="author_page.cfm?id=81100083454">Robert E. Schapire</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 9-16</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143846" title="DOI">10.1145/1143844.1143846</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143846&ftid=364230&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow2" style="display:inline;"><br /><div style="display:inline">We experimentally study on-line investment algorithms first proposed by Agarwal and Hazan and extended by Hazan et al. which achieve almost the same wealth as the best constant-rebalanced portfolio determined in hindsight. These algorithms are the first ...</div></span>
<span id="toHide2" style="display:none;"><br /><div style="display:inline">We experimentally study on-line investment algorithms first proposed by Agarwal and Hazan and extended by Hazan et al. which achieve almost the same wealth as the best constant-rebalanced portfolio determined in hindsight. These algorithms are the first to combine optimal logarithmic regret bounds with efficient deterministic computability. They are based on the Newton method for offline optimization which, unlike previous approaches, exploits second order information. After analyzing the algorithm using the potential function introduced by Agarwal and Hazan, we present extensive experiments on actual financial data. These experiments confirm the theoretical advantage of our algorithms, which yield higher returns and run considerably faster than previous algorithms with optimal regret. Additionally, we perform financial analysis using mean-variance calculations and the Sharpe ratio.</div></span> <a id="expcoll2" href="JavaScript: expandcollapse('expcoll2',2)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143847">Higher order learning with graphs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100632752">Sameer Agarwal</a>,
<a href="author_page.cfm?id=81100516903">Kristin Branson</a>,
<a href="author_page.cfm?id=81100352084">Serge Belongie</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 17-24</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143847" title="DOI">10.1145/1143844.1143847</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143847&ftid=364231&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow3" style="display:inline;"><br /><div style="display:inline">Recently there has been considerable interest in learning with higher order relations (i.e., three-way or higher) in the unsupervised and semi-supervised settings. Hypergraphs and tensors have been proposed as the natural way of representing these relations ...</div></span>
<span id="toHide3" style="display:none;"><br /><div style="display:inline">Recently there has been considerable interest in learning with higher order relations (i.e., three-way or higher) in the unsupervised and semi-supervised settings. Hypergraphs and tensors have been proposed as the natural way of representing these relations and their corresponding algebra as the natural tools for operating on them. In this paper we argue that hypergraphs are not a natural representation for higher order relations, indeed pairwise as well as higher order relations can be handled using graphs. We show that various formulations of the semi-supervised and the unsupervised learning problem on hypergraphs result in the same graph theoretic problem and can be analyzed using existing tools.</div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143848">Ranking on graph data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100632763">Shivani Agarwal</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 25-32</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143848" title="DOI">10.1145/1143844.1143848</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143848&ftid=364232&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow4" style="display:inline;"><br /><div style="display:inline">In ranking, one is given examples of order relationships among objects, and the goal is to learn from these examples a real-valued ranking function that induces a ranking or ordering over the object space. We consider the problem of learning such a ranking ...</div></span>
<span id="toHide4" style="display:none;"><br /><div style="display:inline">In ranking, one is given examples of order relationships among objects, and the goal is to learn from these examples a real-valued ranking function that induces a ranking or ordering over the object space. We consider the problem of learning such a ranking function when the data is represented as a graph, in which vertices correspond to objects and edges encode similarities between objects. Building on recent developments in regularization theory for graphs and corresponding Laplacian-based methods for classification, we develop an algorithmic framework for learning ranking functions on graph data. We provide generalization guarantees for our algorithms via recent results based on the notion of algorithmic stability, and give experimental evidence of the potential benefits of our framework.</div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143849">Robust probabilistic projections</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100099808">C&#233;dric Archambeau</a>,
<a href="author_page.cfm?id=81315488107">Nicolas Delannay</a>,
<a href="author_page.cfm?id=81100065701">Michel Verleysen</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 33-40</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143849" title="DOI">10.1145/1143844.1143849</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143849&ftid=364233&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow5" style="display:inline;"><br /><div style="display:inline">Principal components and canonical correlations are at the root of many exploratory data mining techniques and provide standard pre-processing tools in machine learning. Lately, probabilistic reformulations of these methods have been proposed (Roweis, ...</div></span>
<span id="toHide5" style="display:none;"><br /><div style="display:inline">Principal components and canonical correlations are at the root of many exploratory data mining techniques and provide standard pre-processing tools in machine learning. Lately, probabilistic reformulations of these methods have been proposed (Roweis, 1998; Tipping & Bishop, 1999b; Bach & Jordan, 2005). They are based on a Gaussian density model and are therefore, like their non-probabilistic counterpart, very sensitive to atypical observations. In this paper, we introduce robust probabilistic principal component analysis and robust probabilistic canonical correlation analysis. Both are based on a Student-<i>t</i> density model. The resulting probabilistic reformulations are more suitable in practice as they handle outliers in a natural way. We compute maximum likelihood estimates of the parameters by means of the EM algorithm.</div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143850">A DC-programming algorithm for kernel selection</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81381592518">Andreas Argyriou</a>,
<a href="author_page.cfm?id=81100548040">Raphael Hauser</a>,
<a href="author_page.cfm?id=81100202838">Charles A. Micchelli</a>,
<a href="author_page.cfm?id=81100025866">Massimiliano Pontil</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 41-48</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143850" title="DOI">10.1145/1143844.1143850</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143850&ftid=364643&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow6" style="display:inline;"><br /><div style="display:inline">We address the problem of learning a kernel for a given supervised learning task. Our approach consists in searching within the convex hull of a prescribed set of basic kernels for one which minimizes a convex regularization functional. A unique feature ...</div></span>
<span id="toHide6" style="display:none;"><br /><div style="display:inline">We address the problem of learning a kernel for a given supervised learning task. Our approach consists in searching within the convex hull of a prescribed set of basic kernels for one which minimizes a convex regularization functional. A unique feature of this approach compared to others in the literature is that the number of basic kernels can be infinite. We only require that they are continuously parameterized. For example, the basic kernels could be isotropic Gaussians with variance in a prescribed interval or even Gaussians parameterized by multiple continuous parameters. Our work builds upon a formulation involving a minimax optimization problem and a recently proposed greedy algorithm for learning the kernel. Although this optimization problem is not convex, it belongs to the larger class of DC (difference of convex functions) programs. Therefore, we apply recent results from DC optimization theory to create a new algorithm for learning the kernel. Our experimental results on benchmark data sets show that this algorithm outperforms a previously proposed method.</div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143851">Relational temporal difference learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315487500">Nima Asgharbeygi</a>,
<a href="author_page.cfm?id=81100293399">David Stracuzzi</a>,
<a href="author_page.cfm?id=81100101009">Pat Langley</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 49-56</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143851" title="DOI">10.1145/1143844.1143851</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143851&ftid=364234&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow7" style="display:inline;"><br /><div style="display:inline">We introduce relational temporal difference learning as an effective approach to solving multi-agent Markov decision problems with large state spaces. Our algorithm uses temporal difference reinforcement to learn a distributed value function represented ...</div></span>
<span id="toHide7" style="display:none;"><br /><div style="display:inline">We introduce relational temporal difference learning as an effective approach to solving multi-agent Markov decision problems with large state spaces. Our algorithm uses temporal difference reinforcement to learn a distributed value function represented over a conceptual hierarchy of relational predicates. We present experiments using two domains from the General Game Playing repository, in which we observe that our system achieves higher learning rates than non-relational methods. We also discuss related work and directions for future research.</div></span> <a id="expcoll7" href="JavaScript: expandcollapse('expcoll7',7)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143852">A new approach to data driven clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315487826">Arik Azran</a>,
<a href="author_page.cfm?id=81100572858">Zoubin Ghahramani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 57-64</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143852" title="DOI">10.1145/1143844.1143852</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143852&ftid=364235&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow8" style="display:inline;"><br /><div style="display:inline">We consider the problem of clustering in its most basic form where only a local metric on the data space is given. No parametric statistical model is assumed, and the number of clusters is learned from the data. We introduce, analyze and demonstrate ...</div></span>
<span id="toHide8" style="display:none;"><br /><div style="display:inline">We consider the problem of clustering in its most basic form where only a local metric on the data space is given. No parametric statistical model is assumed, and the number of clusters is learned from the data. We introduce, analyze and demonstrate a novel approach to clustering where data points are viewed as nodes of a graph, and pairwise similarities are used to derive a transition probability matrix <i>P</i> for a Markov random walk between them. The algorithm automatically reveals structure at increasing scales by varying the number of steps taken by this random walk. Points are represented as rows of <i>P</i><sup><i>t</i></sup>, which are the <i>t</i>-step distributions of the walk starting at that point; these distributions are then clustered using a KL-minimizing iterative algorithm. Both the number of clusters, and the number of steps that 'best reveal' it, are found by optimizing spectral properties of <i>P</i>.</div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143853">Agnostic active learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81300252501">Maria-Florina Balcan</a>,
<a href="author_page.cfm?id=81100514603">Alina Beygelzimer</a>,
<a href="author_page.cfm?id=81100453722">John Langford</a>
</span>
</td>
 </tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 65-72</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143853" title="DOI">10.1145/1143844.1143853</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143853&ftid=364236&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow9" style="display:inline;"><br /><div style="display:inline">We state and analyze the first active learning algorithm which works in the presence of arbitrary forms of noise. The algorithm, A2 (for Agnostic Active), relies only upon the assumption that the samples are drawn i.i.d. from ...</div></span>
<span id="toHide9" style="display:none;"><br /><div style="display:inline">We state and analyze the first active learning algorithm which works in the presence of arbitrary forms of noise. The algorithm, <i>A</i><sup>2</sup> (for Agnostic Active), relies only upon the assumption that the samples are drawn <i>i.i.d</i>. from a fixed distribution. We show that <i>A</i><sup>2</sup> achieves an exponential improvement (i.e., requires only <i>O</i> (ln 1/&epsilon;) samples to find an &epsilon;-optimal classifier) over the usual sample complexity of supervised learning, for several settings considered before in the realizable case. These include learning threshold classifiers and learning homogeneous linear separators with respect to an input distribution which is uniform over the unit sphere.</div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143854">On a theory of learning with similarity functions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81300252501">Maria-Florina Balcan</a>,
<a href="author_page.cfm?id=81100160291">Avrim Blum</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 73-80</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143854" title="DOI">10.1145/1143844.1143854</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143854&ftid=364644&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow10" style="display:inline;"><br /><div style="display:inline">Kernel functions have become an extremely popular tool in machine learning, with an attractive theory as well. This theory views a kernel as implicitly mapping data points into a possibly very high dimensional space, and describes a kernel function as ...</div></span>
<span id="toHide10" style="display:none;"><br /><div style="display:inline">Kernel functions have become an extremely popular tool in machine learning, with an attractive theory as well. This theory views a kernel as implicitly mapping data points into a possibly very high dimensional space, and describes a kernel function as being good for a given learning problem if data is separable by a large margin in that implicit space. However, while quite elegant, this theory does not directly correspond to one's intuition of a good kernel as a good similarity function. Furthermore, it may be difficult for a domain expert to use the theory to help design an appropriate kernel for the learning task at hand since the implicit mapping may not be easy to calculate. Finally, the requirement of positive semi-definiteness may rule out the most natural pairwise similarity functions for the given problem domain.In this work we develop an alternative, more general theory of learning with similarity functions (i.e., sufficient conditions for a similarity function to allow one to learn well) that does not require reference to implicit spaces, and does not require the function to be positive semi-definite (or even symmetric). Our results also generalize the standard theory in the sense that any good kernel function under the usual definition can be shown to also be a good similarity function under our definition (though with some loss in the parameters). In this way, we provide the first steps towards a theory of kernels that describes the effectiveness of a given kernel function in terms of natural similarity-based properties.</div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143855">On Bayesian bounds</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100144629">Arindam Banerjee</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 81-88</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143855" title="DOI">10.1145/1143844.1143855</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143855&ftid=364237&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow11" style="display:inline;"><br /><div style="display:inline">We show that several important Bayesian bounds studied in machine learning, both in the batch as well as the online setting, arise by an application of a simple compression lemma. In particular, we derive (i) PAC-Bayesian bounds in the batch setting, ...</div></span>
<span id="toHide11" style="display:none;"><br /><div style="display:inline">We show that several important Bayesian bounds studied in machine learning, both in the batch as well as the online setting, arise by an application of a simple compression lemma. In particular, we derive (i) PAC-Bayesian bounds in the batch setting, (ii) Bayesian log-loss bounds and (iii) Bayesian bounded-loss bounds in the online setting using the compression lemma. Although every setting has different semantics for prior, posterior and loss, we show that the core bound argument is the same. The paper simplifies our understanding of several important and apparently disparate results, as well as brings to light a powerful tool for developing similar arguments for other methods.</div></span> <a id="expcoll11" href="JavaScript: expandcollapse('expcoll11',11)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143856">Convex optimization techniques for fitting sparse Gaussian graphical models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81363593572">Onureena Banerjee</a>,
<a href="author_page.cfm?id=81100540027">Laurent El Ghaoui</a>,
<a href="author_page.cfm?id=81315488475">Alexandre d'Aspremont</a>,
<a href="author_page.cfm?id=81315490321">Georges Natsoulis</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 89-96</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143856" title="DOI">10.1145/1143844.1143856</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143856&ftid=364238&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow12" style="display:inline;"><br /><div style="display:inline">We consider the problem of fitting a large-scale covariance matrix to multivariate Gaussian data in such a way that the inverse is sparse, thus providing model selection. Beginning with a dense empirical covariance matrix, we solve a maximum likelihood ...</div></span>
<span id="toHide12" style="display:none;"><br /><div style="display:inline">We consider the problem of fitting a large-scale covariance matrix to multivariate Gaussian data in such a way that the inverse is sparse, thus providing model selection. Beginning with a dense empirical covariance matrix, we solve a maximum likelihood problem with an <i>l</i><inf>1</inf>-norm penalty term added to encourage sparsity in the inverse. For models with tens of nodes, the resulting problem can be solved using standard interior-point algorithms for convex optimization, but these methods scale poorly with problem size. We present two new algorithms aimed at solving problems with a thousand nodes. The first, based on Nesterov's first-order algorithm, yields a rigorous complexity estimate for the problem, with a much better dependence on problem size than interior-point methods. Our second algorithm uses block coordinate descent, updating row/columns of the covariance matrix sequentially. Experiments with genomic data show that our method is able to uncover biologically interpretable connections among genes.</div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143857">Cover trees for nearest neighbor</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100514603">Alina Beygelzimer</a>,
<a href="author_page.cfm?id=81100155305">Sham Kakade</a>,
<a href="author_page.cfm?id=81100453722">John Langford</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 97-104</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143857" title="DOI">10.1145/1143844.1143857</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143857&ftid=364239&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow13" style="display:inline;"><br /><div style="display:inline">We present a tree data structure for fast nearest neighbor operations in general n-point metric spaces (where the data set consists of n points). The data structure requires O(n) space regardless of the metric's structure ...</div></span>
<span id="toHide13" style="display:none;"><br /><div style="display:inline">We present a tree data structure for fast nearest neighbor operations in general <i>n</i>-point metric spaces (where the data set consists of <i>n</i> points). The data structure requires <i>O</i>(<i>n</i>) space <i>regardless</i> of the metric's structure yet maintains all performance properties of a navigating net (Krauthgamer & Lee, 2004b). If the point set has a bounded expansion constant <i>c</i>, which is a measure of the intrinsic dimensionality, as defined in (Karger & Ruhl, 2002), the cover tree data structure can be constructed in <i>O</i> (<i>c</i><sup>6</sup><i>n</i> log <i>n</i>) time. Furthermore, nearest neighbor queries require time only logarithmic in <i>n</i>, in particular <i>O</i> (<i>c</i><sup>12</sup> log <i>n</i>) time. Our experimental results show speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets.</div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143858">Graph model selection using maximum likelihood</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81321489273">Ivona Bez&#225;kov&#225;</a>,
<a href="author_page.cfm?id=81100634828">Adam Kalai</a>,
<a href="author_page.cfm?id=81100171547">Rahul Santhanam</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 105-112</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143858" title="DOI">10.1145/1143844.1143858</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143858&ftid=364645&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow14" style="display:inline;"><br /><div style="display:inline">In recent years, there has been a proliferation of theoretical graph models, e.g., preferential attachment and small-world models, motivated by real-world graphs such as the Internet topology. To address the natural question of which model is best for ...</div></span>
<span id="toHide14" style="display:none;"><br /><div style="display:inline">In recent years, there has been a proliferation of theoretical graph models, e.g., preferential attachment and small-world models, motivated by real-world graphs such as the Internet topology. To address the natural question of which model is best for a particular data set, we propose a model selection criterion for graph models. Since each model is in fact a probability distribution over graphs, we suggest using Maximum Likelihood to compare graph models and select their parameters. Interestingly, for the case of graph models, computing likelihoods is a difficult algorithmic task. However, we design and implement MCMC algorithms for computing the maximum likelihood for four popular models: a power-law random graph model, a preferential attachment model, a small-world model, and a uniform random graph model. We hope that this novel use of ML will objectify comparisons between graph models.</div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143859">Dynamic topic models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100028344">David M. Blei</a>,
<a href="author_page.cfm?id=81100055408">John D. Lafferty</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 113-120</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143859" title="DOI">10.1145/1143844.1143859</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143859&ftid=364240&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow15" style="display:inline;"><br /><div style="display:inline">A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. ...</div></span>
<span id="toHide15" style="display:none;"><br /><div style="display:inline">A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal <i>Science</i> from 1880 through 2000.</div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143860">Predictive search distributions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315487666">Edwin V. Bonilla</a>,
<a href="author_page.cfm?id=81100005410">Christopher K. I. Williams</a>,
<a href="author_page.cfm?id=81315487735">Felix V. Agakov</a>,
<a href="author_page.cfm?id=81100096445">John Cavazos</a>,
<a href="author_page.cfm?id=81331505178">John Thomson</a>,
<a href="author_page.cfm?id=81452607145">Michael F. P. O'Boyle</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 121-128</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143860" title="DOI">10.1145/1143844.1143860</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143860&ftid=364241&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow16" style="display:inline;"><br /><div style="display:inline">Estimation of Distribution Algorithms (EDAs) are a popular approach to learn a probability distribution over the "good" solutions to a combinatorial optimization problem. Here we consider the case where there is a collection of such optimization problems ...</div></span>
<span id="toHide16" style="display:none;"><br /><div style="display:inline">Estimation of Distribution Algorithms (EDAs) are a popular approach to learn a probability distribution over the "good" solutions to a combinatorial optimization problem. Here we consider the case where there is a collection of such optimization problems with learned distributions, and where each problem can be characterized by some vector of features. Now we can define a machine learning problem to predict the distribution of good solutions <i>q</i>(<i>s</i>|<b>x</b>) for a new problem with features <b>x</b>, where <i>s</i> denotes a solution. This predictive distribution is then used to focus the search. We demonstrate the utility of our method on a compiler optimization task where the goal is to find a sequence of code transformations to make the code run fastest. Results on a set of 12 different benchmarks on two distinct architectures show that our approach consistently leads to significant improvements in performance.</div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143861">Learning predictive state representations using non-blind policies</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100395285">Michael Bowling</a>,
<a href="author_page.cfm?id=81315490654">Peter McCracken</a>,
<a href="author_page.cfm?id=81100415166">Michael James</a>,
<a href="author_page.cfm?id=81315490669">James Neufeld</a>,
<a href="author_page.cfm?id=81100304191">Dana Wilkinson</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 129-136</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143861" title="DOI">10.1145/1143844.1143861</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143861&ftid=364242&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow17" style="display:inline;"><br /><div style="display:inline">Predictive state representations (PSRs) are powerful models of non-Markovian decision processes that differ from traditional models (e.g., HMMs, POMDPs) by representing state using only observable quantities. Because of this, PSRs can be learned solely ...</div></span>
<span id="toHide17" style="display:none;"><br /><div style="display:inline">Predictive state representations (PSRs) are powerful models of non-Markovian decision processes that differ from traditional models (e.g., HMMs, POMDPs) by representing state using only observable quantities. Because of this, PSRs can be learned solely using data from interaction with the process. The majority of existing techniques, though, explicitly or implicitly require that this data be gathered using a blind policy, where actions are selected independently of preceding observations. This is a severe limitation for practical learning of PSRs. We present two methods for fixing this limitation in most of the existing PSR algorithms: one when the policy is known and one when it is not. We then present an efficient optimization for computing good exploration policies to be used when learning a PSR. The exploration policies, which are not blind, significantly lower the amount of data needed to build an accurate model, thus demonstrating the importance of non-blind policies.</div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143862">Efficient co-regularised least squares regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100150852">Ulf Brefeld</a>,
<a href="author_page.cfm?id=81100186149">Thomas G&#228;rtner</a>,
<a href="author_page.cfm?id=81100180901">Tobias Scheffer</a>,
<a href="author_page.cfm?id=81100099179">Stefan Wrobel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 137-144</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143862" title="DOI">10.1145/1143844.1143862</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143862&ftid=364646&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow18" style="display:inline;"><br /><div style="display:inline">In many applications, unlabelled examples are inexpensive and easy to obtain. Semi-supervised approaches try to utilise such examples to reduce the predictive error. In this paper, we investigate a semi-supervised least squares regression algorithm based ...</div></span>
<span id="toHide18" style="display:none;"><br /><div style="display:inline">In many applications, unlabelled examples are inexpensive and easy to obtain. Semi-supervised approaches try to utilise such examples to reduce the predictive error. In this paper, we investigate a semi-supervised least squares regression algorithm based on the co-learning approach. Similar to other semi-supervised algorithms, our base algorithm has cubic runtime complexity in the number of unlabelled examples. To be able to handle larger sets of unlabelled examples, we devise a semi-parametric variant that scales <i>linearly</i> in the number of unlabelled examples. Experiments show a significant error reduction by co-regularisation and a large runtime improvement for the semi-parametric approximation. Last but not least, we propose a distributed procedure that can be applied without collecting all data at a single site.</div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143863">Semi-supervised learning for structured output variables</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100150852">Ulf Brefeld</a>,
<a href="author_page.cfm?id=81100180901">Tobias Scheffer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 145-152</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143863" title="DOI">10.1145/1143844.1143863</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143863&ftid=364243&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow19" style="display:inline;"><br /><div style="display:inline">The problem of learning a mapping between input and structured, interdependent output variables covers sequential, spatial, and relational learning as well as predicting recursive structures. Joint feature representations of the input and output variables ...</div></span>
<span id="toHide19" style="display:none;"><br /><div style="display:inline">The problem of learning a mapping between input and structured, interdependent output variables covers sequential, spatial, and relational learning as well as predicting recursive structures. Joint feature representations of the input and output variables have paved the way to leveraging discriminative learners such as SVMs to this class of problems. We address the problem of semi-supervised learning in joint input output spaces. The co-training approach is based on the principle of maximizing the consensus among multiple independent hypotheses; we develop this principle into a semi-supervised support vector learning algorithm for joint input output spaces and arbitrary loss functions. Experiments investigate the benefit of semi-supervised structured models in terms of accuracy and F1 score.</div></span> <a id="expcoll19" href="JavaScript: expandcollapse('expcoll19',19)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143864">Fast nonparametric clustering with Gaussian blurring mean-shift</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100235215">Miguel &#193;. Carreira-Perpi&#241;&#225;n</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 153-160</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143864" title="DOI">10.1145/1143844.1143864</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143864&ftid=364244&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow20" style="display:inline;"><br /><div style="display:inline">We revisit Gaussian blurring mean-shift (GBMS), a procedure that iteratively sharpens a dataset by moving each data point according to the Gaussian mean-shift algorithm (GMS). (1) We give a criterion to stop the procedure as soon as clustering structure ...</div></span>
<span id="toHide20" style="display:none;"><br /><div style="display:inline">We revisit Gaussian blurring mean-shift (GBMS), a procedure that iteratively sharpens a dataset by moving each data point according to the Gaussian mean-shift algorithm (GMS). (1) We give a criterion to stop the procedure as soon as clustering structure has arisen and show that this reliably produces image segmentations as good as those of GMS but much faster. (2) We prove that GBMS has convergence of cubic order with Gaussian clusters (much faster than GMS's, which is of linear order) and that the local principal component converges last, which explains the powerful clustering and denoising properties of GBMS. (3) We show a connection with spectral clustering that suggests GBMS is much faster. (4) We further accelerate GBMS by interleaving connected-components and blurring steps, achieving 2x--4x speedups without introducing an approximation error. In summary, our accelerated GBMS is a simple, fast, nonparametric algorithm that achieves segmentations of state-of-the-art quality.</div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143865">An empirical comparison of supervised learning algorithms</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100100877">Rich Caruana</a>,
<a href="author_page.cfm?id=81309509185">Alexandru Niculescu-Mizil</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 161-168</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143865" title="DOI">10.1145/1143844.1143865</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143865&ftid=364245&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow21" style="display:inline;"><br /><div style="display:inline">A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison ...</div></span>
<span id="toHide21" style="display:none;"><br /><div style="display:inline">A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the effect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods.</div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143866">Robust Euclidean embedding</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421595307">Lawrence Cayton</a>,
<a href="author_page.cfm?id=81100357774">Sanjoy Dasgupta</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 169-176</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143866" title="DOI">10.1145/1143844.1143866</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143866&ftid=364246&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow22" style="display:inline;"><br /><div style="display:inline">We derive a robust Euclidean embedding procedure based on semidefinite programming that may be used in place of the popular classical multidimensional scaling (cMDS) algorithm. We motivate this algorithm by arguing that cMDS is not particularly robust ...</div></span>
<span id="toHide22" style="display:none;"><br /><div style="display:inline">We derive a robust Euclidean embedding procedure based on semidefinite programming that may be used in place of the popular classical multidimensional scaling (cMDS) algorithm. We motivate this algorithm by arguing that cMDS is not particularly robust and has several other deficiencies. General-purpose semidefinite programming solvers are too memory intensive for medium to large sized applications, so we also describe a fast subgradient-based implementation of the robust algorithm. Additionally, since cMDS is often used for dimensionality reduction, we provide an in-depth look at reducing dimensionality with embedding procedures. In particular, we show that it is NP-hard to find optimal low-dimensional embeddings under a variety of cost functions.</div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143867">Hierarchical classification: combining Bayes with SVM</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100023564">Nicol&#242; Cesa-Bianchi</a>,
<a href="author_page.cfm?id=81100176377">Claudio Gentile</a>,
<a href="author_page.cfm?id=81339540198">Luca Zaniboni</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 177-184</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143867" title="DOI">10.1145/1143844.1143867</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143867&ftid=364247&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow23" style="display:inline;"><br /><div style="display:inline">We study hierarchical classification in the general case when an instance could belong to more than one class node in the underlying taxonomy. Experiments done in previous work showed that a simple hierarchy of Support Vectors Machines (SVM) with a top-down ...</div></span>
<span id="toHide23" style="display:none;"><br /><div style="display:inline">We study hierarchical classification in the general case when an instance could belong to more than one class node in the underlying taxonomy. Experiments done in previous work showed that a simple hierarchy of Support Vectors Machines (SVM) with a top-down evaluation scheme has a surprisingly good performance on this kind of task. In this paper, we introduce a refined evaluation scheme which turns the hierarchical SVM classifier into an approximator of the Bayes optimal classifier with respect to a simple stochastic model for the labels. Experiments on synthetic datasets, generated according to this stochastic model, show that our refined algorithm outperforms the simple hierarchical SVM. On real-world data, however, the advantage brought by our approach is a bit less clear. We conjecture this is due to a higher noise rate for the training labels in the low levels of the taxonomy.</div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143868">A continuation method for semi-supervised SVMs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100497375">Olivier Chapelle</a>,
<a href="author_page.cfm?id=81333487902">Mingmin Chi</a>,
<a href="author_page.cfm?id=81100149328">Alexander Zien</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 185-192</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143868" title="DOI">10.1145/1143844.1143868</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143868&ftid=364248&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow24" style="display:inline;"><br /><div style="display:inline">Semi-Supervised Support Vector Machines (S3VMs) are an appealing method for using unlabeled data in classification: their objective function favors decision boundaries which do not cut clusters. However their main problem is that the optimization ...</div></span>
<span id="toHide24" style="display:none;"><br /><div style="display:inline">Semi-Supervised Support Vector Machines (S<sup>3</sup>VMs) are an appealing method for using unlabeled data in classification: their objective function favors decision boundaries which do not cut clusters. However their main problem is that the optimization problem is non-convex and has many <i>local minima</i>, which often results in suboptimal performances. In this paper we propose to use a global optimization technique known as <i>continuation</i> to alleviate this problem. Compared to other algorithms minimizing the same objective function, our continuation method often leads to lower test errors.</div></span> <a id="expcoll24" href="JavaScript: expandcollapse('expcoll24',24)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143869">A regularization framework for multiple-instance learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100460165">Pak-Ming Cheung</a>,
<a href="author_page.cfm?id=81100525095">James T. Kwok</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 193-200</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143869" title="DOI">10.1145/1143844.1143869</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143869&ftid=364249&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow25" style="display:inline;"><br /><div style="display:inline">This paper focuses on kernel methods for multi-instance learning. Existing methods require the prediction of the bag to be identical to the maximum of those of its individual instances. However, this is too restrictive as only the sign is important in ...</div></span>
<span id="toHide25" style="display:none;"><br /><div style="display:inline">This paper focuses on kernel methods for multi-instance learning. Existing methods require the prediction of the bag to be identical to the maximum of those of its individual instances. However, this is too restrictive as only the sign is important in classification. In this paper, we provide a more complete regularization framework for MI learning by allowing the use of different loss functions between the outputs of a bag and its associated instances. This is especially important as we generalize this for multi-instance regression. Moreover, both bag and instance information can now be directly used in the optimization. Instead of using heuristics to solve the resultant non-linear optimization problem, we use the constrained concave-convex procedure which has well-studied convergence properties. Experiments on both classification and regression data sets show that the proposed method leads to improved performance.</div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143870">Trading convexity for scalability</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100001072">Ronan Collobert</a>,
<a href="author_page.cfm?id=81330498356">Fabian Sinz</a>,
<a href="author_page.cfm?id=81100015405">Jason Weston</a>,
<a href="author_page.cfm?id=81100263096">L&#233;on Bottou</a>
 </span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 201-208</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143870" title="DOI">10.1145/1143844.1143870</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143870&ftid=364250&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow26" style="display:inline;"><br /><div style="display:inline">Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide ...</div></span>
<span id="toHide26" style="display:none;"><br /><div style="display:inline">Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide scalability advantages over convexity. We show how concave-convex programming can be applied to produce (<i>i</i>) faster SVMs where training errors are no longer support vectors, and (<i>ii</i>) much faster Transductive SVMs.</div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143871">Learning algorithms for online principal-agent problems (and selling goods online)</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100416361">Vincent Conitzer</a>,
<a href="author_page.cfm?id=81323490129">Nikesh Garera</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 209-216</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143871" title="DOI">10.1145/1143844.1143871</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143871&ftid=364251&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow27" style="display:inline;"><br /><div style="display:inline">In a principal-agent problem, a principal seeks to motivate an agent to take a certain action beneficial to the principal, while spending as little as possible on the reward. This is complicated by the fact that the principal does not know the ...</div></span>
<span id="toHide27" style="display:none;"><br /><div style="display:inline">In a <i>principal-agent problem</i>, a principal seeks to motivate an agent to take a certain action beneficial to the principal, while spending as little as possible on the reward. This is complicated by the fact that the principal does not know the agent's utility function (or <i>type</i>). We study the online setting where at each round, the principal encounters a new agent, and the principal sets the rewards anew. At the end of each round, the principal only finds out the action that the agent took, but not his type. The principal must learn how to set the rewards optimally. We show that this setting generalizes the setting of selling a digital good online.We study and experimentally compare three main approaches to this problem. First, we show how to apply a standard bandit algorithm to this setting. Second, for the case where the distribution of agent types is fixed (but unknown to the principal), we introduce a new gradient ascent algorithm. Third, for the case where the distribution of agents' types is fixed, and the principal has a prior belief (distribution) over a limited class of type distributions, we study a Bayesian approach.</div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143872">Dealing with non-stationary environments using context detection</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315488457">Bruno C. da Silva</a>,
<a href="author_page.cfm?id=81315487907">Eduardo W. Basso</a>,
<a href="author_page.cfm?id=81100208621">Ana L. C. Bazzan</a>,
<a href="author_page.cfm?id=81320489237">Paulo M. Engel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 217-224</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143872" title="DOI">10.1145/1143844.1143872</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143872&ftid=364252&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow28" style="display:inline;"><br /><div style="display:inline">In this paper we introduce RL-CD, a method for solving reinforcement learning problems in non-stationary environments. The method is based on a mechanism for creating, updating and selecting one among several partial models of the environment. The partial ...</div></span>
<span id="toHide28" style="display:none;"><br /><div style="display:inline">In this paper we introduce RL-CD, a method for solving reinforcement learning problems in non-stationary environments. The method is based on a mechanism for creating, updating and selecting one among several partial models of the environment. The partial models are incrementally built according to the system's capability of making predictions regarding a given sequence of observations. We propose, formalize and show the efficiency of this method both in a simple non-stationary environment and in a noisy scenario. We show that RL-CD performs better than two standard reinforcement learning algorithms and that it has advantages over methods specifically designed to cope with non-stationarity. Finally, we present known limitations of the method and future works.</div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143873">Locally adaptive classification piloted by uncertainty</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81543987856">Juan Dai</a>,
<a href="author_page.cfm?id=81100044797">Shuicheng Yan</a>,
<a href="author_page.cfm?id=81452597691">Xiaoou Tang</a>,
<a href="author_page.cfm?id=81100525095">James T. Kwok</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 225-232</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143873" title="DOI">10.1145/1143844.1143873</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143873&ftid=364253&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow29" style="display:inline;"><br /><div style="display:inline">Locally adaptive classifiers are usually superior to the use of a single global classifier. However, there are two major problems in designing locally adaptive classifiers. First, how to place the local classifiers, and, second, how to combine them together. ...</div></span>
<span id="toHide29" style="display:none;"><br /><div style="display:inline">Locally adaptive classifiers are usually superior to the use of a single global classifier. However, there are two major problems in designing locally adaptive classifiers. First, how to place the local classifiers, and, second, how to combine them together. In this paper, instead of placing the classifiers based on the data distribution only, we propose a <i>responsibility mixture model</i> that uses the uncertainty associated with the classification at each training sample. Using this model, the local classifiers are placed near the decision boundary where they are most effective. A set of local classifiers are then learned to form a global classifier by maximizing an estimate of the probability that the samples will be correctly classified with a nearest neighbor classifier. Experimental results on both artificial and real-world data sets demonstrate its superiority over traditional algorithms.</div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143874">The relationship between Precision-Recall and ROC curves</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100603602">Jesse Davis</a>,
<a href="author_page.cfm?id=81331493217">Mark Goadrich</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 233-240</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143874" title="DOI">10.1145/1143844.1143874</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143874&ftid=364254&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow30" style="display:inline;"><br /><div style="display:inline">Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an ...</div></span>
<span id="toHide30" style="display:none;"><br /><div style="display:inline">Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.</div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143875">Discriminative cluster analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100338725">Fernando De la Torre</a>,
<a href="author_page.cfm?id=81100496698">Takeo Kanade</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 241-248</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143875" title="DOI">10.1145/1143844.1143875</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143875&ftid=364255&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow31" style="display:inline;"><br /><div style="display:inline">Clustering is one of the most widely used statistical tools for data analysis. Among all existing clustering techniques, k-means is a very popular method because of its ease of programming and because it accomplishes a good trade-off between achieved ...</div></span>
<span id="toHide31" style="display:none;"><br /><div style="display:inline">Clustering is one of the most widely used statistical tools for data analysis. Among all existing clustering techniques, k-means is a very popular method because of its ease of programming and because it accomplishes a good trade-off between achieved performance and computational complexity. However, k-means is prone to local minima problems, and it does not scale too well with high dimensional data sets. A common approach to dealing with high dimensional data is to cluster in the space spanned by the principal components (PC). In this paper, we show the benefits of clustering in a low dimensional discriminative space rather than in the PC space (generative). In particular, we propose a new clustering algorithm called Discriminative Cluster Analysis (DCA). DCA jointly performs dimensionality reduction and clustering. Several toy and real examples show the benefits of DCA versus traditional PCA+k-means clustering. Additionally, a new matrix formulation is proposed and connections with related techniques such as spectral graph methods and linear discriminant analysis are provided.</div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143876">Collaborative prediction using ensembles of Maximum Margin Matrix Factorizations</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100571429">Dennis DeCoste</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 249-256</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143876" title="DOI">10.1145/1143844.1143876</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143876&ftid=364256&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow32" style="display:inline;"><br /><div style="display:inline">Fast gradient-based methods for Maximum Margin Matrix Factorization (MMMF) were recently shown to have great promise (Rennie & Srebro, 2005), including significantly outperforming the previous state-of-the-art methods on some standard collaborative prediction ...</div></span>
<span id="toHide32" style="display:none;"><br /><div style="display:inline">Fast gradient-based methods for Maximum Margin Matrix Factorization (MMMF) were recently shown to have great promise (Rennie & Srebro, 2005), including significantly outperforming the previous state-of-the-art methods on some standard collaborative prediction benchmarks (including MovieLens). In this paper, we investigate ways to further improve the performance of MMMF, by casting it within an ensemble approach. We explore and evaluate a variety of alternative ways to define such ensembles. We show that our resulting ensembles can perform significantly better than a single MMMF model, along multiple evaluation metrics. In fact, we find that ensembles of partially trained MMMF models can sometimes even give better predictions in total training time comparable to a single MMMF model.</div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143877">Learning the structure of Factored Markov Decision Processes in reinforcement learning problems</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81388597879">Thomas Degris</a>,
<a href="author_page.cfm?id=81100002877">Olivier Sigaud</a>,
<a href="author_page.cfm?id=81100264263">Pierre-Henri Wuillemin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 257-264</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143877" title="DOI">10.1145/1143844.1143877</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143877&ftid=364257&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow33" style="display:inline;"><br /><div style="display:inline">Recent decision-theoric planning algorithms are able to find optimal solutions in large problems, using Factored Markov Decision Processes (FMDPs). However, these algorithms need a perfect knowledge of the structure of the problem. In this paper, we ...</div></span>
<span id="toHide33" style="display:none;"><br /><div style="display:inline">Recent decision-theoric planning algorithms are able to find optimal solutions in large problems, using Factored Markov Decision Processes (FMDPs). However, these algorithms need a perfect knowledge of the structure of the problem. In this paper, we propose SDYNA, a general framework for addressing large reinforcement learning problems by trial-and-error and with no initial knowledge of their structure. SDYNA integrates incremental planning algorithms based on FMDPs with supervised learning techniques building structured representations of the problem. We describe SPITI, an instantiation of SDYNA, that uses incremental decision tree induction to learn the structure of a problem combined with an incremental version of the <i>Structured Value Iteration</i> algorithm. We show that SPITI can build a factored representation of a reinforcement learning problem and may improve the policy faster than tabular reinforcement learning algorithms by exploiting the generalization property of decision tree induction algorithms.</div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143878">Efficient learning of Naive Bayes classifiers under class-conditional classification noise</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100508538">Fran&#231;ois Denis</a>,
<a href="author_page.cfm?id=99658678486">Christophe Nicolas Magnan</a>,
<a href="author_page.cfm?id=81100251676">Liva Ralaivola</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 265-272</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143878" title="DOI">10.1145/1143844.1143878</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143878&ftid=364258&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow34" style="display:inline;"><br /><div style="display:inline">We address the problem of efficiently learning Naive Bayes classifiers under class-conditional classification noise (CCCN). Naive Bayes classifiers rely on the hypothesis that the distributions associated to each class are product distributions. When ...</div></span>
<span id="toHide34" style="display:none;"><br /><div style="display:inline">We address the problem of efficiently learning Naive Bayes classifiers under class-conditional classification noise (CCCN). Naive Bayes classifiers rely on the hypothesis that the distributions associated to each class are product distributions. When data is subject to CCC-noise, these conditional distributions are themselves mixtures of product distributions. We give analytical formulas which makes it possible to identify them from data subject to CCCN. Then, we design a learning algorithm based on these formulas able to learn Naive Bayes classifiers under CCCN. We present results on artificial datasets and datasets extracted from the UCI repository database. These results show that CCCN can be efficiently and successfully handled.</div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143879">Learning user preferences for sets of objects</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100325069">Marie desJardins</a>,
<a href="author_page.cfm?id=81381597668">Eric Eaton</a>,
<a href="author_page.cfm?id=81100138325">Kiri L. Wagstaff</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 273-280</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143879" title="DOI">10.1145/1143844.1143879</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143879&ftid=364259&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow35" style="display:inline;"><br /><div style="display:inline">Most work on preference learning has focused on pairwise preferences or rankings over individual items. In this paper, we present a method for learning preferences over sets of items. Our learning method takes as input a collection of positive ...</div></span>
<span id="toHide35" style="display:none;"><br /><div style="display:inline">Most work on preference learning has focused on pairwise preferences or rankings over individual items. In this paper, we present a method for learning preferences over <i>sets</i> of items. Our learning method takes as input a collection of positive examples---that is, one or more sets that have been identified by a user as desirable. Kernel density estimation is used to estimate the value function for individual items, and the desired set diversity is estimated from the average set diversity observed in the collection. Since this is a new learning problem, we introduce a new evaluation methodology and evaluate the learning method on two data collections: synthetic blocks-world data and a new real-world music data collection that we have gathered.</div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143880"><i>R</i><sub>1</sub>-PCA: rotational invariant <i>L</i><sub>1</sub>-norm principal component analysis for robust subspace factorization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100136610">Chris Ding</a>,
<a href="author_page.cfm?id=81309500562">Ding Zhou</a>,
<a href="author_page.cfm?id=81100332307">Xiaofeng He</a>,
<a href="author_page.cfm?id=81100528811">Hongyuan Zha</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 281-288</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143880" title="DOI">10.1145/1143844.1143880</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143880&ftid=364260&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow36" style="display:inline;"><br /><div style="display:inline">Principal component analysis (PCA) minimizes the sum of squared errors (L2-norm) and is sensitive to the presence of outliers. We propose a rotational invariant L1-norm PCA (R1-PCA). R1-PCA ...</div></span>
<span id="toHide36" style="display:none;"><br /><div style="display:inline">Principal component analysis (PCA) minimizes the sum of squared errors (<i>L</i><inf>2</inf>-norm) and is sensitive to the presence of outliers. We propose a <i>rotational invariant L</i><inf>1</inf>-norm PCA (<i>R</i><inf>1</inf>-PCA). <i>R</i><inf>1</inf>-PCA is similar to PCA in that (1) it has a unique global solution, (2) the solution are principal eigenvectors of a robust covariance matrix (re-weighted to soften the effects of outliers), (3) the solution is rotational invariant. These properties are not shared by the <i>L</i><inf>1</inf>-norm PCA. A new subspace iteration algorithm is given to compute <i>R</i><inf>1</inf>-PCA efficiently. Experiments on several real-life datasets show <i>R</i><inf>1</inf>-PCA can effectively handle outliers. We extend <i>R</i><inf>1</inf>-norm to <i>K</i>-means clustering and show that <i>L</i><inf>1</inf>-norm <i>K</i>-means leads to poor results while <i>R</i><inf>1</inf>-<i>K</i>-means outperforms standard <i>K</i>-means.</div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143881">Clustering documents with an exponential-family approximation of the Dirichlet compound multinomial distribution</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81339498029">Charles Elkan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 289-296</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143881" title="DOI">10.1145/1143844.1143881</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143881&ftid=364261&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow37" style="display:inline;"><br /><div style="display:inline">The Dirichlet compound multinomial (DCM) distribution, also called the multivariate Polya distribution, is a model for text documents that takes into account burstiness: the fact that if a word occurs once in a document, it is likely to occur repeatedly. ...</div></span>
<span id="toHide37" style="display:none;"><br /><div style="display:inline">The Dirichlet compound multinomial (DCM) distribution, also called the multivariate Polya distribution, is a model for text documents that takes into account burstiness: the fact that if a word occurs once in a document, it is likely to occur repeatedly. We derive a new family of distributions that are approximations to DCM distributions and constitute an exponential family, unlike DCM distributions. We use these so-called EDCM distributions to obtain insights into the properties of DCM distributions, and then derive an algorithm for EDCM maximum-likelihood training that is many times faster than the corresponding method for DCM distributions. Next, we investigate expectation-maximization with EDCM components and deterministic annealing as a new clustering algorithm for documents. Experiments show that the new algorithm is competitive with the best methods in the literature, and superior from the point of view of finding models with low perplexity.</div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143882">A graphical model for predicting protein molecular function</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100303171">Barbara E. Engelhardt</a>,
<a href="author_page.cfm?id=81339507945">Michael I. Jordan</a>,
<a href="author_page.cfm?id=81100396495">Steven E. Brenner</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 297-304</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143882" title="DOI">10.1145/1143844.1143882</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143882&ftid=364262&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow38" style="display:inline;"><br /><div style="display:inline">We present a simple statistical model of molecular function evolution to predict protein function. The model description encodes general knowledge of how molecular function evolves within a phylogenetic tree based on the proteins' sequence. Inputs are ...</div></span>
<span id="toHide38" style="display:none;"><br /><div style="display:inline">We present a simple statistical model of molecular function evolution to predict protein function. The model description encodes general knowledge of how molecular function evolves within a phylogenetic tree based on the proteins' sequence. Inputs are a phylogeny for a set of evolutionarily related protein sequences and any available function characterizations for those proteins. Posterior probabilities for each protein are used to predict the molecular function of that protein. We present results from applying our model to three protein families, and compare our prediction results on the extant proteins to other available protein function prediction methods. For the deaminase family, our method achieves 93.9% where related methods BLAST achieves 72.7%, GOtcha achieves 87.9%, and Orthostrapper achieves 72.7% in prediction accuracy.</div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143883">Qualitative reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81361603566">Arkady Epshteyn</a>,
<a href="author_page.cfm?id=81100619998">Gerald DeJong</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 305-312</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143883" title="DOI">10.1145/1143844.1143883</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143883&ftid=364263&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow39" style="display:inline;"><br /><div style="display:inline">When the transition probabilities and rewards of a Markov Decision Process are specified exactly, the problem can be solved without any interaction with the environment. When no such specification is available, the agent's only recourse is a long and ...</div></span>
<span id="toHide39" style="display:none;"><br /><div style="display:inline">When the transition probabilities and rewards of a Markov Decision Process are specified exactly, the problem can be solved without any interaction with the environment. When no such specification is available, the agent's only recourse is a long and potentially dangerous exploration. We present a framework which allows the expert to specify imprecise knowledge of transition probabilities in terms of stochastic dominance constraints. Our algorithm can be used to find optimal policies for qualitatively specified problems, or, when no such solution is available, to decrease the required amount of exploration. The algorithm's behavior is demonstrated on simulations of two classic problems: mountain car ascent and cart pole balancing.</div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143884">Online multiclass learning by interclass hypothesis sharing</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81336489159">Michael Fink</a>,
<a href="author_page.cfm?id=81100489319">Shai Shalev-Shwartz</a>,
<a href="author_page.cfm?id=81100308085">Yoram Singer</a>,
<a href="author_page.cfm?id=81331505935">Shimon Ullman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 313-320</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143884" title="DOI">10.1145/1143844.1143884</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143884&ftid=364264&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow40" style="display:inline;"><br /><div style="display:inline">We describe a general framework for online multiclass learning based on the notion of hypothesis sharing. In our framework sets of classes are associated with hypotheses. Thus, all classes within a given set share the same hypothesis. This framework ...</div></span>
<span id="toHide40" style="display:none;"><br /><div style="display:inline">We describe a general framework for online multiclass learning based on the notion of hypothesis sharing. In our framework sets of classes are associated with hypotheses. Thus, all classes within a given set share the same hypothesis. This framework includes as special cases commonly used constructions for multiclass categorization such as allocating a unique hypothesis for each class and allocating a single common hypothesis for all classes. We generalize the multiclass Perceptron to our framework and derive a unifying mistake bound analysis. Our construction naturally extends to settings where the number of classes is not known in advance but, rather, is revealed along the online learning process. We demonstrate the merits of our approach by comparing it to previous methods on both synthetic and natural datasets.</div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143885">Regression with the optimised combination technique</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100414749">Jochen Garcke</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 321-328</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143885" title="DOI">10.1145/1143844.1143885</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143885&ftid=364265&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow41" style="display:inline;"><br /><div style="display:inline">We consider the sparse grid combination technique for regression, which we regard as a problem of function reconstruction in some given function space. We use a regularised least squares approach, discretised by sparse grids and solved using the so-called ...</div></span>
<span id="toHide41" style="display:none;"><br /><div style="display:inline">We consider the sparse grid combination technique for regression, which we regard as a problem of function reconstruction in some given function space. We use a regularised least squares approach, discretised by sparse grids and solved using the so-called combination technique, where a certain sequence of conventional grids is employed. The sparse grid solution is then obtained by addition of the partial solutions with combination co-efficients dependent on the involved grids. This approach shows instabilities in certain situations and is not guaranteed to converge with higher discretisation levels. In this article we apply the recently introduced optimised combination technique, which repairs these instabilities. Now the combination coefficients also depend on the function to be reconstructed, resulting in a non-linear approximation method which achieves very competitive results. We show that the computational complexity of the improved method still scales only linear in regard to the number of data.</div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143886">A note on mixtures of experts for multiclass responses: approximation rate and Consistent Bayesian Inference</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81310492200">Yang Ge</a>,
<a href="author_page.cfm?id=81319494645">Wenxin Jiang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 329-335</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143886" title="DOI">10.1145/1143844.1143886</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143886&ftid=364266&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow42" style="display:inline;"><br /><div style="display:inline">We report that mixtures of m multinomial logistic regression can be used to approximate a class of 'smooth' probability models for multiclass responses. With bounded second derivatives of log-odds, the approximation rate is O(m-2/s) ...</div></span>
<span id="toHide42" style="display:none;"><br /><div style="display:inline">We report that mixtures of <i>m</i> multinomial logistic regression can be used to approximate a class of 'smooth' probability models for multiclass responses. With bounded second derivatives of log-odds, the approximation rate is <i>O</i>(<i>m</i><sup>-2/<i>s</i></sup>) in Hellinger distance or <i>O</i>(<i>m</i><sup>-4/<i>s</i></sup>) in Kullback-Leibler divergence. Here <i>s</i> = dim(<i>x</i>) is the dimension of the input space (or the number of predictors). With the availability of training data of size <i>n</i>, we also show that 'consistency' in multiclass regression and classification can be achieved, simultaneously for all classes, when posterior based inference is performed in a Bayesian framework. Loosely speaking, such 'consistency' refers to performance being often close to the best possible for large <i>n</i>. Consistency can be achieved either by taking <i>m</i> = <i>m</i><inf><i>n</i></inf>, or by taking <i>m</i> to be uniformly distributed among {1, ...,<i>m</i><inf><i>n</i></inf>} according to the prior, where 1 &pr; <i>m</i><inf><i>n</i></inf> &pr; <i>n</i><sup><i>a</i></sup> in order as <i>n</i> grows, for some <i>a</i> &isin; (0, 1).</div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143887">The rate adapting poisson model for information retrieval and object recognition</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315488753">Peter V. Gehler</a>,
<a href="author_page.cfm?id=81300347901">Alex D. Holub</a>,
<a href="author_page.cfm?id=81100461506">Max Welling</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 337-344</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143887" title="DOI">10.1145/1143844.1143887</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143887&ftid=364544&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow43" style="display:inline;"><br /><div style="display:inline">Probabilistic modelling of text data in the bag-of-words representation has been dominated by directed graphical models such as pLSI, LDA, NMF, and discrete PCA. Recently, state of the art performance on visual object recognition has also been reported ...</div></span>
<span id="toHide43" style="display:none;"><br /><div style="display:inline">Probabilistic modelling of text data in the bag-of-words representation has been dominated by directed graphical models such as pLSI, LDA, NMF, and discrete PCA. Recently, state of the art performance on visual object recognition has also been reported using variants of these models. We introduce an alternative undirected graphical model suitable for modelling count data. This "Rate Adapting Poisson" (RAP) model is shown to generate superior dimensionally reduced representations for subsequent retrieval or classification. Models are trained using contrastive divergence while inference of latent topical representations is efficiently achieved through a simple matrix multiplication.</div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143888">Kernelizing the output of tree-based methods</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100360352">Pierre Geurts</a>,
<a href="author_page.cfm?id=81100172542">Louis Wehenkel</a>,
<a href="author_page.cfm?id=81100133327">Florence d'Alch&#233;-Buc</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 345-352</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143888" title="DOI">10.1145/1143844.1143888</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143888&ftid=364545&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow44" style="display:inline;"><br /><div style="display:inline">We extend tree-based methods to the prediction of structured outputs using a kernelization of the algorithm that allows one to grow trees as soon as a kernel can be defined on the output space. The resulting algorithm, called output kernel trees (OK3), ...</div></span>
<span id="toHide44" style="display:none;"><br /><div style="display:inline">We extend tree-based methods to the prediction of structured outputs using a kernelization of the algorithm that allows one to grow trees as soon as a kernel can be defined on the output space. The resulting algorithm, called output kernel trees (OK3), generalizes classification and regression trees as well as tree-based ensemble methods in a principled way. It inherits several features of these methods such as interpretability, robustness to irrelevant variables, and input scalability. When only the Gram matrix over the outputs of the learning sample is given, it learns the output kernel as a function of inputs. We show that the proposed algorithm works well on an image reconstruction task and on a biological network inference problem.</div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143889">Nightmare at test time: robust learning by feature deletion</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100607997">Amir Globerson</a>,
<a href="author_page.cfm?id=81100445880">Sam Roweis</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 353-360</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143889" title="DOI">10.1145/1143844.1143889</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143889&ftid=364546&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow45" style="display:inline;"><br /><div style="display:inline">When constructing a classifier from labeled data, it is important not to assign too much weight to any single input feature, in order to increase the robustness of the classifier. This is particularly important in domains with nonstationary feature distributions ...</div></span>
<span id="toHide45" style="display:none;"><br /><div style="display:inline">When constructing a classifier from labeled data, it is important not to assign too much weight to any single input feature, in order to increase the robustness of the classifier. This is particularly important in domains with nonstationary feature distributions or with input sensor failures. A common approach to achieving such robustness is to introduce regularization which <i>spreads</i> the weight more evenly between the features. However, this strategy is very generic, and cannot induce robustness specifically tailored to the classification task at hand. In this work, we introduce a new algorithm for avoiding single feature over-weighting by analyzing robustness using a game theoretic formalization. We develop classifiers which are optimally resilient to <i>deletion</i> of features in a minimax sense, and show how to construct such classifiers using quadratic programming. We illustrate the applicability of our methods on spam filtering and handwritten digit recognition tasks, where feature deletion is indeed a realistic noise model.</div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143890">A choice model with infinitely many latent features</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315488842">Dilan G&#246;r&#252;r</a>,
<a href="author_page.cfm?id=81430642523">Frank J&#228;kel</a>,
<a href="author_page.cfm?id=81309509063">Carl Edward Rasmussen</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 361-368</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143890" title="DOI">10.1145/1143844.1143890</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143890&ftid=364547&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow46" style="display:inline;"><br /><div style="display:inline">Elimination by aspects (EBA) is a probabilistic choice model describing how humans decide between several options. The options from which the choice is made are characterized by binary features and associated weights. For instance, when choosing which ...</div></span>
<span id="toHide46" style="display:none;"><br /><div style="display:inline">Elimination by aspects (EBA) is a probabilistic choice model describing how humans decide between several options. The options from which the choice is made are characterized by binary features and associated weights. For instance, when choosing which mobile phone to buy the features to consider may be: long lasting battery, color screen, etc. Existing methods for inferring the parameters of the model assume pre-specified features. However, the features that lead to the observed choices are not always known. Here, we present a non-parametric Bayesian model to infer the features of the options and the corresponding weights from choice data. We use the Indian buffet process (IBP) as a prior over the features. Inference using Markov chain Monte Carlo (MCMC) in conjugate IBP models has been previously described. The main contribution of this paper is an MCMC algorithm for the EBA model that can also be used in inference for other non-conjugate IBP models---this may broaden the use of IBP priors considerably.</div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143891">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81310503225">Alex Graves</a>,
<a href="author_page.cfm?id=81416596092">Santiago Fern&#225;ndez</a>,
<a href="author_page.cfm?id=81381597688">Faustino Gomez</a>,
<a href="author_page.cfm?id=81409592380">J&#252;rgen Schmidhuber</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 369-376</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143891" title="DOI">10.1145/1143844.1143891</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143891&ftid=364548&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow47" style="display:inline;"><br /><div style="display:inline">Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) ...</div></span>
<span id="toHide47" style="display:none;"><br /><div style="display:inline">Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.</div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143892">Practical solutions to the problem of diagonal dominance in kernel document clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315489014">Derek Greene</a>,
<a href="author_page.cfm?id=81100562954">P&#225;draig Cunningham</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 377-384</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143892" title="DOI">10.1145/1143844.1143892</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143892&ftid=364549&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow48" style="display:inline;"><br /><div style="display:inline">In supervised kernel methods, it has been observed that the performance of the SVM classifier is poor in cases where the diagonal entries of the Gram matrix are large relative to the off-diagonal entries. This problem, referred to as diagonal dominance, ...</div></span>
<span id="toHide48" style="display:none;"><br /><div style="display:inline">In supervised kernel methods, it has been observed that the performance of the SVM classifier is poor in cases where the diagonal entries of the Gram matrix are large relative to the off-diagonal entries. This problem, referred to as <i>diagonal dominance</i>, often occurs when certain kernel functions are applied to sparse high-dimensional data, such as text corpora. In this paper we investigate the implications of diagonal dominance for unsupervised kernel methods, specifically in the task of document clustering. We propose a selection of strategies for addressing this issue, and evaluate their effectiveness in producing more accurate and stable clusterings.</div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143893">Fast transpose methods for kernel learning on sparse data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100405478">Patrick Haffner</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 385-392</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143893" title="DOI">10.1145/1143844.1143893</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143893&ftid=364550&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow49" style="display:inline;"><br /><div style="display:inline">Kernel-based learning algorithms, such as Support Vector Machines (SVMs) or Perceptron, often rely on sequential optimization where a few examples are added at each iteration. Updating the kernel matrix usually requires matrix-vector multiplications. ...</div></span>
<span id="toHide49" style="display:none;"><br /><div style="display:inline">Kernel-based learning algorithms, such as Support Vector Machines (SVMs) or Perceptron, often rely on sequential optimization where a few examples are added at each iteration. Updating the kernel matrix usually requires matrix-vector multiplications. We propose a new method based on transposition to speedup this computation on sparse data. Instead of dot-products over sparse feature vectors, our computation incrementally merges lists of training examples and minimizes access to the data. Caching and shrinking are also optimized for sparsity. On very large natural language tasks (tagging, translation, text classification) with sparse feature representations, a 20 to 80-fold speedup over LIBSVM is observed using the same SMO algorithm. Theory and experiments explain what type of sparsity structure is needed for this approach to work, and why its adaptation to Maxent sequential optimization is inefficient.</div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143894">An analysis of graph cut size for transductive learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315488801">Steve Hanneke</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 393-399</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143894" title="DOI">10.1145/1143844.1143894</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143894&ftid=364551&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow50" style="display:inline;"><br /><div style="display:inline">I consider the setting of transductive learning of vertex labels in graphs, in which a graph with n vertices is sampled according to some unknown distribution; there is a true labeling of the vertices such that each vertex is assigned to exactly ...</div></span>
<span id="toHide50" style="display:none;"><br /><div style="display:inline">I consider the setting of transductive learning of vertex labels in graphs, in which a graph with <i>n</i> vertices is sampled according to some unknown distribution; there is a true labeling of the vertices such that each vertex is assigned to exactly one of <i>k</i> classes, but the labels of only some (random) subset of the vertices are revealed to the learner. The task is then to find a labeling of the remaining (unlabeled) vertices that agrees as much as possible with the true labeling. Several existing algorithms are based on the assumption that adjacent vertices are usually labeled the same. In order to better understand algorithms based on this assumption, I derive data-dependent bounds on the fraction of mislabeled vertices, based on the number (or total weight) of edges between vertices differing in predicted label (i.e., the size of the <i>cut</i>).</div></span> <a id="expcoll50" href="JavaScript: expandcollapse('expcoll50',50)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143895">Learning a kernel function for classification with small training samples</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100478711">Tomer Hertz</a>,
<a href="author_page.cfm?id=81300336101">Aharon Bar Hillel</a>,
<a href="author_page.cfm?id=81100572427">Daphna Weinshall</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 401-408</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143895" title="DOI">10.1145/1143844.1143895</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143895&ftid=364552&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow51" style="display:inline;"><br /><div style="display:inline">When given a small sample, we show that classification with SVM can be considerably enhanced by using a kernel function learned from the training data prior to discrimination. This kernel is also shown to enhance retrieval based on data similarity. Specifically, ...</div></span>
<span id="toHide51" style="display:none;"><br /><div style="display:inline">When given a small sample, we show that classification with SVM can be considerably enhanced by using a kernel function learned from the training data prior to discrimination. This kernel is also shown to enhance retrieval based on data similarity. Specifically, we describe <i>KernelBoost</i> - a boosting algorithm which computes a kernel function as a combination of 'weak' space partitions. The kernel learning method naturally incorporates domain knowledge in the form of unlabeled data (i.e. in a semi-supervised or transductive settings), and also in the form of labeled samples from relevant related problems (i.e. in a learning-to-learn scenario). The latter goal is accomplished by learning a <i>single</i> kernel function for all classes. We show comparative evaluations of our method on datasets from the UCI repository. We demonstrate performance enhancement on two challenging tasks: digit classification with kernel SVM, and facial image retrieval based on image similarity as measured by the learnt kernel.</div></span> <a id="expcoll51" href="JavaScript: expandcollapse('expcoll51',51)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143896">Looping suffix tree-based inference of partially observable hidden state</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315489417">Michael P. Holmes</a>,
<a href="author_page.cfm?id=81100608549">Charles Lee Isbell, Jr</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 409-416</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143896" title="DOI">10.1145/1143844.1143896</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143896&ftid=364553&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow52" style="display:inline;"><br /><div style="display:inline">We present a solution for inferring hidden state from sensorimotor experience when the environment takes the form of a POMDP with deterministic transition and observation functions. Such environments can appear to be arbitrarily complex and non-deterministic ...</div></span>
<span id="toHide52" style="display:none;"><br /><div style="display:inline">We present a solution for inferring hidden state from sensorimotor experience when the environment takes the form of a POMDP with deterministic transition and observation functions. Such environments can appear to be arbitrarily complex and non-deterministic on the surface, but are actually deterministic with respect to the unobserved underlying state. We show that there always exists a finite history-based representation that fully captures the unobserved world state, allowing for perfect prediction of action effects. This representation takes the form of a looping prediction suffix tree (PST). We derive a sound and complete algorithm for learning a looping PST from a sufficient sample of sensorimotor experience. We also give empirical illustrations of the advantages conferred by this approach, and characterize the approximations to the looping PST that are made by existing algorithms such as Variable Length Markov Models, Utile Suffix Memory and Causal State Splitting Reconstruction.</div></span> <a id="expcoll52" href="JavaScript: expandcollapse('expcoll52',52)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143897">Batch mode active learning and its application to medical image classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100337419">Steven C. H. Hoi</a>,
<a href="author_page.cfm?id=81100054575">Rong Jin</a>,
<a href="author_page.cfm?id=81315492687">Jianke Zhu</a>,
<a href="author_page.cfm?id=81100033051">Michael R. Lyu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 417-424</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143897" title="DOI">10.1145/1143844.1143897</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143897&ftid=364554&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow53" style="display:inline;"><br /><div style="display:inline">The goal of active learning is to select the most informative examples for manual labeling. Most of the previous studies in active learning have focused on selecting a single unlabeled example in each iteration. This could be inefficient since ...</div></span>
<span id="toHide53" style="display:none;"><br /><div style="display:inline">The goal of active learning is to select the most informative examples for manual labeling. Most of the previous studies in active learning have focused on selecting a <i>single</i> unlabeled example in each iteration. This could be inefficient since the classification model has to be retrained for every labeled example. In this paper, we present a framework for "<b>batch mode active learning</b>" that applies the Fisher information matrix to select a number of informative examples simultaneously. The key computational challenge is how to efficiently identify the subset of unlabeled examples that can result in the largest reduction in the Fisher information. To resolve this challenge, we propose an efficient greedy algorithm that is based on the property of submodular functions. Our empirical studies with five UCI datasets and one real-world medical image classification show that the proposed batch mode active learning algorithm is more effective than the state-of-the-art algorithms for active learning.</div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143898">Ranking individuals by group comparisons</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81330492374">Tzu-Kuo Huang</a>,
<a href="author_page.cfm?id=81384612017">Chih-Jen Lin</a>,
<a href="author_page.cfm?id=81330500504">Ruby C. Weng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 425-432</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143898" title="DOI">10.1145/1143844.1143898</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143898&ftid=364555&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow54" style="display:inline;"><br /><div style="display:inline">This paper proposes new approaches to rank individuals from their group competition results. Many real-world problems are of this type. For example, ranking players from team games is important in some sports. We propose an exponential model to solve ...</div></span>
<span id="toHide54" style="display:none;"><br /><div style="display:inline">This paper proposes new approaches to rank individuals from their group competition results. Many real-world problems are of this type. For example, ranking players from team games is important in some sports. We propose an exponential model to solve such problems. To estimate individual rankings through the proposed model we introduce two convex minimization formulas with easy and efficient solution procedures. Experiments on real bridge records and multi-class classification demonstrate the viability of the proposed model.</div></span> <a id="expcoll54" href="JavaScript: expandcollapse('expcoll54',54)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143899">Hidden process models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81430672641">Rebecca A. Hutchinson</a>,
<a href="author_page.cfm?id=81314484253">Tom M. Mitchell</a>,
<a href="author_page.cfm?id=81315491323">Indrayana Rustandi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 433-440</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143899" title="DOI">10.1145/1143844.1143899</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143899&ftid=364556&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow55" style="display:inline;"><br /><div style="display:inline">We introduce Hidden Process Models (HPMs), a class of probabilistic models for multivariate time series data. The design of HPMs has been motivated by the challenges of modeling hidden cognitive processes in the brain, given functional Magnetic Resonance ...</div></span>
<span id="toHide55" style="display:none;"><br /><div style="display:inline">We introduce Hidden Process Models (HPMs), a class of probabilistic models for multivariate time series data. The design of HPMs has been motivated by the challenges of modeling hidden cognitive processes in the brain, given functional Magnetic Resonance Imaging (fMRI) data. fMRI data is sparse, high-dimensional, non-Markovian, and often involves prior knowledge of the form "hidden event A occurs <i>n</i> times within the interval [<i>t,t</i>&prime;]." HPMs provide a generalization of the widely used General Linear Model approaches to fMRI analysis, and HPMs can also be viewed as a subclass of Dynamic Bayes Networks.</div></span> <a id="expcoll55" href="JavaScript: expandcollapse('expcoll55',55)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143900">Estimating relatedness via data compression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81351593525">Brendan Juba</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 441-448</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143900" title="DOI">10.1145/1143844.1143900</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143900&ftid=364557&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow56" style="display:inline;"><br /><div style="display:inline">We show that it is possible to use data compression on independently obtained hypotheses from various tasks to algorithmically provide guarantees that the tasks are sufficiently related to benefit from multitask learning. We give uniform bounds in terms ...</div></span>
<span id="toHide56" style="display:none;"><br /><div style="display:inline">We show that it is possible to use data compression on independently obtained hypotheses from various tasks to algorithmically provide guarantees that the tasks are sufficiently related to benefit from multitask learning. We give uniform bounds in terms of the empirical average error for the true average error of the n hypotheses provided by deterministic learning algorithms drawing independent samples from a set of <i>n</i> unknown computable task distributions over finite sets.</div></span> <a id="expcoll56" href="JavaScript: expandcollapse('expcoll56',56)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143901">Automatic basis function construction for approximate dynamic programming and reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100011329">Philipp W. Keller</a>,
<a href="author_page.cfm?id=81100515533">Shie Mannor</a>,
<a href="author_page.cfm?id=81100275806">Doina Precup</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 449-456</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143901" title="DOI">10.1145/1143844.1143901</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143901&ftid=364558&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow57" style="display:inline;"><br /><div style="display:inline">We address the problem of automatically constructing basis functions for linear approximation of the value function of a Markov Decision Process (MDP). Our work builds on results by Bertsekas and Casta&ntilde;on (1989) who proposed a method for automatically ...</div></span>
<span id="toHide57" style="display:none;"><br /><div style="display:inline">We address the problem of automatically constructing basis functions for linear approximation of the value function of a Markov Decision Process (MDP). Our work builds on results by Bertsekas and Casta&ntilde;on (1989) who proposed a method for automatically aggregating states to speed up value iteration. We propose to use neighborhood component analysis (Goldberger et al., 2005), a dimensionality reduction technique created for supervised learning, in order to map a high-dimensional state space to a low-dimensional space, based on the Bellman error, or on the temporal difference (TD) error. We then place basis function in the lower-dimensional space. These are added as new features for the linear function approximator. This approach is applied to a high-dimensional inventory control problem.</div></span> <a id="expcoll57" href="JavaScript: expandcollapse('expcoll57',57)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143902">Personalized handwriting recognition via biased regularization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315489649">Wolf Kienzle</a>,
<a href="author_page.cfm?id=81100301174">Kumar Chellapilla</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 457-464</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143902" title="DOI">10.1145/1143844.1143902</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143902&ftid=364559&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow58" style="display:inline;"><br /><div style="display:inline">We present a new approach to personalized handwriting recognition. The problem, also known as writer adaptation, consists of converting a generic (user-independent) recognizer into a personalized (user-dependent) one, which has an improved recognition ...</div></span>
<span id="toHide58" style="display:none;"><br /><div style="display:inline">We present a new approach to personalized handwriting recognition. The problem, also known as writer adaptation, consists of converting a generic (user-independent) recognizer into a personalized (user-dependent) one, which has an improved recognition rate for a particular user. The adaptation step usually involves user-specific samples, which leads to the fundamental question of how to fuse this new information with that captured by the generic recognizer. We propose adapting the recognizer by minimizing a regularized risk functional (a modified SVM) where the prior knowledge from the generic recognizer enters through a modified regularization term. The result is a simple personalization framework with very good practical properties. Experiments on a 100 class real-world data set show that the number of errors can be reduced by over 40% with as few as five user samples per character.</div></span> <a id="expcoll58" href="JavaScript: expandcollapse('expcoll58',58)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143903">Optimal kernel selection in Kernel Fisher discriminant analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100663861">Seung-Jean Kim</a>,
<a href="author_page.cfm?id=81100037461">Alessandro Magnani</a>,
<a href="author_page.cfm?id=81406594004">Stephen Boyd</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 465-472</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143903" title="DOI">10.1145/1143844.1143903</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143903&ftid=364560&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow59" style="display:inline;"><br /><div style="display:inline">In Kernel Fisher discriminant analysis (KFDA), we carry out Fisher linear discriminant analysis in a high dimensional feature space defined implicitly by a kernel. The performance of KFDA depends on the choice of the kernel; in this paper, we consider ...</div></span>
<span id="toHide59" style="display:none;"><br /><div style="display:inline">In Kernel Fisher discriminant analysis (KFDA), we carry out Fisher linear discriminant analysis in a high dimensional feature space defined implicitly by a kernel. The performance of KFDA depends on the choice of the kernel; in this paper, we consider the problem of finding the optimal kernel, over a given convex set of kernels. We show that this optimal kernel selection problem can be reformulated as a tractable convex optimization problem which interior-point methods can solve globally and efficiently. The kernel selection method is demonstrated with some UCI machine learning benchmark examples.</div></span> <a id="expcoll59" href="JavaScript: expandcollapse('expcoll59',59)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143904">Pareto optimal linear classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100663861">Seung-Jean Kim</a>,
<a href="author_page.cfm?id=81100037461">Alessandro Magnani</a>,
<a href="author_page.cfm?id=81315491344">Sikandar Samar</a>,
<a href="author_page.cfm?id=81406594004">Stephen Boyd</a>,
<a href="author_page.cfm?id=81416603183">Johan Lim</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 473-480</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143904" title="DOI">10.1145/1143844.1143904</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143904&ftid=364561&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow60" style="display:inline;"><br /><div style="display:inline">We consider the problem of choosing a linear classifier that minimizes misclassification probabilities in two-class classification, which is a bi-criterion problem, involving a trade-off between two objectives. We assume that the class-conditional distributions ...</div></span>
<span id="toHide60" style="display:none;"><br /><div style="display:inline">We consider the problem of choosing a linear classifier that minimizes misclassification probabilities in two-class classification, which is a bi-criterion problem, involving a trade-off between two objectives. We assume that the class-conditional distributions are Gaussian. This assumption makes it computationally tractable to find Pareto optimal linear classifiers whose classification capabilities are inferior to no other linear ones. The main purpose of this paper is to establish several robustness properties of those classifiers with respect to variations and uncertainties in the distributions. We also extend the results to kernel-based classification. Finally, we show how to carry out trade-off analysis empirically with a finite number of given labeled data.</div></span> <a id="expcoll60" href="JavaScript: expandcollapse('expcoll60',60)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143905">Fast particle smoothing: if I had a million particles</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315489844">Mike Klaas</a>,
<a href="author_page.cfm?id=81323488063">Mark Briers</a>,
<a href="author_page.cfm?id=81335489736">Nando de Freitas</a>,
<a href="author_page.cfm?id=81100562973">Arnaud Doucet</a>,
<a href="author_page.cfm?id=81331499208">Simon Maskell</a>,
<a href="author_page.cfm?id=81331497590">Dustin Lang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 481-488</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143905" title="DOI">10.1145/1143844.1143905</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143905&ftid=364562&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow61" style="display:inline;"><br /><div style="display:inline">We propose efficient particle smoothing methods for generalized state-spaces models. Particle smoothing is an expensive O(N2) algorithm, where N is the number of particles. We overcome this problem by integrating dual tree recursions ...</div></span>
<span id="toHide61" style="display:none;"><br /><div style="display:inline">We propose efficient particle smoothing methods for generalized state-spaces models. Particle smoothing is an expensive <i>O(N</i><sup>2</sup>) algorithm, where <i>N</i> is the number of particles. We overcome this problem by integrating dual tree recursions and fast multipole techniques with forward-backward smoothers, a new generalized two-filter smoother and a maximum <i>a posteriori</i> (MAP) smoother. Our experiments show that these improvements can substantially increase the practicality of particle smoothing.</div></span> <a id="expcoll61" href="JavaScript: expandcollapse('expcoll61',61)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143906">Autonomous shaping: knowledge transfer in reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315489714">George Konidaris</a>,
<a href="author_page.cfm?id=81100207208">Andrew Barto</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 489-496</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143906" title="DOI">10.1145/1143844.1143906</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143906&ftid=364563&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow62" style="display:inline;"><br /><div style="display:inline">We introduce the use of learned shaping rewards in reinforcement learning tasks, where an agent uses prior experience on a sequence of tasks to learn a portable predictor that estimates intermediate rewards, resulting in accelerated learning in later ...</div></span>
<span id="toHide62" style="display:none;"><br /><div style="display:inline">We introduce the use of learned shaping rewards in reinforcement learning tasks, where an agent uses prior experience on a sequence of tasks to learn a portable predictor that estimates intermediate rewards, resulting in accelerated learning in later tasks that are related but distinct. Such agents can be trained on a sequence of relatively easy tasks in order to develop a more informative measure of reward that can be transferred to improve performance on more difficult tasks without requiring a hand coded shaping function. We use a rod positioning task to show that this significantly improves performance even after a very brief training period.</div></span> <a id="expcoll62" href="JavaScript: expandcollapse('expcoll62',62)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143907">Data association for topic intensity tracking</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100460663">Andreas Krause</a>,
<a href="author_page.cfm?id=81100096826">Jure Leskovec</a>,
<a href="author_page.cfm?id=81100629945">Carlos Guestrin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 497-504</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143907" title="DOI">10.1145/1143844.1143907</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143907&ftid=364564&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow63" style="display:inline;"><br /><div style="display:inline">We present a unified model of what was traditionally viewed as two separate tasks: data association and intensity tracking of multiple topics over time. In the data association part, the task is to assign a topic (a class) to each data point, and the ...</div></span>
<span id="toHide63" style="display:none;"><br /><div style="display:inline">We present a unified model of what was traditionally viewed as two separate tasks: data association and intensity tracking of multiple topics over time. In the data association part, the task is to assign a topic (a class) to each data point, and the intensity tracking part models the bursts and changes in intensities of topics over time. Our approach to this problem combines an extension of Factorial Hidden Markov models for topic intensity tracking with exponential order statistics for implicit data association. Experiments on text and email datasets show that the interplay of classification and topic intensity tracking improves the accuracy of both classification and intensity tracking. Even a little noise in topic assignments can mislead the traditional algorithms. However, our approach detects correct topic intensities even with 30% topic noise.</div></span> <a id="expcoll63" href="JavaScript: expandcollapse('expcoll63',63)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143908">Learning low-rank kernel matrices</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100083449">Brian Kulis</a>,
<a href="author_page.cfm?id=81100034876">M&#225;ty&#225;s Sustik</a>,
<a href="author_page.cfm?id=81100098715">Inderjit Dhillon</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 505-512</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143908" title="DOI">10.1145/1143844.1143908</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143908&ftid=364565&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow64" style="display:inline;"><br /><div style="display:inline">Kernel learning plays an important role in many machine learning tasks. However, algorithms for learning a kernel matrix often scale poorly, with running times that are cubic in the number of data points. In this paper, we propose efficient algorithms ...</div></span>
<span id="toHide64" style="display:none;"><br /><div style="display:inline">Kernel learning plays an important role in many machine learning tasks. However, algorithms for learning a kernel matrix often scale poorly, with running times that are cubic in the number of data points. In this paper, we propose efficient algorithms for learning low-rank kernel matrices; our algorithms scale linearly in the number of data points and quadratically in the rank of the kernel. We introduce and employ Bregman matrix divergences for rank-deficient matrices---these divergences are natural for our problem since they preserve the rank as well as positive semi-definiteness of the kernel matrix. Special cases of our framework yield faster algorithms for various existing kernel learning problems. Experimental results demonstrate the effectiveness of our algorithms in learning both low-rank and full-rank kernels.</div></span> <a id="expcoll64" href="JavaScript: expandcollapse('expcoll64',64)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143909">Local distance preservation in the GP-LVM through back constraints</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100574233">Neil D. Lawrence</a>,
<a href="author_page.cfm?id=81321496751">Joaquin Qui&#241;onero-Candela</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 513-520</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143909" title="DOI">10.1145/1143844.1143909</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143909&ftid=364566&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow65" style="display:inline;"><br /><div style="display:inline">The Gaussian process latent variable model (GP-LVM) is a generative approach to nonlinear low dimensional embedding, that provides a smooth probabilistic mapping from latent to data space. It is also a non-linear generalization of probabilistic PCA (PPCA) ...</div></span>
<span id="toHide65" style="display:none;"><br /><div style="display:inline">The Gaussian process latent variable model (GP-LVM) is a generative approach to nonlinear low dimensional embedding, that provides a smooth probabilistic mapping from latent to data space. It is also a non-linear generalization of probabilistic PCA (PPCA) (Tipping & Bishop, 1999). While most approaches to non-linear dimensionality methods focus on preserving local distances in data space, the GP-LVM focusses on exactly the opposite. Being a smooth mapping from latent to data space, it focusses on keeping things apart in latent space that are far apart in data space. In this paper we first provide an overview of dimensionality reduction techniques, placing the emphasis on the kind of distance relation preserved. We then show how the GP-LVM can be generalized, through back constraints, to additionally preserve local distances. We give illustrative experiments on common data sets.</div></span> <a id="expcoll65" href="JavaScript: expandcollapse('expcoll65',65)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143910">Simpler knowledge-based support vector machines</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81339511241">Quoc V. Le</a>,
<a href="author_page.cfm?id=81100243402">Alex J. Smola</a>,
<a href="author_page.cfm?id=81100186149">Thomas G&#228;rtner</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 521-528</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143910" title="DOI">10.1145/1143844.1143910</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143910&ftid=364567&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow66" style="display:inline;"><br /><div style="display:inline">If appropriately used, prior knowledge can significantly improve the predictive accuracy of learning algorithms or reduce the amount of training data needed. In this paper we introduce a simple method to incorporate prior knowledge in support vector ...</div></span>
<span id="toHide66" style="display:none;"><br /><div style="display:inline">If appropriately used, prior knowledge can significantly improve the predictive accuracy of learning algorithms or reduce the amount of training data needed. In this paper we introduce a simple method to incorporate prior knowledge in support vector machines by modifying the hypothesis space rather than the optimization problem. The optimization problem is amenable to solution by the constrained concave convex procedure, which finds a local optimum. The paper discusses different kinds of prior knowledge and demonstrates the applicability of the approach in some characteristic experiments.</div></span> <a id="expcoll66" href="JavaScript: expandcollapse('expcoll66',66)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143911">Using query-specific variance estimates to combine Bayesian classifiers</a></span></td>
</tr>
<tr>
<td> </td>
 <td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81375607385">Chi-Hoon Lee</a>,
<a href="author_page.cfm?id=81100082147">Russ Greiner</a>,
<a href="author_page.cfm?id=81451597400">Shaojun Wang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 529-536</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143911" title="DOI">10.1145/1143844.1143911</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143911&ftid=364568&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow67" style="display:inline;"><br /><div style="display:inline">Many of today's best classification results are obtained by combining the responses of a set of base classifiers to produce an answer for the query. This paper explores a novel "query specific" combination rule: After learning a set of simple belief ...</div></span>
<span id="toHide67" style="display:none;"><br /><div style="display:inline">Many of today's best classification results are obtained by combining the responses of a set of base classifiers to produce an answer for the query. This paper explores a novel "query specific" combination rule: After learning a set of simple belief network classifiers, we produce an answer to each query by combining their individual responses, using weights based inversely on their respective <i>variances</i> around their responses. These variances are based on the uncertainty of the network parameters, which in turn depend on the training datasample. In essence, this variance quantifies the base classifier's confidence of its response to this query. Our experimental results show that these "mixture-using-variance belief net classifiers" MUVS work effectively, especially when the base classifiers are learned using balanced bootstrap samples and when their results are combined using James-Stein shrinkage. We also found that our variance-based combination rule performed better than both bagging and AdaBoost, even on the set of base classifiers produced by AdaBoost itself. Finally, this framework is extremely efficient, as both the learning and the classification components require only straight-line code.</div></span> <a id="expcoll67" href="JavaScript: expandcollapse('expcoll67',67)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143912">A probabilistic model for text kernels</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315490073">Alain Lehmann</a>,
<a href="author_page.cfm?id=81310500608">John Shawe-Taylor</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 537-544</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143912" title="DOI">10.1145/1143844.1143912</a></span></td>
</tr>
<tr> 
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143912&ftid=364569&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow68" style="display:inline;"><br /><div style="display:inline">This paper explores several kernels in the context of text classification. A novel view of how documents might have been created is introduced and kernels are derived from this framework. The relations between these kernels as well as to the Gaussian ...</div></span>
<span id="toHide68" style="display:none;"><br /><div style="display:inline">This paper explores several kernels in the context of text classification. A novel view of how documents might have been created is introduced and kernels are derived from this framework. The relations between these kernels as well as to the Gaussian kernel are discussed. Moreover, the popular <i>tf-idf</i> weighting scheme will be derived as a natural consequence. Finally, the kernels have been evaluated on the <i>Reuters Corpus Volume I</i> newswire database to assess their quality in a topic classification application.</div></span> <a id="expcoll68" href="JavaScript: expandcollapse('expcoll68',68)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143913">Efficient MAP approximation for dense energy functions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100065204">Marius Leordeanu</a>,
<a href="author_page.cfm?id=81100021352">Martial Hebert</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 545-552</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143913" title="DOI">10.1145/1143844.1143913</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143913&ftid=364570&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow69" style="display:inline;"><br /><div style="display:inline">We present an efficient method for maximizing energy functions with first and second order potentials, suitable for MAP labeling estimation problems that arise in undirected graphical models. Our approach is to relax the integer constraints on the solution ...</div></span>
<span id="toHide69" style="display:none;"><br /><div style="display:inline">We present an efficient method for maximizing energy functions with first and second order potentials, suitable for MAP labeling estimation problems that arise in undirected graphical models. Our approach is to relax the integer constraints on the solution in two steps. First we efficiently obtain the relaxed global optimum following a procedure similar to the iterative power method for finding the largest eigenvector of a matrix. Next, we map the relaxed optimum on a simplex and show that the new energy obtained has a certain optimal bound. Starting from this energy we follow an efficient coordinate ascent procedure that is guaranteed to increase the energy at every step and converge to a solution that obeys the initial integral constraints. We also present a sufficient condition for ascent procedures that guarantees the increase in energy at every step.</div></span> <a id="expcoll69" href="JavaScript: expandcollapse('expcoll69',69)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143914">Nonstationary kernel combination</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100350276">Darrin P. Lewis</a>,
<a href="author_page.cfm?id=81100510477">Tony Jebara</a>,
<a href="author_page.cfm?id=81100587603">William Stafford Noble</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 553-560</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143914" title="DOI">10.1145/1143844.1143914</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143914&ftid=364571&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow70" style="display:inline;"><br /><div style="display:inline">The power and popularity of kernel methods stem in part from their ability to handle diverse forms of structured inputs, including vectors, graphs and strings. Recently, several methods have been proposed for combining kernels from heterogeneous data ...</div></span>
<span id="toHide70" style="display:none;"><br /><div style="display:inline">The power and popularity of kernel methods stem in part from their ability to handle diverse forms of structured inputs, including vectors, graphs and strings. Recently, several methods have been proposed for combining kernels from heterogeneous data sources. However, all of these methods produce stationary combinations; i.e., the relative weights of the various kernels do not vary among input examples. This article proposes a method for combining multiple kernels in a nonstationary fashion. The approach uses a large-margin latent-variable generative model within the maximum entropy discrimination (MED) framework. Latent parameter estimation is rendered tractable by variational bounds and an iterative optimization procedure. The classifier we use is a log-ratio of Gaussian mixtures, in which each component is implicitly mapped via a Mercer kernel function. We show that the support vector machine is a special case of this model. In this approach, discriminative parameter estimation is feasible via a fast sequential minimal optimization algorithm. Empirical results are presented on synthetic data, several benchmarks, and on a protein function annotation task.</div></span> <a id="expcoll70" href="JavaScript: expandcollapse('expcoll70',70)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143915">Region-based value iteration for partially observable Markov decision processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315489927">Hui Li</a>,
<a href="author_page.cfm?id=81309499575">Xuejun Liao</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 561-568</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143915" title="DOI">10.1145/1143844.1143915</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143915&ftid=364572&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow71" style="display:inline;"><br /><div style="display:inline">An approximate region-based value iteration (RBVI) algorithm is proposed to find the optimal policy for a partially observable Markov decision process (POMDP). The proposed RBVI approximates the true polyhedral partition of the belief simplex with an ...</div></span>
<span id="toHide71" style="display:none;"><br /><div style="display:inline">An approximate region-based value iteration (RBVI) algorithm is proposed to find the optimal policy for a partially observable Markov decision process (POMDP). The proposed RBVI approximates the true polyhedral partition of the belief simplex with an ellipsoidal partition, such that the optimal value function is linear in each of the ellipsoidal regions. The position and shape of each region, as well as the gradient (alpha-vector) of the optimal value function in the region, are parameterized explicitly, and are estimated via efficient expectation maximization (EM) and variational Bayesian EM (VBEM), based on a set of selected sample belief points. The RBVI maintains a much smaller number of alpha-vectors than point-based methods and yields a more parsimonious representation that approximates the true value function in the maximum likelihood (ML) sense. The results on benchmark problems show that the proposed RBVI is comparable in performance to state-of-the-art algorithms, despite of the small number of alpha-vectors that are used.</div></span> <a id="expcoll71" href="JavaScript: expandcollapse('expcoll71',71)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143916">Multiclass boosting with repartitioning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100476091">Ling Li</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 569-576</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143916" title="DOI">10.1145/1143844.1143916</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143916&ftid=364573&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow72" style="display:inline;"><br /><div style="display:inline">A multiclass classification problem can be reduced to a collection of binary problems with the aid of a coding matrix. The quality of the final solution, which is an ensemble of base classifiers learned on the binary problems, is affected by both the ...</div></span>
<span id="toHide72" style="display:none;"><br /><div style="display:inline">A multiclass classification problem can be reduced to a collection of binary problems with the aid of a coding matrix. The quality of the final solution, which is an ensemble of base classifiers learned on the binary problems, is affected by both the performance of the base learner and the error-correcting ability of the coding matrix. A coding matrix with strong error-correcting ability may not be overall optimal if the binary problems are too hard for the base learner. Thus a trade-off between error-correcting and base learning should be sought. In this paper, we propose a new multiclass boosting algorithm that modifies the coding matrix according to the learning ability of the base learner. We show experimentally that our algorithm is very efficient in optimizing the multiclass margin cost, and outperforms existing multiclass algorithms such as AdaBoost.ECC and one-vs-one. The improvement is especially significant when the base learner is not very powerful.</div></span> <a id="expcoll72" href="JavaScript: expandcollapse('expcoll72',72)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143917">Pachinko allocation: DAG-structured mixture models of topic correlations</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81537247056">Wei Li</a>,
<a href="author_page.cfm?id=81100553872">Andrew McCallum</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 577-584</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143917" title="DOI">10.1145/1143844.1143917</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143917&ftid=364574&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow73" style="display:inline;"><br /><div style="display:inline">Latent Dirichlet allocation (LDA) and other related topic models are increasingly popular tools for summarization and manifold discovery in discrete data. However, LDA does not capture correlations between topics. In this paper, we introduce the pachinko ...</div></span>
<span id="toHide73" style="display:none;"><br /><div style="display:inline">Latent Dirichlet allocation (LDA) and other related topic models are increasingly popular tools for summarization and manifold discovery in discrete data. However, LDA does not capture correlations between topics. In this paper, we introduce the <i>pachinko allocation</i> model (PAM), which captures arbitrary, nested, and possibly sparse correlations between topics using a directed acyclic graph (DAG). The leaves of the DAG represent individual words in the vocabulary, while each interior node represents a correlation among its children, which may be words or other interior nodes (topics). PAM provides a flexible alternative to recent work by Blei and Lafferty (2006), which captures correlations only between <i>pairs</i> of topics. Using text data from newsgroups, historic NIPS proceedings and other research paper corpora, we show improved performance of PAM in document classification, likelihood of held-out data, the ability to support finer-grained topics, and topical keyword coherence.</div></span> <a id="expcoll73" href="JavaScript: expandcollapse('expcoll73',73)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143918">Spectral clustering for multi-type relational data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81375607652">Bo Long</a>,
<a href="author_page.cfm?id=81451593853">Zhongfei (Mark) Zhang</a>,
<a href="author_page.cfm?id=81452596593">Xiaoyun W&#250;</a>,
<a href="author_page.cfm?id=81350576309">Philip S. Yu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 585-592</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143918" title="DOI">10.1145/1143844.1143918</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143918&ftid=364575&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow74" style="display:inline;"><br /><div style="display:inline">Clustering on multi-type relational data has attracted more and more attention in recent years due to its high impact on various important applications, such as Web mining, e-commerce and bioinformatics. However, the research on general multi-type relational ...</div></span>
<span id="toHide74" style="display:none;"><br /><div style="display:inline">Clustering on multi-type relational data has attracted more and more attention in recent years due to its high impact on various important applications, such as Web mining, e-commerce and bioinformatics. However, the research on general multi-type relational data clustering is still limited and preliminary. The contribution of the paper is three-fold. First, we propose a general model, the collective factorization on related matrices, for multi-type relational data clustering. The model is applicable to relational data with various structures. Second, under this model, we derive a novel algorithm, the spectral relational clustering, to cluster multi-type interrelated data objects simultaneously. The algorithm iteratively embeds each type of data objects into low dimensional spaces and benefits from the interactions among the hidden structures of different types of data objects. Extensive experiments demonstrate the promise and effectiveness of the proposed algorithm. Third, we show that the existing spectral clustering algorithms can be considered as the special cases of the proposed model and algorithm. This demonstrates the good theoretic generality of the proposed model and algorithm.</div></span> <a id="expcoll74" href="JavaScript: expandcollapse('expcoll74',74)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143919">Combined central and subspace clustering for computer vision applications</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81453643252">Le Lu</a>,
<a href="author_page.cfm?id=81100420262">Ren&#233; Vidal</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 593-600</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143919" title="DOI">10.1145/1143844.1143919</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143919&ftid=364576&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow75" style="display:inline;"><br /><div style="display:inline">Central and subspace clustering methods are at the core of many segmentation problems in computer vision. However, both methods fail to give the correct segmentation in many practical scenarios, e.g., when data points are close to the intersection of ...</div></span>
<span id="toHide75" style="display:none;"><br /><div style="display:inline">Central and subspace clustering methods are at the core of many segmentation problems in computer vision. However, both methods fail to give the correct segmentation in many practical scenarios, e.g., when data points are close to the intersection of two subspaces or when two cluster centers in different subspaces are spatially close. In this paper, we address these challenges by considering the problem of clustering a set of points lying in a union of subspaces and distributed around multiple cluster centers inside each subspace. We propose a generalization of Kmeans and Ksubspaces that clusters the data by minimizing a cost function that combines both central and subspace distances. Experiments on synthetic data compare our algorithm favorably against four other clustering methods. We also test our algorithm on computer vision problems such as face clustering with varying illumination and video shot segmentation of dynamic scenes.</div></span> <a id="expcoll75" href="JavaScript: expandcollapse('expcoll75',75)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143920">Fast direct policy evaluation using multiscale analysis of Markov diffusion processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317488955">Mauro Maggioni</a>,
<a href="author_page.cfm?id=81100132345">Sridhar Mahadevan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 601-608</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143920" title="DOI">10.1145/1143844.1143920</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143920&ftid=364577&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow76" style="display:inline;"><br /><div style="display:inline">Policy evaluation is a critical step in the approximate solution of large Markov decision processes (MDPs), typically requiring O(|S|3) to directly solve the Bellman system of |S| linear equations (where |S| is ...</div></span>
<span id="toHide76" style="display:none;"><br /><div style="display:inline">Policy evaluation is a critical step in the approximate solution of large Markov decision processes (MDPs), typically requiring <i>O</i>(|<i>S</i>|<sup>3</sup>) to directly solve the Bellman system of |<i>S</i>| linear equations (where |<i>S</i>| is the state space size in the discrete case, and the sample size in the continuous case). In this paper we apply a recently introduced multiscale framework for analysis on graphs to design a faster algorithm for policy evaluation. For a fixed policy &pi;, this framework efficiently constructs a multiscale decomposition of the random walk <i>P</i>&pi; associated with the policy &pi;. This enables efficiently computing medium and long term state distributions, approximation of value functions, and the <i>direct</i> computation of the potential operator (<i>I</i> - &gamma;<i>P</i><sup>&pi;</sup>)<sup>-1</sup> needed to solve Bellman's equation. We show that even a preliminary non-optimized version of the solver competes with highly optimized iterative techniques, requiring in many cases a complexity of <i>O</i>(|<i>S</i>|).</div></span> <a id="expcoll76" href="JavaScript: expandcollapse('expcoll76',76)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143921">Pruning in ordered bagging ensembles</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315490767">Gonzalo Mart&#237;nez-Mu&#241;oz</a>,
<a href="author_page.cfm?id=81100422508">Alberto Su&#225;rez</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 609-616</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143921" title="DOI">10.1145/1143844.1143921</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143921&ftid=364578&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow77" style="display:inline;"><br /><div style="display:inline">We present a novel ensemble pruning method based on reordering the classifiers obtained from bagging and then selecting a subset for aggregation. Ordering the classifiers generated in bagging makes it possible to build subensembles of increasing size ...</div></span>
<span id="toHide77" style="display:none;"><br /><div style="display:inline">We present a novel ensemble pruning method based on reordering the classifiers obtained from bagging and then selecting a subset for aggregation. Ordering the classifiers generated in bagging makes it possible to build subensembles of increasing size by including first those classifiers that are expected to perform best when aggregated. Ensemble pruning is achieved by halting the aggregation process before all the classifiers generated are included into the ensemble. Pruned subensembles containing between 15% and 30% of the initial pool of classifiers, besides being smaller, improve the generalization performance of the full bagging ensemble in the classification problems investigated.</div></span> <a id="expcoll77" href="JavaScript: expandcollapse('expcoll77',77)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143922">Learning high-order MRF priors of color images</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81388594589">Julian J. McAuley</a>,
<a href="author_page.cfm?id=81100060357">Tib&#233;rio S. Caetano</a>,
<a href="author_page.cfm?id=81100243402">Alex J. Smola</a>,
<a href="author_page.cfm?id=81100089584">Matthias O. Franz</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 617-624</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143922" title="DOI">10.1145/1143844.1143922</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143922&ftid=364579&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow78" style="display:inline;"><br /><div style="display:inline">In this paper, we use large neighborhood Markov random fields to learn rich prior models of color images. Our approach extends the monochromatic Fields of Experts model (Roth & Black, 2005a) to color images. In the Fields of Experts model, ...</div></span>
<span id="toHide78" style="display:none;"><br /><div style="display:inline">In this paper, we use large neighborhood Markov random fields to learn rich prior models of color images. Our approach extends the monochromatic <i>Fields of Experts</i> model (Roth & Black, 2005a) to color images. In the <i>Fields of Experts</i> model, the curse of dimensionality due to very large clique sizes is circumvented by parameterizing the potential functions according to a product of experts. We introduce simplifications to the original approach by Roth and Black which allow us to cope with the increased clique size (typically 3x3x3 or 5x5x3 pixels) of color images. Experimental results are presented for image denoising which evidence improvements over state-of-the-art monochromatic image priors.</div></span> <a id="expcoll78" href="JavaScript: expandcollapse('expcoll78',78)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143923">The uniqueness of a good optimum for K-means</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100082085">Marina Meil&#259;</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 625-632</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143923" title="DOI">10.1145/1143844.1143923</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143923&ftid=364580&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow79" style="display:inline;"><br /><div style="display:inline">If we have found a "good" clustering C of a data set, can we prove that C is not far from the (unknown) best clustering Copt of these data? Perhaps surprisingly, the answer to this question is sometimes yes. When ...</div></span>
<span id="toHide79" style="display:none;"><br /><div style="display:inline">If we have found a "good" clustering <i>C</i> of a data set, can we prove that <i>C</i> is not far from the (unknown) best clustering <i>C</i><sup><i>opt</i></sup> of these data? Perhaps surprisingly, the answer to this question is sometimes yes. When "goodness" is measured by the distortion of K-means clustering, this paper proves spectral bounds on the distance <i>d</i>(<i>C, C</i><sup><i>opt</i></sup>). The bounds exist in the case when the data admits a low distortion clustering.</div></span> <a id="expcoll79" href="JavaScript: expandcollapse('expcoll79',79)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143924">Kernel information embeddings</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81310503065">Roland Memisevic</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 633-640</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143924" title="DOI">10.1145/1143844.1143924</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143924&ftid=364581&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow80" style="display:inline;"><br /><div style="display:inline">We describe a family of embedding algorithms that are based on nonparametric estimates of mutual information (MI). Using Parzen window estimates of the distribution in the joint (input, embedding)-space, we derive a MI-based objective function for dimensionality ...</div></span>
<span id="toHide80" style="display:none;"><br /><div style="display:inline">We describe a family of embedding algorithms that are based on nonparametric estimates of mutual information (MI). Using Parzen window estimates of the distribution in the joint (input, embedding)-space, we derive a MI-based objective function for dimensionality reduction that can be optimized directly with respect to a set of latent data representatives. Various types of supervision signal can be introduced within the framework by replacing plain MI with several forms of conditional MI. Examples of the semi-(un)supervised algorithms that we obtain this way are a new model for manifold alignment, and a new type of embedding method that performs 'conditional dimensionality reduction'.</div></span> <a id="expcoll80" href="JavaScript: expandcollapse('expcoll80',80)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143925">Generalized spectral bounds for sparse LDA</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100370957">Baback Moghaddam</a>,
<a href="author_page.cfm?id=81100499890">Yair Weiss</a>,
<a href="author_page.cfm?id=81100356625">Shai Avidan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 641-648</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143925" title="DOI">10.1145/1143844.1143925</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143925&ftid=364582&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow81" style="display:inline;"><br /><div style="display:inline">We present a discrete spectral framework for the sparse or cardinality-constrained solution of a generalized Rayleigh quotient. This NP-hard combinatorial optimization problem is central to supervised learning tasks such as sparse LDA, feature ...</div></span>
<span id="toHide81" style="display:none;"><br /><div style="display:inline">We present a discrete spectral framework for the sparse or <i>cardinality-constrained</i> solution of a generalized Rayleigh quotient. This NP-hard combinatorial optimization problem is central to supervised learning tasks such as sparse LDA, feature selection and relevance ranking for classification. We derive a new generalized form of the <i>Inclusion Principle</i> for variational eigenvalue bounds, leading to exact and <i>optimal</i> sparse linear discriminants using branch-and-bound search. An efficient greedy (approximate) technique is also presented. The generalization performance of our sparse LDA algorithms is demonstrated with real-world UCI ML benchmarks and compared to a leading SVM-based gene selection algorithm for cancer classification.</div></span> <a id="expcoll81" href="JavaScript: expandcollapse('expcoll81',81)">expand</a>
</div>
 </td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143926">Learning to impersonate</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100319011">Moni Naor</a>,
<a href="author_page.cfm?id=81300112501">Guy N. Rothblum</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 649-656</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143926" title="DOI">10.1145/1143844.1143926</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143926&ftid=364583&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow82" style="display:inline;"><br /><div style="display:inline">Consider Alice and Bob, who have some shared secret which helps Alice to identify Bob-impersonators, and Eve, who does not know their secret. Eve wants to impersonate Bob and "fool" Alice. If Eve is computationally unbounded, how long does she need to ...</div></span>
<span id="toHide82" style="display:none;"><br /><div style="display:inline">Consider Alice and Bob, who have some shared secret which helps Alice to identify Bob-impersonators, and Eve, who does not know their secret. Eve wants to impersonate Bob and "fool" Alice. If Eve is computationally unbounded, how long does she need to observe Bob before she can impersonate him? What is a good strategy for Eve? If (cryptographic) one-way functions exist, an efficient Eve cannot impersonate even very simple Bobs, but if they do not exist, can Eve learn to impersonate any efficient Bob?We formalize these questions in a new computational learning model, which we believe captures a wide variety of natural learning tasks, and tightly bound the number of observations Eve makes in terms of the secret's entropy. We then show that if one-way functions do not exist, then an efficient Eve can learn to impersonate any efficient Bob nearly as well as an unbounded Eve.For the full version of this work see (Naor & Rothblum, 2006).</div></span> <a id="expcoll82" href="JavaScript: expandcollapse('expcoll82',82)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143927">Online decoding of Markov models under latency constraints</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100599579">Mukund Narasimhan</a>,
<a href="author_page.cfm?id=81100432611">Paul Viola</a>,
<a href="author_page.cfm?id=81100465115">Michael Shilman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 657-664</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143927" title="DOI">10.1145/1143844.1143927</a></span></td>
</tr>
 <tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143927&ftid=364584&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow83" style="display:inline;"><br /><div style="display:inline">The Viterbi algorithm is an efficient and optimal method for decoding linear-chain Markov Models. However, the entire input sequence must be observed before the labels for any time step can be generated, and therefore Viterbi cannot be directly applied ...</div></span>
<span id="toHide83" style="display:none;"><br /><div style="display:inline">The Viterbi algorithm is an efficient and optimal method for decoding linear-chain Markov Models. However, the entire input sequence must be observed before the labels for any time step can be generated, and therefore Viterbi cannot be directly applied to online/interactive/streaming scenarios without incurring significant (possibly unbounded) latency. A widely used approach is to break the input stream into fixed-size windows, and apply Viterbi to each window. Larger windows lead to higher accuracy, but result in higher latency.We propose several alternative algorithms to the fixed-sized window decoding approach. These approaches compute a certainty measure on predicted labels that allows us to trade off latency for expected accuracy dynamically, without having to choose a fixed window size up front. Not surprisingly, this more principled approach gives us a substantial improvement over choosing a fixed window. We show the effectiveness of the approach for the task of spotting semi-structured information in large documents. When compared to full Viterbi, the approach suffers a 0.1 percent error degradation with a average latency of 2.6 time steps (versus the potentially infinite latency of Viterbi). When compared to fixed windows Viterbi, we achieve a 40x reduction in error and 6x reduction in latency.</div></span> <a id="expcoll83" href="JavaScript: expandcollapse('expcoll83',83)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143928">Learning hierarchical task networks by observation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100209863">Negin Nejati</a>,
<a href="author_page.cfm?id=81100101009">Pat Langley</a>,
<a href="author_page.cfm?id=81331497180">Tolga Konik</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 665-672</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143928" title="DOI">10.1145/1143844.1143928</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143928&ftid=364585&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
 <span id="toShow84" style="display:inline;"><br /><div style="display:inline">Knowledge-based planning methods offer benefits over classical techniques, but they are time consuming and costly to construct. There has been research on learning plan knowledge from search, but this can take substantial computer time and may even fail ...</div></span>
<span id="toHide84" style="display:none;"><br /><div style="display:inline">Knowledge-based planning methods offer benefits over classical techniques, but they are time consuming and costly to construct. There has been research on learning plan knowledge from search, but this can take substantial computer time and may even fail to find solutions on complex tasks. Here we describe another approach that observes sequences of operators taken from expert solutions to problems and learns hierarchical task networks from them. The method has similarities to previous algorithms for explanation-based learning, but differs in its ability to acquire hierarchical structures and in the generality of learned conditions. These increase the method's capability to transfer learned knowledge to other problems and supports the acquisition of recursive procedures. After presenting the learning algorithm, we report experiments that compare its abilities to other techniques on two planning domains. In closing, we review related work and directions for future research.</div></span> <a id="expcoll84" href="JavaScript: expandcollapse('expcoll84',84)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143929">Reinforcement learning for optimized trade execution</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81300271401">Yuriy Nevmyvaka</a>,
<a href="author_page.cfm?id=81451595896">Yi Feng</a>,
<a href="author_page.cfm?id=81406594573">Michael Kearns</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 673-680</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143929" title="DOI">10.1145/1143844.1143929</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143929&ftid=364586&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow85" style="display:inline;"><br /><div style="display:inline">We present the first large-scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets. Our experiments are based on 1.5 years of millisecond time-scale limit order data from ...</div></span>
<span id="toHide85" style="display:none;"><br /><div style="display:inline">We present the first large-scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets. Our experiments are based on 1.5 years of millisecond time-scale limit order data from NASDAQ, and demonstrate the promise of reinforcement learning methods to market microstructure problems. Our learning algorithm introduces and exploits a natural "low-impact" factorization of the state space.</div></span> <a id="expcoll85" href="JavaScript: expandcollapse('expcoll85',85)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143930">Concept boundary detection for speeding up SVMs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100464442">Navneet Panda</a>,
<a href="author_page.cfm?id=81344489120">Edward Y. Chang</a>,
<a href="author_page.cfm?id=81100657852">Gang Wu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 681-688</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143930" title="DOI">10.1145/1143844.1143930</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143930&ftid=364587&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow86" style="display:inline;"><br /><div style="display:inline">Support Vector Machines (SVMs) suffer from an O(n2) training cost, where n denotes the number of training instances. In this paper, we propose an algorithm to select boundary instances as training data to substantially ...</div></span>
<span id="toHide86" style="display:none;"><br /><div style="display:inline">Support Vector Machines (SVMs) suffer from an <i>O</i>(<i>n</i><sup><i>2</i></sup>) training cost, where <i>n</i> denotes the number of training instances. In this paper, we propose an algorithm to select boundary instances as training data to substantially reduce <i>n</i>. Our proposed algorithm is motivated by the result of (Burges, 1999) that, removing non-support vectors from the training set does not change SVM training results. Our algorithm eliminates instances that are likely to be non-support vectors. In the concept-independent preprocessing step of our algorithm, we prepare nearest-neighbor lists for training instances. In the concept-specific sampling step, we can then effectively select useful training data for each target concept. Empirical studies show our algorithm to be effective in reducing <i>n</i>, outperforming other competing downsampling algorithms without significantly compromising testing accuracy.</div></span> <a id="expcoll86" href="JavaScript: expandcollapse('expcoll86',86)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143931">The support vector decomposition machine</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452615698">Francisco Pereira</a>,
<a href="author_page.cfm?id=81100037343">Geoffrey Gordon</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 689-696</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143931" title="DOI">10.1145/1143844.1143931</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143931&ftid=364588&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow87" style="display:inline;"><br /><div style="display:inline">In machine learning problems with tens of thousands of features and only dozens or hundreds of independent training examples, dimensionality reduction is essential for good learning performance. In previous work, many researchers have treated the learning ...</div></span>
<span id="toHide87" style="display:none;"><br /><div style="display:inline">In machine learning problems with tens of thousands of features and only dozens or hundreds of independent training examples, dimensionality reduction is essential for good learning performance. In previous work, many researchers have treated the learning problem in two separate phases: first use an algorithm such as singular value decomposition to reduce the dimensionality of the data set, and then use a classification algorithm such as na&iuml;ve Bayes or support vector machines to learn a classifier. We demonstrate that it is possible to combine the two goals of dimensionality reduction and classification into a single learning objective, and present a novel and efficient algorithm which optimizes this objective directly. We present experimental results in fMRI analysis which show that we can achieve better learning performance and lower-dimensional representations than two-phase approaches can.</div></span> <a id="expcoll87" href="JavaScript: expandcollapse('expcoll87',87)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143932">An analytic solution to discrete Bayesian reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100464266">Pascal Poupart</a>,
<a href="author_page.cfm?id=81100229753">Nikos Vlassis</a>,
<a href="author_page.cfm?id=81339505018">Jesse Hoey</a>,
<a href="author_page.cfm?id=81542403556">Kevin Regan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 697-704</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143932" title="DOI">10.1145/1143844.1143932</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143932&ftid=364589&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow88" style="display:inline;"><br /><div style="display:inline">Reinforcement learning (RL) was originally proposed as a framework to allow agents to learn in an online fashion as they interact with their environment. Existing RL algorithms come short of achieving this goal because the amount of exploration required ...</div></span>
<span id="toHide88" style="display:none;"><br /><div style="display:inline">Reinforcement learning (RL) was originally proposed as a framework to allow agents to learn in an online fashion as they interact with their environment. Existing RL algorithms come short of achieving this goal because the amount of exploration required is often too costly and/or too time consuming for online learning. As a result, RL is mostly used for offline learning in simulated environments. We propose a new algorithm, called BEETLE, for effective online learning that is computationally efficient while minimizing the amount of exploration. We take a Bayesian model-based approach, framing RL as a partially observable Markov decision process. Our two main contributions are the analytical derivation that the optimal value function is the upper envelope of a set of multivariate polynomials, and an efficient point-based value iteration algorithm that exploits this simple parameterization.</div></span> <a id="expcoll88" href="JavaScript: expandcollapse('expcoll88',88)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143933">MISSL: multiple-instance semi-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309485680">Rouhollah Rahmani</a>,
<a href="author_page.cfm?id=81331493194">Sally A. Goldman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 705-712</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143933" title="DOI">10.1145/1143844.1143933</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143933&ftid=364590&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow89" style="display:inline;"><br /><div style="display:inline">There has been much work on applying multiple-instance (MI) learning to content-based image retrieval (CBIR) where the goal is to rank all images in a known repository using a small labeled data set. Most existing MI learning algorithms are non-transductive ...</div></span>
<span id="toHide89" style="display:none;"><br /><div style="display:inline">There has been much work on applying multiple-instance (MI) learning to content-based image retrieval (CBIR) where the goal is to rank all images in a known repository using a small labeled data set. Most existing MI learning algorithms are non-transductive in that the images in the repository serve only as test data and are not used in the learning process. We present MISSL (Multiple-Instance Semi-Supervised Learning) that transforms any MI problem into an input for a graph-based single-instance semi-supervised learning method that encodes the MI aspects of the problem simultaneously working at both the bag and point levels. Unlike most prior MI learning algorithms, MISSL makes use of the unlabeled data.</div></span> <a id="expcoll89" href="JavaScript: expandcollapse('expcoll89',89)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143934">Constructing informative priors using transfer learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315491047">Rajat Raina</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>,
<a href="author_page.cfm?id=81100246010">Daphne Koller</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 713-720</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143934" title="DOI">10.1145/1143844.1143934</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143934&ftid=364591&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow90" style="display:inline;"><br /><div style="display:inline">Many applications of supervised learning require good generalization from limited labeled data. In the Bayesian setting, we can try to achieve this goal by using an informative prior over the parameters, one that encodes useful domain knowledge. Focusing ...</div></span>
<span id="toHide90" style="display:none;"><br /><div style="display:inline">Many applications of supervised learning require good generalization from limited labeled data. In the Bayesian setting, we can try to achieve this goal by using an informative prior over the parameters, one that encodes useful domain knowledge. Focusing on logistic regression, we present an algorithm for automatically constructing a multivariate Gaussian prior with a full covariance matrix for a given supervised learning task. This prior relaxes a commonly used but overly simplistic independence assumption, and allows parameters to be dependent. The algorithm uses other "similar" learning problems to estimate the covariance of pairs of individual parameters. We then use a semidefinite program to combine these estimates and learn a good prior for the current learning task. We apply our methods to binary text classification, and demonstrate a 20 to 40% test error reduction over a commonly used prior.</div></span> <a id="expcoll90" href="JavaScript: expandcollapse('expcoll90',90)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143935">CN = CPCN</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100251676">Liva Ralaivola</a>,
<a href="author_page.cfm?id=81100508538">Fran&#231;ois Denis</a>,
<a href="author_page.cfm?id=99658678486">Christophe Nicolas Magnan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 721-728</span></td>
</tr>
<tr>
<td></td>
 <td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143935" title="DOI">10.1145/1143844.1143935</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143935&ftid=364592&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow91" style="display:inline;"><br /><div style="display:inline">We address the issue of the learnability of concept classes under three classification noise models in the probably approximately correct framework. After introducing the Class-Conditional Classification Noise (CCCN) model, we investigate the ...</div></span>
<span id="toHide91" style="display:none;"><br /><div style="display:inline">We address the issue of the learnability of concept classes under three classification noise models in the <i>probably approximately correct</i> framework. After introducing the Class-Conditional Classification Noise (CCCN) model, we investigate the problem of the learnability of concept classes under this particular setting and we show that concept classes that are learnable under the well-known uniform classification noise (CN) setting are also CCCN-learnable, which gives CN = CCCN. We then use this result to prove the equality between the set of concept classes that are CN-learnable and the set of concept classes that are learnable in the Constant Partition Classification Noise (CPCN) setting, or, in other words, we show that CN = CPCN.</div></span> <a id="expcoll91" href="JavaScript: expandcollapse('expcoll91',91)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143936">Maximum margin planning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315491197">Nathan D. Ratliff</a>,
<a href="author_page.cfm?id=81100623616">J. Andrew Bagnell</a>,
<a href="author_page.cfm?id=81100579912">Martin A. Zinkevich</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 729-736</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143936" title="DOI">10.1145/1143844.1143936</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143936&ftid=364593&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow92" style="display:inline;"><br /><div style="display:inline">Imitation learning of sequential, goal-directed behavior by standard supervised techniques is often difficult. We frame learning such behaviors as a maximum margin structured prediction problem over a space of policies. In this approach, we learn mappings ...</div></span>
<span id="toHide92" style="display:none;"><br /><div style="display:inline">Imitation learning of sequential, goal-directed behavior by standard supervised techniques is often difficult. We frame learning such behaviors as a maximum margin structured prediction problem over a space of policies. In this approach, we learn mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior. Further, we demonstrate a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference. Although the technique is general, it is particularly relevant in problems where A* and dynamic programming approaches make learning policies tractable in problems beyond the limitations of a QP formulation. We demonstrate our approach applied to route planning for outdoor mobile robots, where the behavior a designer wishes a planner to execute is often clear, while specifying cost functions that engender this behavior is a much more difficult task.</div></span> <a id="expcoll92" href="JavaScript: expandcollapse('expcoll92',92)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143937">Quadratic programming relaxations for metric labeling and Markov random field MAP estimation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100169005">Pradeep Ravikumar</a>,
<a href="author_page.cfm?id=81100055408">John Lafferty</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 737-744</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143937" title="DOI">10.1145/1143844.1143937</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143937&ftid=364594&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow93" style="display:inline;"><br /><div style="display:inline">Quadratic program relaxations are proposed as an alternative to linear program relaxations and tree reweighted belief propagation for the metric labeling or MAP estimation problem. An additional convex relaxation of the quadratic approximation is shown ...</div></span>
<span id="toHide93" style="display:none;"><br /><div style="display:inline">Quadratic program relaxations are proposed as an alternative to linear program relaxations and tree reweighted belief propagation for the metric labeling or MAP estimation problem. An additional convex relaxation of the quadratic approximation is shown to have additive approximation guarantees that apply even when the graph weights have mixed sign or do not come from a metric. The approximations are extended in a manner that allows tight variational relaxations of the MAP problem, although they generally involve non-convex optimization. Experiments carried out on synthetic data show that the quadratic approximations can be more accurate and computationally efficient than the linear programming and propagation based alternatives.</div></span> <a id="expcoll93" href="JavaScript: expandcollapse('expcoll93',93)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143938">Categorization in multiple category systems</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100227162">Jean-Michel Renders</a>,
<a href="author_page.cfm?id=81100467616">Eric Gaussier</a>,
<a href="author_page.cfm?id=81100154131">Cyril Goutte</a>,
<a href="author_page.cfm?id=99659150443">Francois Pacull</a>,
<a href="author_page.cfm?id=81322491817">Gabriela Csurka</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 745-752</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143938" title="DOI">10.1145/1143844.1143938</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143938&ftid=364595&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow94" style="display:inline;"><br /><div style="display:inline">We explore the situation in which documents have to be categorized into more than one category system, a situation we refer to as multiple-view categorization. More particularly, we address the case where two different categorizers have already ...</div></span>
<span id="toHide94" style="display:none;"><br /><div style="display:inline">We explore the situation in which documents have to be categorized into more than one category system, a situation we refer to as <i>multiple-view categorization</i>. More particularly, we address the case where two different categorizers have already been built based on non-necessarily identical training sets, each one labeled using one category system. On the top of these categorizers considered as black-boxes, we propose some algorithms able to exploit a third training set containing a few examples annotated in both category systems. Such a situation arises for example in large companies where incoming mails have to be routed to several departments, each one relying on its own category system. We focus here on exploiting possible dependencies between category systems in order to refine the categorization decisions made by categorizers trained independently on different category systems. After a description of the multiple categorization problem, we present several possible solutions, based either on a categorization or reweighting approach, and compare them on real data. Lastly, we show how the multimedia categorization problem can be cast as a multiple categorization problem and assess our methods in this framework.</div></span> <a id="expcoll94" href="JavaScript: expandcollapse('expcoll94',94)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143939">How boosting the margin can also boost classifier complexity</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81324492975">Lev Reyzin</a>,
<a href="author_page.cfm?id=81100083454">Robert E. Schapire</a>
 </span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 753-760</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143939" title="DOI">10.1145/1143844.1143939</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143939&ftid=364596&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow95" style="display:inline;"><br /><div style="display:inline">Boosting methods are known not to usually overfit training data even as the size of the generated classifiers becomes large. Schapire et al. attempted to explain this phenomenon in terms of the margins the classifier achieves on training examples. Later, ...</div></span>
<span id="toHide95" style="display:none;"><br /><div style="display:inline">Boosting methods are known not to usually overfit training data even as the size of the generated classifiers becomes large. Schapire et al. attempted to explain this phenomenon in terms of the margins the classifier achieves on training examples. Later, however, Breiman cast serious doubt on this explanation by introducing a boosting algorithm, arc-gv, that can generate a higher margins distribution than AdaBoost and yet performs worse. In this paper, we take a close look at Breiman's compelling but puzzling results. Although we can reproduce his main finding, we find that the poorer performance of arc-gv can be explained by the increased complexity of the base classifiers it uses, an explanation supported by our experiments and entirely consistent with the margins theory. Thus, we find maximizing the margins is desirable, but not necessarily at the expense of other factors, especially base-classifier complexity.</div></span> <a id="expcoll95" href="JavaScript: expandcollapse('expcoll95',95)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143940">Combining discriminative features to infer complex trajectories</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100265794">David A. Ross</a>,
<a href="author_page.cfm?id=81100099327">Simon Osindero</a>,
<a href="author_page.cfm?id=81100597418">Richard S. Zemel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 761-768</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143940" title="DOI">10.1145/1143844.1143940</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143940&ftid=364597&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow96" style="display:inline;"><br /><div style="display:inline">We propose a new model for the probabilistic estimation of continuous state variables from a sequence of observations, such as tracking the position of an object in video. This mapping is modeled as a product of dynamics experts (features relating the ...</div></span>
<span id="toHide96" style="display:none;"><br /><div style="display:inline">We propose a new model for the probabilistic estimation of continuous state variables from a sequence of observations, such as tracking the position of an object in video. This mapping is modeled as a product of dynamics experts (features relating the state at adjacent time-steps) and observation experts (features relating the state to the image sequence). Individual features are flexible in that they can switch on or off at each time-step depending on their inferred relevance (or on additional side information), and discriminative in that they need not model the full generative likelihood of the data. When trained conditionally, this permits the inclusion of a broad range of rich features (for example, features relying on observations from multiple time-steps), and allows the relevance of features to be learned from labeled sequences.</div></span> <a id="expcoll96" href="JavaScript: expandcollapse('expcoll96',96)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143941">Sequential update of ADtrees</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100255879">Josep Roure</a>,
<a href="author_page.cfm?id=81100042782">Andrew W. Moore</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 769-776</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143941" title="DOI">10.1145/1143844.1143941</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143941&ftid=364598&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow97" style="display:inline;"><br /><div style="display:inline">Ingcreasingly, data-mining algorithms must deal with databases that continuously grow over time. These algorithms must avoid repeatedly scanning their databases. When database attributes are symbolic, ADtrees have already shown to be efficient structures ...</div></span>
<span id="toHide97" style="display:none;"><br /><div style="display:inline">Ingcreasingly, data-mining algorithms must deal with databases that continuously grow over time. These algorithms must avoid repeatedly scanning their databases. When database attributes are symbolic, ADtrees have already shown to be efficient structures to store sufficient statistics in main memory and to accelerate the mining process in batch environments. Here we present an efficient method to sequentially update ADtrees that is suitable for incremental environments.</div></span> <a id="expcoll97" href="JavaScript: expandcollapse('expcoll97',97)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143942">Predictive linear-Gaussian models of controlled stochastic dynamical systems</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100224503">Matthew Rudary</a>,
<a href="author_page.cfm?id=81327491508">Satinder Singh</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 777-784</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143942" title="DOI">10.1145/1143844.1143942</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143942&ftid=364599&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow98" style="display:inline;"><br /><div style="display:inline">We introduce the controlled predictive linear-Gaussian model (cPLG), a model that uses predictive state to model discrete-time dynamical systems with real-valued observations and vector-valued actions. This extends the PLG, an uncontrolled model recently ...</div></span>
<span id="toHide98" style="display:none;"><br /><div style="display:inline">We introduce the controlled predictive linear-Gaussian model (cPLG), a model that uses predictive state to model discrete-time dynamical systems with real-valued observations and vector-valued actions. This extends the PLG, an uncontrolled model recently introduced by Rudary et al. (2005). We show that the cPLG subsumes controlled linear dynamical systems (LDS, also called Kalman filter models) of equal dimension, but requires fewer parameters. We also introduce the predictive linear-quadratic Gaussian problem, a cost-minimization problem based on the cPLG that we show is equivalent to linear-quadratic Gaussian problems (LQG, sometimes called LQR). We present an algorithm to estimate cPLG parameters from data, and show that our algorithm is a consistent estimation procedure. Finally, we present empirical results suggesting that our algorithm performs favorably compared to expectation maximization on controlled LDS models.</div></span> <a id="expcoll98" href="JavaScript: expandcollapse('expcoll98',98)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143943">A statistical approach to rule learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100319837">Ulrich R&#252;ckert</a>,
<a href="author_page.cfm?id=81100175708">Stefan Kramer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 785-792</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143943" title="DOI">10.1145/1143844.1143943</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143943&ftid=364600&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow99" style="display:inline;"><br /><div style="display:inline">We present a new, statistical approach to rule learning. Doing so, we address two of the problems inherent in traditional rule learning: The computational hardness of finding rule sets with low training error and the need for capacity control to avoid ...</div></span>
<span id="toHide99" style="display:none;"><br /><div style="display:inline">We present a new, statistical approach to rule learning. Doing so, we address two of the problems inherent in traditional rule learning: The computational hardness of finding rule sets with low training error and the need for capacity control to avoid over-fitting. The chosen representation involves weights attached to rules. Instead of optimizing the error rate directly, we optimize for rule sets that have large margin and low variance. This can be formulated as a convex optimization problem allowing for efficient computation. Given the representation and the optimization procedure, we effectively yield weighted clauses in a CNF-like representation. To avoid overfitting, we propose a model selection strategy that utilizes a novel concentration inequality. Empirical tests show that the system is competitive with existing rule learning algorithms and that its flexible learning bias can be adjusted to improve predictive accuracy considerably.</div></span> <a id="expcoll99" href="JavaScript: expandcollapse('expcoll99',99)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143944">Efficient inference on sequence segmentation models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100043079">Sunita Sarawagi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 793-800</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143944" title="DOI">10.1145/1143844.1143944</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143944&ftid=364601&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow100" style="display:inline;"><br /><div style="display:inline">Sequence segmentation is a flexible and highly accurate mechanism for modeling several applications. Inference on segmentation models involves dynamic programming computations that in the worst case can be cubic in the length of a sequence. In contrast, ...</div></span>
<span id="toHide100" style="display:none;"><br /><div style="display:inline">Sequence segmentation is a flexible and highly accurate mechanism for modeling several applications. Inference on segmentation models involves dynamic programming computations that in the worst case can be cubic in the length of a sequence. In contrast, typical sequence labeling models require linear time. We remove this limitation of segmentation models vis-a-vis sequential models by designing a succinct representation of potentials common across overlapping segments. We exploit such potentials to design efficient inference algorithms that are both analytically shown to have a lower complexity and empirically found to be comparable to sequential models for typical extraction tasks.</div></span> <a id="expcoll100" href="JavaScript: expandcollapse('expcoll100',100)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143945">Cost-sensitive learning with conditional Markov networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81361600464">Prithviraj Sen</a>,
<a href="author_page.cfm?id=81100205081">Lise Getoor</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 801-808</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143945" title="DOI">10.1145/1143844.1143945</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143945&ftid=364602&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow101" style="display:inline;"><br /><div style="display:inline">There has been a recent, growing interest in classification and link prediction in structured domains. Methods such as CRFs (Lafferty et al., 2001) and RMNs (Taskar et al., 2002) support flexible mechanisms for modeling correlations due to the link structure. ...</div></span>
<span id="toHide101" style="display:none;"><br /><div style="display:inline">There has been a recent, growing interest in classification and link prediction in structured domains. Methods such as CRFs (Lafferty et al., 2001) and RMNs (Taskar et al., 2002) support flexible mechanisms for modeling correlations due to the link structure. In addition, in many structured domains, there is an interesting structure in the risk or cost function associated with different misclassifications. There is a rich tradition of cost-sensitive learning applied to unstructured (IID) data. Here we propose a general framework which can capture correlations in the link structure and handle <i>structured</i> cost functions. We present a novel cost-sensitive structured classifier based on Maximum Entropy principles that directly determines the cost-sensitive classification. We contrast this with an approach which employs a standard 0/1 loss structured classifier followed by minimization of the expected cost of misclassification. We demonstrate the utility of our proposed classifier with experiments on both synthetic and real-world data.</div></span> <a id="expcoll101" href="JavaScript: expandcollapse('expcoll101',101)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143946">Feature value acquisition in testing: a sequential batch test algorithm</a></span></td>
</tr>
<tr>
<td> </td>
 <td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317489690">Victor S. Sheng</a>,
<a href="author_page.cfm?id=81100159332">Charles X. Ling</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 809-816</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143946" title="DOI">10.1145/1143844.1143946</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143946&ftid=364603&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow102" style="display:inline;"><br /><div style="display:inline">In medical diagnosis, doctors often have to order sets of medical tests in sequence in order to make an accurate diagnosis of patient diseases. While doing so they have to make a trade-off between the cost of the tests and possible misdiagnosis. In this ...</div></span>
<span id="toHide102" style="display:none;"><br /><div style="display:inline">In medical diagnosis, doctors often have to order sets of medical tests in sequence in order to make an accurate diagnosis of patient diseases. While doing so they have to make a trade-off between the cost of the tests and possible misdiagnosis. In this paper, we use cost-sensitive learning to model this process. We assume that test examples (new patients) may contain missing values, and their actual values can be acquired at cost (similar to doing medical tests) in order to reduce misclassification errors (misdiagnosis). We propose a novel Sequential Batch Test algorithm that can acquire sets of attribute values in sequence, similar to sets of medical tests ordered by doctors in sequence. The goal of our algorithm is to minimize the total cost (i.e., the trade-off) of acquiring attribute values and misclassifications. We demonstrate the effectiveness of our algorithm, and show that it outperforms previous methods significantly. Our algorithm can be readily applied in real-world diagnosis tasks. A case study on the heart disease is given in the paper.</div></span> <a id="expcoll102" href="JavaScript: expandcollapse('expcoll102',102)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143947">Permutation invariant SVMs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315491884">Pannagadatta K. Shivaswamy</a>,
<a href="author_page.cfm?id=81100510477">Tony Jebara</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 817-824</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143947" title="DOI">10.1145/1143844.1143947</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143947&ftid=364604&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow103" style="display:inline;"><br /><div style="display:inline">We extend Support Vector Machines to input spaces that are sets by ensuring that the classifier is invariant to permutations of sub-elements within each input. Such permutations include reordering of scalars in an input vector, re-orderings of tuples ...</div></span>
<span id="toHide103" style="display:none;"><br /><div style="display:inline">We extend Support Vector Machines to input spaces that are sets by ensuring that the classifier is invariant to permutations of sub-elements within each input. Such permutations include reordering of scalars in an input vector, re-orderings of tuples in an input matrix or re-orderings of general objects (in Hilbert spaces) within a set as well. This approach induces permutational invariance in the classifier which can then be directly applied to unusual set-based representations of data. The permutation invariant Support Vector Machine alternates the Hungarian method for maximum weight matching within the maximum margin learning procedure. We effectively estimate and apply permutations to the input data points to maximize classification margin while minimizing data radius. This procedure has a strong theoretical justification via well established error probability bounds. Experiments are shown on character recognition, 3D object recognition and various UCI datasets.</div></span> <a id="expcoll103" href="JavaScript: expandcollapse('expcoll103',103)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143948">Bayesian learning of measurement and structural models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81406593387">Ricardo Silva</a>,
<a href="author_page.cfm?id=81100130665">Richard Scheines</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 825-832</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143948" title="DOI">10.1145/1143844.1143948</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143948&ftid=364605&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow104" style="display:inline;"><br /><div style="display:inline">We present a Bayesian search algorithm for learning the structure of latent variable models of continuous variables. We stress the importance of applying search operators designed especially for the parametric family used in our models. This is performed ...</div></span>
<span id="toHide104" style="display:none;"><br /><div style="display:inline">We present a Bayesian search algorithm for learning the structure of latent variable models of continuous variables. We stress the importance of applying search operators designed especially for the parametric family used in our models. This is performed by searching for subsets of the observed variables whose covariance matrix can be represented as a sum of a matrix of low rank and a diagonal matrix of residuals. The resulting search procedure is relatively efficient, since the main search operator has a branch factor that grows linearly with the number of variables. The resulting models are often simpler and give a better fit than models based on generalizations of factor analysis or those derived from standard hill-climbing methods.</div></span> <a id="expcoll104" href="JavaScript: expandcollapse('expcoll104',104)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143949">An intrinsic reward mechanism for efficient exploration</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100378234">&#214;zg&#252;r &#350;im&#351;ek</a>,
<a href="author_page.cfm?id=81100207208">Andrew G. Barto</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 833-840</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143949" title="DOI">10.1145/1143844.1143949</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143949&ftid=364606&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow105" style="display:inline;"><br /><div style="display:inline">How should a reinforcement learning agent act if its sole purpose is to efficiently learn an optimal policy for later use? In other words, how should it explore, to be able to exploit later? We formulate this problem as a Markov Decision Process by explicitly ...</div></span>
<span id="toHide105" style="display:none;"><br /><div style="display:inline">How should a reinforcement learning agent act if its sole purpose is to efficiently learn an optimal policy for later use? In other words, how should it explore, to be able to exploit later? We formulate this problem as a Markov Decision Process by explicitly modeling the internal state of the agent and propose a principled heuristic for its solution. We present experimental results in a number of domains, also exploring the algorithm's use for learning a policy for a skill given its reward function---an important but neglected component of skill discovery.</div></span> <a id="expcoll105" href="JavaScript: expandcollapse('expcoll105',105)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143950">Deterministic annealing for semi-supervised kernel machines</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309499649">Vikas Sindhwani</a>,
<a href="author_page.cfm?id=81100176197">S. Sathiya Keerthi</a>,
<a href="author_page.cfm?id=81100497375">Olivier Chapelle</a>
</span>
</td>
</tr>
<tr>
<td></td>
 <td> <span style="padding-left:0">Pages: 841-848</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143950" title="DOI">10.1145/1143844.1143950</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143950&ftid=364607&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow106" style="display:inline;"><br /><div style="display:inline">An intuitive approach to utilizing unlabeled data in kernel-based classification algorithms is to simply treat unknown labels as additional optimization variables. For margin-based loss functions, one can view this approach as attempting to learn low-density ...</div></span>
<span id="toHide106" style="display:none;"><br /><div style="display:inline">An intuitive approach to utilizing unlabeled data in kernel-based classification algorithms is to simply treat unknown labels as additional optimization variables. For margin-based loss functions, one can view this approach as attempting to learn low-density separators. However, this is a hard optimization problem to solve in typical semi-supervised settings where unlabeled data is abundant. The popular Transductive SVM algorithm is a label-switching-retraining procedure that is known to be susceptible to local minima. In this paper, we present a global optimization framework for semi-supervised Kernel machines where an easier problem is parametrically deformed to the original hard problem and minimizers are smoothly tracked. Our approach is motivated from deterministic annealing techniques and involves a sequence of convex optimization problems that are exactly and efficiently solved. We present empirical results on several synthetic and real world datasets that demonstrate the effectiveness of our approach.</div></span> <a id="expcoll106" href="JavaScript: expandcollapse('expcoll106',106)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143951">Feature subset selection bias for classification learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315491981">Surendra K. Singhi</a>,
<a href="author_page.cfm?id=81367594306">Huan Liu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 849-856</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143951" title="DOI">10.1145/1143844.1143951</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143951&ftid=364608&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow107" style="display:inline;"><br /><div style="display:inline">Feature selection is often applied to high-dimensional data prior to classification learning. Using the same training dataset in both selection and learning can result in so-called feature subset selection bias. This bias putatively can exacerbate ...</div></span>
<span id="toHide107" style="display:none;"><br /><div style="display:inline">Feature selection is often applied to high-dimensional data prior to classification learning. Using the <i>same</i> training dataset in both selection and learning can result in so-called feature subset selection bias. This bias putatively can exacerbate data over-fitting and negatively affect classification performance. However, in current practice <i>separate</i> datasets are seldom employed for selection and learning, because dividing the training data into two datasets for feature selection and classifier learning respectively reduces the amount of data that can be used in either task. This work attempts to address this dilemma. We formalize selection bias for classification learning, analyze its statistical properties, and study factors that affect selection bias, as well as how the bias impacts classification learning via various experiments. This research endeavors to provide illustration and explanation why the bias may not cause negative impact in classification as much as expected in regression.</div></span> <a id="expcoll107" href="JavaScript: expandcollapse('expcoll107',107)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143952">Classifying EEG for brain-computer interfaces: learning optimal filters for dynamical system features</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100578710">Le Song</a>,
<a href="author_page.cfm?id=81339498387">Julien Epps</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 857-864</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143952" title="DOI">10.1145/1143844.1143952</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143952&ftid=364609&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow108" style="display:inline;"><br /><div style="display:inline">Classification of multichannel EEG recordings during motor imagination has been exploited successfully for brain-computer interfaces (BCI). In this paper, we consider EEG signals as the outputs of a networked dynamical system (the cortex), and exploit ...</div></span>
<span id="toHide108" style="display:none;"><br /><div style="display:inline">Classification of multichannel EEG recordings during motor imagination has been exploited successfully for brain-computer interfaces (BCI). In this paper, we consider EEG signals as the outputs of a networked dynamical system (the cortex), and exploit novel features from the collective dynamics of the system for classification. Herein, we also propose a new framework for learning optimal filters automatically from the data, by employing a Fisher ratio criterion. Experimental evaluations comparing the proposed dynamical system features with the CSP and the AR features reveal their competitive performance during classification. Results also show the benefits of employing the spatial and the temporal filters optimized using the proposed learning approach.</div></span> <a id="expcoll108" href="JavaScript: expandcollapse('expcoll108',108)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143953">An investigation of computational and informational limits in Gaussian mixture clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100005542">Nathan Srebro</a>,
<a href="author_page.cfm?id=81100350725">Gregory Shakhnarovich</a>,
<a href="author_page.cfm?id=81100445880">Sam Roweis</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 865-872</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143953" title="DOI">10.1145/1143844.1143953</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143953&ftid=364610&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow109" style="display:inline;"><br /><div style="display:inline">We investigate under what conditions clustering by learning a mixture of spherical Gaussians is (a) computationally tractable; and (b) statistically possible. We show that using principal component projection greatly aids in recovering the clustering ...</div></span>
<span id="toHide109" style="display:none;"><br /><div style="display:inline">We investigate under what conditions clustering by learning a mixture of spherical Gaussians is (a) computationally tractable; and (b) statistically possible. We show that using principal component projection greatly aids in recovering the clustering using EM; present empirical evidence that even using such a projection, there is still a large gap between the number of samples needed to recover the clustering using EM, and the number of samples needed without computational restrictions; and characterize the regime in which such a gap exists.</div></span> <a id="expcoll109" href="JavaScript: expandcollapse('expcoll109',109)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143954">Bayesian pattern ranking for move prediction in the game of Go</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331504390">David Stern</a>,
<a href="author_page.cfm?id=81100124906">Ralf Herbrich</a>,
<a href="author_page.cfm?id=81100450764">Thore Graepel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 873-880</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143954" title="DOI">10.1145/1143844.1143954</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143954&ftid=364611&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow110" style="display:inline;"><br /><div style="display:inline">We investigate the problem of learning to predict moves in the board game of Go from game records of expert players. In particular, we obtain a probability distribution over legal moves for professional play in a given position. This distribution has ...</div></span>
<span id="toHide110" style="display:none;"><br /><div style="display:inline">We investigate the problem of learning to predict moves in the board game of Go from game records of expert players. In particular, we obtain a probability distribution over legal moves for professional play in a given position. This distribution has numerous applications in computer Go, including serving as an efficient stand-alone Go player. It would also be effective as a move selector and move sorter for game tree search and as a training tool for Go players. Our method has two major components: a) a pattern extraction scheme for efficiently harvesting patterns of given size and shape from expert game records and b) a Bayesian learning algorithm (in two variants) that learns a distribution over the values of a move given a board position based on the local pattern context. The system is trained on 181,000 expert games and shows excellent prediction performance as indicated by its ability to perfectly predict the moves made by professional Go players in 34% of test positions.</div></span> <a id="expcoll110" href="JavaScript: expandcollapse('expcoll110',110)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143955">PAC model-free reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100216859">Alexander L. Strehl</a>,
<a href="author_page.cfm?id=81331497976">Lihong Li</a>,
<a href="author_page.cfm?id=81309512292">Eric Wiewiora</a>,
<a href="author_page.cfm?id=81100453722">John Langford</a>,
<a href="author_page.cfm?id=81406601119">Michael L. Littman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 881-888</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143955" title="DOI">10.1145/1143844.1143955</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143955&ftid=364612&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow111" style="display:inline;"><br /><div style="display:inline">For a Markov Decision Process with finite state (size S) and action spaces (size A per state), we propose a new algorithm---Delayed Q-Learning. We prove it is PAC, achieving near optimal performance except for &Otilde;(SA) timesteps ...</div></span>
<span id="toHide111" style="display:none;"><br /><div style="display:inline">For a Markov Decision Process with finite state (size <i>S</i>) and action spaces (size <i>A</i> per state), we propose a new algorithm---Delayed Q-Learning. We prove it is PAC, achieving near optimal performance except for &Otilde;(<i>SA</i>) timesteps using <i>O(SA)</i> space, improving on the &Otilde;(<i>S</i><sup>2</sup> <i>A</i>) bounds of best previous algorithms. This result proves efficient reinforcement learning is possible without learning a model of the MDP from experience. Learning takes place from a single continuous thread of experience---no resets nor parallel sampling is used. Beyond its smaller storage and experience requirements, Delayed Q-learning's per-experience computation cost is much less than that of previous PAC algorithms.</div></span> <a id="expcoll111" href="JavaScript: expandcollapse('expcoll111',111)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143956">Experience-efficient learning in associative bandit problems</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100216859">Alexander L. Strehl</a>,
<a href="author_page.cfm?id=81100644310">Chris Mesterharm</a>,
<a href="author_page.cfm?id=81406601119">Michael L. Littman</a>,
<a href="author_page.cfm?id=81100325321">Haym Hirsh</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 889-896</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143956" title="DOI">10.1145/1143844.1143956</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143956&ftid=364613&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow112" style="display:inline;"><br /><div style="display:inline">We formalize the associative bandit problem framework introduced by Kaelbling as a learning-theory problem. The learning environment is modeled as a k-armed bandit where arm payoffs are conditioned on an observable input selected on each trial. ...</div></span>
<span id="toHide112" style="display:none;"><br /><div style="display:inline">We formalize the associative bandit problem framework introduced by Kaelbling as a learning-theory problem. The learning environment is modeled as a <i>k</i>-armed bandit where arm payoffs are conditioned on an observable input selected on each trial. We show that, if the payoff functions are constrained to a known hypothesis class, learning can be performed efficiently with respect to the VC dimension of this class. We formally reduce the problem of PAC classification to the associative bandit problem, producing an efficient algorithm for any hypothesis class for which efficient classification algorithms are known. We demonstrate the approach empirically on a scalable concept class.</div></span> <a id="expcoll112" href="JavaScript: expandcollapse('expcoll112',112)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143957">Full Bayesian network classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323496337">Jiang Su</a>,
<a href="author_page.cfm?id=81100120645">Harry Zhang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 897-904</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143957" title="DOI">10.1145/1143844.1143957</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143957&ftid=364614&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow113" style="display:inline;"><br /><div style="display:inline">The structure of a Bayesian network (BN) encodes variable independence. Learning the structure of a BN, however, is typically of high computational complexity. In this paper, we explore and represent variable independence in learning conditional probability ...</div></span>
<span id="toHide113" style="display:none;"><br /><div style="display:inline">The structure of a Bayesian network (BN) encodes variable independence. Learning the structure of a BN, however, is typically of high computational complexity. In this paper, we explore and represent variable independence in learning conditional probability tables (CPTs), instead of in learning structure. A full Bayesian network is used as the structure and a decision tree is learned for each CPT. The resulting model is called <i>full Bayesian network classifiers (FBCs)</i>. In learning an <i>FBC</i>, learning the decision trees for CPTs captures essentially both variable independence and context-specific independence. We present a novel, efficient decision tree learning, which is also effective in the context of <i>FBC</i> learning. In our experiments, the <i>FBC</i> learning algorithm demonstrates better performance in both classification and ranking compared with other state-of-the-art learning algorithms. In addition, its reduced effort on structure learning makes its time complexity quite low as well.</div></span> <a id="expcoll113" href="JavaScript: expandcollapse('expcoll113',113)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143958">Local Fisher discriminant analysis for supervised dimensionality reduction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100105605">Masashi Sugiyama</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 905-912</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143958" title="DOI">10.1145/1143844.1143958</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143958&ftid=364615&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow114" style="display:inline;"><br /><div style="display:inline">Dimensionality reduction is one of the important preprocessing steps in high-dimensional data analysis. In this paper, we consider the supervised dimensionality reduction problem where samples are accompanied with class labels. Traditional Fisher discriminant ...</div></span>
<span id="toHide114" style="display:none;"><br /><div style="display:inline">Dimensionality reduction is one of the important preprocessing steps in high-dimensional data analysis. In this paper, we consider the supervised dimensionality reduction problem where samples are accompanied with class labels. Traditional Fisher discriminant analysis is a popular and powerful method for this purpose. However, it tends to give undesired results if samples in some class form several separate clusters, i.e., multimodal. In this paper, we propose a new dimensionality reduction method called local Fisher discriminant analysis (LFDA), which is a localized variant of Fisher discriminant analysis. LFDA takes local structure of the data into account so the multimodal data can be embedded appropriately. We also show that LFDA can be extended to non-linear dimensionality reduction scenarios by the kernel trick.</div></span> <a id="expcoll114" href="JavaScript: expandcollapse('expcoll114',114)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143959">Iterative RELIEF for feature weighting</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309512519">Yijun Sun</a>,
<a href="author_page.cfm?id=81100486125">Jian Li</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 913-920</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143959" title="DOI">10.1145/1143844.1143959</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143959&ftid=364616&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow115" style="display:inline;"><br /><div style="display:inline">We propose a series of new feature weighting algorithms, all stemming from a new interpretation of RELIEF as an online algorithm that solves a convex optimization problem with a margin-based objective function. The new interpretation explains the simplicity ...</div></span>
<span id="toHide115" style="display:none;"><br /><div style="display:inline">We propose a series of new feature weighting algorithms, all stemming from a new interpretation of RELIEF as an online algorithm that solves a convex optimization problem with a margin-based objective function. The new interpretation explains the simplicity and effectiveness of RELIEF, and enables us to identify some of its weaknesses. We offer an analytic solution to mitigate these problems. We extend the newly proposed algorithm to handle multiclass problems by using a new multiclass margin definition. To reduce computational costs, an online learning algorithm is also developed. Convergence theorems of the proposed algorithms are presented. Some experiments based on the UCI and microarray datasets are performed to demonstrate the effectiveness of the proposed algorithms.</div></span> <a id="expcoll115" href="JavaScript: expandcollapse('expcoll115',115)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143960">Multiclass reduced-set support vector machines</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81310493876">Benyang Tang</a>,
<a href="author_page.cfm?id=81331499157">Dominic Mazzoni</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 921-928</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143960" title="DOI">10.1145/1143844.1143960</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143960&ftid=364617&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow116" style="display:inline;"><br /><div style="display:inline">There are well-established methods for reducing the number of support vectors in a trained binary support vector machine, often with minimal impact on accuracy. We show how reduced-set methods can be applied to multiclass SVMs made up of several binary ...</div></span>
<span id="toHide116" style="display:none;"><br /><div style="display:inline">There are well-established methods for reducing the number of support vectors in a trained binary support vector machine, often with minimal impact on accuracy. We show how reduced-set methods can be applied to multiclass SVMs made up of several binary SVMs, with significantly better results than reducing each binary SVM independently. Our approach is based on Burges' approach that constructs each reduced-set vector as the pre-image of a vector in kernel space, but we extend this by recomputing the SVM weights and bias optimally using the original SVM objective function. This leads to greater accuracy for a binary reduced-set SVM, and also allows vectors to be "shared" between multiple binary SVMs for greater multiclass accuracy with fewer reduced-set vectors. We also propose computing pre-images using differential evolution, which we have found to be more robust than gradient descent alone. We show experimental results on a variety of problems and find that this new approach is consistently better than previous multiclass reduced-set methods, sometimes with a dramatic difference.</div></span> <a id="expcoll116" href="JavaScript: expandcollapse('expcoll116',116)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143961">Fast and space efficient string kernels using suffix arrays</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315491657">Choon Hui Teo</a>,
<a href="author_page.cfm?id=81100528901">S. V. N. Vishwanathan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 929-936</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143961" title="DOI">10.1145/1143844.1143961</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143961&ftid=364618&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow117" style="display:inline;"><br /><div style="display:inline">String kernels which compare the set of all common substrings between two given strings have recently been proposed by Vishwanathan & Smola (2004). Surprisingly, these kernels can be computed in linear time and linear space using annotated suffix trees. ...</div></span>
<span id="toHide117" style="display:none;"><br /><div style="display:inline">String kernels which compare the set of all common substrings between two given strings have recently been proposed by Vishwanathan & Smola (2004). Surprisingly, these kernels can be computed in linear time and linear space using annotated suffix trees. Even though, in theory, the suffix tree based algorithm requires <i>O</i>(<i>n</i>) space for an <i>n</i> length string, in practice at least 40<i>n</i> bytes are required -- 20<i>n</i> bytes for storing the suffix tree, and an additional 20<i>n</i> bytes for the annotation. This large memory requirement coupled with poor locality of memory access, inherent due to the use of suffix trees, means that the performance of the suffix tree based algorithm deteriorates on large strings. In this paper, we describe a new linear time yet space efficient and scalable algorithm for computing string kernels, based on suffix arrays. Our algorithm is a) faster and easier to implement, b) on the average requires only 19<i>n</i> bytes of storage, and c) exhibits strong locality of memory access. We show that our algorithm can be extended to perform linear time prediction on a test string, and present experiments to validate our claims.</div></span> <a id="expcoll117" href="JavaScript: expandcollapse('expcoll117',117)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143962">Bayesian regression with input noise for high dimensional data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331506008">Jo-Anne Ting</a>,
<a href="author_page.cfm?id=81100460471">Aaron D'Souza</a>,
<a href="author_page.cfm?id=81100472883">Stefan Schaal</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 937-944</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143962" title="DOI">10.1145/1143844.1143962</a></span></td>
</tr>
<tr>
 <td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143962&ftid=364619&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow118" style="display:inline;"><br /><div style="display:inline">This paper examines high dimensional regression with noise-contaminated input and output data. Goals of such learning problems include optimal prediction with noiseless query points and optimal system identification. As a first step, we focus on linear ...</div></span>
<span id="toHide118" style="display:none;"><br /><div style="display:inline">This paper examines high dimensional regression with noise-contaminated input and output data. Goals of such learning problems include optimal prediction with noiseless query points and optimal system identification. As a first step, we focus on linear regression methods, since these can be easily cast into nonlinear learning problems with locally weighted learning approaches. Standard linear regression algorithms generate biased regression estimates if input noise is present and suffer numerically when the data contains redundancy and irrelevancy. Inspired by Factor Analysis Regression, we develop a variational Bayesian algorithm that is robust to ill-conditioned data, automatically detects relevant features, and identifies input and output noise -- all in a computationally efficient way. We demonstrate the effectiveness of our techniques on synthetic data and on a system identification task for a rigid body dynamics model of a robotic vision head. Our algorithm performs 10 to 70% better than previously suggested methods.</div></span> <a id="expcoll118" href="JavaScript: expandcollapse('expcoll118',118)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143963">Probabilistic inference for solving discrete and continuous state Markov Decision Processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331505904">Marc Toussaint</a>,
<a href="author_page.cfm?id=81100305690">Amos Storkey</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 945-952</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143963" title="DOI">10.1145/1143844.1143963</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143963&ftid=364620&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow119" style="display:inline;"><br /><div style="display:inline">Inference in Markov Decision Processes has recently received interest as a means to infer goals of an observed action, policy recognition, and also as a tool to compute policies. A particularly interesting aspect of the approach is that any existing ...</div></span>
<span id="toHide119" style="display:none;"><br /><div style="display:inline">Inference in Markov Decision Processes has recently received interest as a means to infer goals of an observed action, policy recognition, and also as a tool to compute policies. A particularly interesting aspect of the approach is that any existing inference technique in DBNs now becomes available for answering behavioral question--including those on continuous, factorial, or hierarchical state representations. Here we present an Expectation Maximization algorithm for computing optimal policies. Unlike previous approaches we can show that this actually optimizes the discounted expected future return for arbitrary reward functions and without assuming an ad hoc finite total time. The algorithm is generic in that any inference technique can be utilized in the E-step. We demonstrate this for exact inference on a discrete maze and Gaussian belief state propagation in continuous stochastic optimal control problems.</div></span> <a id="expcoll119" href="JavaScript: expandcollapse('expcoll119',119)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143964">Clustering graphs by weighted substructure mining</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100030766">Koji Tsuda</a>,
<a href="author_page.cfm?id=81100510747">Taku Kudo</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 953-960</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143964" title="DOI">10.1145/1143844.1143964</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143964&ftid=364621&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow120" style="display:inline;"><br /><div style="display:inline">Graph data is getting increasingly popular in, e.g., bioinformatics and text processing. A main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of ...</div></span>
<span id="toHide120" style="display:none;"><br /><div style="display:inline">Graph data is getting increasingly popular in, e.g., bioinformatics and text processing. A main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of indicators of all possible subgraphs, the dimensionality gets too large for usual statistical methods. We propose an efficient method for learning a binomial mixture model in this feature space. Combining the <i>l</i><inf>1</inf> regularizer and the data structure called DFS code tree, the MAP estimate of non-zero parameters are computed efficiently by means of the EM algorithm. Our method is applied to the clustering of RNA graphs, and is compared favorably with graph kernels and the spectral graph distance.</div></span> <a id="expcoll120" href="JavaScript: expandcollapse('expcoll120',120)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143965">Active sampling for detecting irrelevant features</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100522049">Sriharsha Veeramachaneni</a>,
<a href="author_page.cfm?id=81315490568">Emanuele Olivetti</a>,
<a href="author_page.cfm?id=81100296767">Paolo Avesani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 961-968</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143965" title="DOI">10.1145/1143844.1143965</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143965&ftid=364622&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow121" style="display:inline;"><br /><div style="display:inline">The general approach for automatically driving data collection using information from previously acquired data is called active learning. Traditional active learning addresses the problem of choosing the unlabeled examples for which the class labels ...</div></span>
<span id="toHide121" style="display:none;"><br /><div style="display:inline">The general approach for automatically driving data collection using information from previously acquired data is called active learning. Traditional active learning addresses the problem of choosing the unlabeled examples for which the class labels are queried with the goal of learning a classifier. In contrast we address the problem of active feature sampling for detecting useless features. We propose a strategy to actively sample the values of new features on class-labeled examples, with the objective of feature relevance assessment. We derive an active feature sampling algorithm from an information theoretic and statistical formulation of the problem. We present experimental results on synthetic, UCI and real world datasets to demonstrate that our active sampling algorithm can provide accurate estimates of feature relevance with lower data acquisition costs than random sampling and other previously proposed sampling algorithms.</div></span> <a id="expcoll121" href="JavaScript: expandcollapse('expcoll121',121)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143966">Accelerated training of conditional random fields with stochastic gradient methods</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100528901">S. V. N. Vishwanathan</a>,
<a href="author_page.cfm?id=81315491934">Nicol N. Schraudolph</a>,
<a href="author_page.cfm?id=81331503247">Mark W. Schmidt</a>,
<a href="author_page.cfm?id=81333490204">Kevin P. Murphy</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 969-976</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143966" title="DOI">10.1145/1143844.1143966</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143966&ftid=364623&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow122" style="display:inline;"><br /><div style="display:inline">We apply Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, to the training of Conditional Random Fields (CRFs). On several large data sets, the resulting optimizer converges to the same quality of solution ...</div></span>
<span id="toHide122" style="display:none;"><br /><div style="display:inline">We apply Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, to the training of Conditional Random Fields (CRFs). On several large data sets, the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS, the leading method reported to date. We report results for both exact and inexact inference techniques.</div></span> <a id="expcoll122" href="JavaScript: expandcollapse('expcoll122',122)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143967">Topic modeling: beyond bag-of-words</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315492225">Hanna M. Wallach</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 977-984</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143967" title="DOI">10.1145/1143844.1143967</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143967&ftid=364624&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow123" style="display:inline;"><br /><div style="display:inline">Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the "bag-of-words" assumption, in which word order is ignored. Previously, these methods have not ...</div></span>
<span id="toHide123" style="display:none;"><br /><div style="display:inline">Some models of textual corpora employ text generation methods involving <i>n</i>-gram statistics, while others use latent topic variables inferred using the "bag-of-words" assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both <i>n</i>-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful.</div></span> <a id="expcoll123" href="JavaScript: expandcollapse('expcoll123',123)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143968">Label propagation through linear neighborhoods</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81408592258">Fei Wang</a>,
<a href="author_page.cfm?id=81372592002">Changshui Zhang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 985-992</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143968" title="DOI">10.1145/1143844.1143968</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143968&ftid=364625&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow124" style="display:inline;"><br /><div style="display:inline">A novel semi-supervised learning approach is proposed based on a linear neighborhood model, which assumes that each data point can be linearly reconstructed from its neighborhood. Our algorithm, named Linear Neighborhood Propagation (LNP), can ...</div></span>
<span id="toHide124" style="display:none;"><br /><div style="display:inline">A novel semi-supervised learning approach is proposed based on a linear neighborhood model, which assumes that each data point can be linearly reconstructed from its neighborhood. Our algorithm, named <i>Linear Neighborhood Propagation (LNP)</i>, can propagate the labels from the labeled points to the whole dataset using these linear neighborhoods with sufficient smoothness. We also derive an easy way to extend <i>LNP</i> to out-of-sample data. Promising experimental results are presented for synthetic data, digit and text classification tasks.</div></span> <a id="expcoll124" href="JavaScript: expandcollapse('expcoll124',124)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143969">Two-dimensional solution path for support vector regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81539583956">Gang Wang</a>,
<a href="author_page.cfm?id=81100614347">Dit-Yan Yeung</a>,
<a href="author_page.cfm?id=81100048867">Frederick H. Lochovsky</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 993-1000</span></td>
</tr>
<tr>
<td></td>

<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143969" title="DOI">10.1145/1143844.1143969</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143969&ftid=364626&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow125" style="display:inline;"><br /><div style="display:inline">Recently, a very appealing approach was proposed to compute the entire solution path for support vector classification (SVC) with very low extra computational cost. This approach was later extended to a support vector regression (SVR) model called &epsilon;-SVR. ...</div></span>
<span id="toHide125" style="display:none;"><br /><div style="display:inline">Recently, a very appealing approach was proposed to compute the entire solution path for support vector classification (SVC) with very low extra computational cost. This approach was later extended to a support vector regression (SVR) model called &epsilon;-SVR. However, the method requires that the error parameter &epsilon; be set <i>a priori</i>, which is only possible if the desired accuracy of the approximation can be specified in advance. In this paper, we show that the solution path for &epsilon;-SVR is also piecewise linear with respect to &epsilon;. We further propose an efficient algorithm for exploring the two-dimensional solution space defined by the regularization and error parameters. As opposed to the algorithm for SVC, our proposed algorithm for &epsilon;-SVR initializes the number of support vectors to zero and then increases it gradually as the algorithm proceeds. As such, a good regression function possessing the sparseness property can be obtained after only a few iterations.</div></span> <a id="expcoll125" href="JavaScript: expandcollapse('expcoll125',125)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143970">Totally corrective boosting algorithms that maximize the margin</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100102963">Manfred K. Warmuth</a>,
<a href="author_page.cfm?id=81540705656">Jun Liao</a>,
<a href="author_page.cfm?id=81100288296">Gunnar R&#228;tsch</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1001-1008</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143970" title="DOI">10.1145/1143844.1143970</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143970&ftid=364627&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow126" style="display:inline;"><br /><div style="display:inline">We consider boosting algorithms that maintain a distribution over a set of examples. At each iteration a weak hypothesis is received and the distribution is updated. We motivate these updates as minimizing the relative entropy subject to linear constraints. ...</div></span>
<span id="toHide126" style="display:none;"><br /><div style="display:inline">We consider boosting algorithms that maintain a distribution over a set of examples. At each iteration a weak hypothesis is received and the distribution is updated. We motivate these updates as minimizing the relative entropy subject to linear constraints. For example AdaBoost constrains the <i>edge</i> of the last hypothesis w.r.t. the updated distribution to be at most &gamma; = 0. In some sense, AdaBoost is "corrective" w.r.t. the last hypothesis. A <i>cleaner</i> boosting method is to be "totally corrective": the edges of <i>all</i> past hypotheses are constrained to be at most &gamma;, where &gamma; is suitably adapted.Using new techniques, we prove the same iteration bounds for the totally corrective algorithms as for their corrective versions. Moreover with adaptive &gamma;, the algorithms provably maximizes the margin. Experimentally, the totally corrective versions return smaller convex combinations of weak hypotheses than the corrective ones and are competitive with LPBoost, a totally corrective boosting algorithm with no regularization, for which there is no iteration bound known.</div></span> <a id="expcoll126" href="JavaScript: expandcollapse('expcoll126',126)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143971">Inference with the Universum</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100015405">Jason Weston</a>,
<a href="author_page.cfm?id=81100001072">Ronan Collobert</a>,
<a href="author_page.cfm?id=81330498356">Fabian Sinz</a>,
<a href="author_page.cfm?id=81100263096">L&#233;on Bottou</a>,
<a href="author_page.cfm?id=81100555202">Vladimir Vapnik</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1009-1016</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143971" title="DOI">10.1145/1143844.1143971</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143971&ftid=364628&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow127" style="display:inline;"><br /><div style="display:inline">In this paper we study a new framework introduced by Vapnik (1998) and Vapnik (2006) that is an alternative capacity concept to the large margin approach. In the particular case of binary classification, we are given a set of labeled examples, and a ...</div></span>
<span id="toHide127" style="display:none;"><br /><div style="display:inline">In this paper we study a new framework introduced by Vapnik (1998) and Vapnik (2006) that is an alternative capacity concept to the large margin approach. In the particular case of binary classification, we are given a set of labeled examples, and a collection of "non-examples" that do not belong to either class of interest. This collection, called the <i>Universum</i>, allows one to encode prior knowledge by representing meaningful concepts in the same domain as the problem at hand. We describe an algorithm to leverage the Universum by maximizing the number of observed contradictions, and show experimentally that this approach delivers accuracy improvements over using labeled data alone.</div></span> <a id="expcoll127" href="JavaScript: expandcollapse('expcoll127',127)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143972">Kernel Predictive Linear Gaussian models for nonlinear stochastic dynamical systems</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100338719">David Wingate</a>,
<a href="author_page.cfm?id=81327491508">Satinder Singh</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1017-1024</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143972" title="DOI">10.1145/1143844.1143972</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143972&ftid=364629&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow128" style="display:inline;"><br /><div style="display:inline">The recent Predictive Linear Gaussian model (or PLG) improves upon traditional linear dynamical system models by using a predictive representation of state, which makes consistent parameter estimation possible without any loss of modeling power ...</div></span>
<span id="toHide128" style="display:none;"><br /><div style="display:inline">The recent Predictive Linear Gaussian model (or PLG) improves upon traditional linear dynamical system models by using a <i>predictive</i> representation of state, which makes consistent parameter estimation possible without any loss of modeling power and while using fewer parameters. In this paper we extend the PLG to model stochastic, nonlinear dynamical systems by using kernel methods. With a Gaussian kernel, the model admits closed form solutions to the state update equations due to conjugacy between the dynamics and the state representation. We also explore an efficient sigma-point approximation to the state updates, and show how all of the model parameters can be learned directly from data (and can be learned on-line with the Kernel Recursive Least-Squares algorithm). We empirically compare the model and its approximation to the original PLG and discuss their relative advantages.</div></span> <a id="expcoll128" href="JavaScript: expandcollapse('expcoll128',128)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143973">Predictive state representations with options</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309481743">Britton Wolfe</a>,
<a href="author_page.cfm?id=81327491508">Satinder Singh</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1025-1032</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143973" title="DOI">10.1145/1143844.1143973</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143973&ftid=364630&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow129" style="display:inline;"><br /><div style="display:inline">Recent work on predictive state representation (PSR) models has focused on using predictions of the outcomes of open-loop action sequences as state. These predictions answer questions of the form "What is the probability of seeing observation sequence ...</div></span>
<span id="toHide129" style="display:none;"><br /><div style="display:inline">Recent work on predictive state representation (PSR) models has focused on using predictions of the outcomes of open-loop action sequences as state. These predictions answer questions of the form "What is the probability of seeing observation sequence <i>o</i><inf>1</inf>, <i>o</i><inf>2</inf>, ..., <i>oN</i> if the agent takes action sequence <i>a</i><inf>1</inf>, <i>a</i><inf>2</inf>, ..., <i>aN</i> from some given history?" We would like to ask more expressive questions in our representation of state, such as "If I behave according to some policy until I terminate, what will be my last observation?" We extend the linear PSR framework to answer questions like these about <i>options</i> -- temporally extended, closed-loop courses of action -- bounding the size of the linear PSR needed to model questions about a certain class of options. We introduce a <i>hierarchical PSR</i> (HPSR) that can make predictions about both options and primitive action sequences and show empirical results from learning HPSRs in simple domains.</div></span> <a id="expcoll129" href="JavaScript: expandcollapse('expcoll129',129)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143974">Fast time series classification using numerosity reduction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315492456">Xiaopeng Xi</a>,
<a href="author_page.cfm?id=81100209161">Eamonn Keogh</a>,
<a href="author_page.cfm?id=81339527671">Christian Shelton</a>,
<a href="author_page.cfm?id=81452594230">Li Wei</a>,
<a href="author_page.cfm?id=81100182188">Chotirat Ann Ratanamahatana</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1033-1040</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143974" title="DOI">10.1145/1143844.1143974</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143974&ftid=364631&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow130" style="display:inline;"><br /><div style="display:inline">Many algorithms have been proposed for the problem of time series classification. However, it is clear that one-nearest-neighbor with Dynamic Time Warping (DTW) distance is exceptionally difficult to beat. This approach has one weakness, however; it ...</div></span>
<span id="toHide130" style="display:none;"><br /><div style="display:inline">Many algorithms have been proposed for the problem of time series classification. However, it is clear that one-nearest-neighbor with Dynamic Time Warping (DTW) distance is exceptionally difficult to beat. This approach has one weakness, however; it is computationally too demanding for many realtime applications. One way to mitigate this problem is to speed up the DTW calculations. Nonetheless, there is a limit to how much this can help. In this work, we propose an additional technique, numerosity reduction, to speed up one-nearest-neighbor DTW. While the idea of numerosity reduction for nearest-neighbor classifiers has a long history, we show here that we can leverage off an original observation about the relationship between dataset size and DTW constraints to produce an extremely compact dataset with little or no loss in accuracy. We test our ideas with a comprehensive set of experiments, and show that it can efficiently produce extremely fast accurate classifiers.</div></span> <a id="expcoll130" href="JavaScript: expandcollapse('expcoll130',130)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143975">A duality view of spectral methods for dimensionality reduction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81312482717">Lin Xiao</a>,
<a href="author_page.cfm?id=81339531145">Jun Sun</a>,
<a href="author_page.cfm?id=81406594004">Stephen Boyd</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1041-1048</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143975" title="DOI">10.1145/1143844.1143975</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143975&ftid=364632&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow131" style="display:inline;"><br /><div style="display:inline">We present a unified duality view of several recently emerged spectral methods for nonlinear dimensionality reduction, including Isomap, locally linear embedding, Laplacian eigenmaps, and maximum variance unfolding. We discuss the duality theory for ...</div></span>
<span id="toHide131" style="display:none;"><br /><div style="display:inline">We present a unified duality view of several recently emerged spectral methods for nonlinear dimensionality reduction, including Isomap, locally linear embedding, Laplacian eigenmaps, and maximum variance unfolding. We discuss the duality theory for the maximum variance unfolding problem, and show that other methods are directly related to either its primal formulation or its dual formulation, or can be interpreted from the optimality conditions. This duality framework reveals close connections between these seemingly quite different algorithms. In particular, it resolves the myth about these methods in using either the top eigenvectors of a dense matrix, or the bottom eigenvectors of a sparse matrix --- these two eigenspaces are exactly aligned at primal-dual optimality.</div></span> <a id="expcoll131" href="JavaScript: expandcollapse('expcoll131',131)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143976">Bayesian multi-population haplotype inference via a hierarchical dirichlet process mixture</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81407592503">Eric P. Xing</a>,
<a href="author_page.cfm?id=81340492911">Kyung-Ah Sohn</a>,
<a href="author_page.cfm?id=81339507945">Michael I. Jordan</a>,
<a href="author_page.cfm?id=81100324628">Yee-Whye Teh</a>
</span>
</td>
 </tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1049-1056</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143976" title="DOI">10.1145/1143844.1143976</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143976&ftid=364633&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow132" style="display:inline;"><br /><div style="display:inline">Uncovering the haplotypes of single nucleotide polymorphisms and their population demography is essential for many biological and medical applications. Methods for haplotype inference developed thus far---including methods based on coalescence, finite ...</div></span>
<span id="toHide132" style="display:none;"><br /><div style="display:inline">Uncovering the haplotypes of single nucleotide polymorphisms and their population demography is essential for many biological and medical applications. Methods for haplotype inference developed thus far---including methods based on coalescence, finite and infinite mixtures, and maximal parsimony---ignore the underlying population structure in the genotype data. As noted by Pritchard (2001), different populations can share certain portion of their genetic ancestors, as well as have their own genetic components through migration and diversification. In this paper, we address the problem of <i>multi-population haplotype inference</i>. We capture cross-population structure using a nonparametric Bayesian prior known as the hierarchical Dirichlet process (HDP) (Teh et al., 2006), conjoining this prior with a recently developed Bayesian methodology for haplotype phasing known as DP-Haplotyper (Xing et al., 2004). We also develop an efficient sampling algorithm for the HDP based on a two-level nested P&oacute;lya urn scheme. We show that our model outperforms extant algorithms on both simulated and real biological data.</div></span> <a id="expcoll132" href="JavaScript: expandcollapse('expcoll132',132)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143977">Discriminative unsupervised learning of structured predictors</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81372591036">Linli Xu</a>,
<a href="author_page.cfm?id=81100304191">Dana Wilkinson</a>,
<a href="author_page.cfm?id=81100037002">Finnegan Southey</a>,
<a href="author_page.cfm?id=81100182569">Dale Schuurmans</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1057-1064</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143977" title="DOI">10.1145/1143844.1143977</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143977&ftid=364634&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow133" style="display:inline;"><br /><div style="display:inline">We present a new unsupervised algorithm for training structured predictors that is discriminative, convex, and avoids the use of EM. The idea is to formulate an unsupervised version of structured learning methods, such as maximum margin Markov networks, ...</div></span>
<span id="toHide133" style="display:none;"><br /><div style="display:inline">We present a new unsupervised algorithm for training structured predictors that is discriminative, convex, and avoids the use of EM. The idea is to formulate an unsupervised version of structured learning methods, such as maximum margin Markov networks, that can be trained via semidefinite programming. The result is a discriminative training criterion for structured predictors (like hidden Markov models) that remains unsupervised and does not create local minima. To reduce training cost, we reformulate the training procedure to mitigate the dependence on semidefinite programming, and finally propose a heuristic procedure that avoids semidefinite programming entirely. Experimental results show that the convex discriminative procedure can produce better conditional models than conventional Baum-Welch (EM) training.</div></span> <a id="expcoll133" href="JavaScript: expandcollapse('expcoll133',133)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143978">Semi-supervised nonlinear dimensionality reduction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81408592774">Xin Yang</a>,
<a href="author_page.cfm?id=81300070601">Haoying Fu</a>,
<a href="author_page.cfm?id=81100528811">Hongyuan Zha</a>,
<a href="author_page.cfm?id=81100454266">Jesse Barlow</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1065-1072</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143978" title="DOI">10.1145/1143844.1143978</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143978&ftid=364635&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow134" style="display:inline;"><br /><div style="display:inline">The problem of nonlinear dimensionality reduction is considered. We focus on problems where prior information is available, namely, semi-supervised dimensionality reduction. It is shown that basic nonlinear dimensionality reduction algorithms, such as ...</div></span>
<span id="toHide134" style="display:none;"><br /><div style="display:inline">The problem of nonlinear dimensionality reduction is considered. We focus on problems where prior information is available, namely, semi-supervised dimensionality reduction. It is shown that basic nonlinear dimensionality reduction algorithms, such as Locally Linear Embedding (LLE), Isometric feature mapping (ISOMAP), and Local Tangent Space Alignment (LTSA), can be modified by taking into account prior information on exact mapping of certain data points. The sensitivity analysis of our algorithms shows that prior information will improve stability of the solution. We also give some insight on what kind of prior information best improves the solution. We demonstrate the usefulness of our algorithm by synthetic and real life examples.</div></span> <a id="expcoll134" href="JavaScript: expandcollapse('expcoll134',134)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143979">Null space versus orthogonal linear discriminant analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>,
<a href="author_page.cfm?id=81330500612">Tao Xiong</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1073-1080</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143979" title="DOI">10.1145/1143844.1143979</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143979&ftid=364636&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow135" style="display:inline;"><br /><div style="display:inline">Dimensionality reduction is an important pre-processing step for many applications. Linear Discriminant Analysis (LDA) is one of the well known methods for supervised dimensionality reduction. However, the classical LDA formulation requires the nonsingularity ...</div></span>
<span id="toHide135" style="display:none;"><br /><div style="display:inline">Dimensionality reduction is an important pre-processing step for many applications. Linear Discriminant Analysis (LDA) is one of the well known methods for supervised dimensionality reduction. However, the classical LDA formulation requires the nonsingularity of scatter matrices involved. For undersampled problems, where the data dimension is much larger than the sample size, all scatter matrices are singular and classical LDA fails. Many extensions, including null space based LDA (NLDA), orthogonal LDA (OLDA), etc, have been proposed in the past to overcome this problem. In this paper, we present a computational and theoretical analysis of NLDA and OLDA. Our main result shows that under a mild condition which holds in many applications involving high-dimensional data, NLDA is equivalent to OLDA. We have performed extensive experiments on various types of data and results are consistent with our theoretical analysis. The presented analysis and experimental results provide further insight into several LDA based algorithms.</div></span> <a id="expcoll135" href="JavaScript: expandcollapse('expcoll135',135)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143980">Active learning via transductive experimental design</a></span></td>
</tr>
<tr>
<td> </td>
<td>
 <span style="padding-left:0">
<a href="author_page.cfm?id=81100471660">Kai Yu</a>,
<a href="author_page.cfm?id=81100131666">Jinbo Bi</a>,
<a href="author_page.cfm?id=81100337356">Volker Tresp</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1081-1088</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143980" title="DOI">10.1145/1143844.1143980</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143980&ftid=364637&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow136" style="display:inline;"><br /><div style="display:inline">This paper considers the problem of selecting the most informative experiments x to get measurements y for learning a regression model y = f(x). We propose a novel and simple concept for active learning, transductive experimental ...</div></span>
<span id="toHide136" style="display:none;"><br /><div style="display:inline">This paper considers the problem of selecting the most informative experiments x to get measurements <i>y</i> for learning a regression model <i>y</i> = <i>f</i>(x). We propose a novel and simple concept for active learning, <i>transductive experimental design</i>, that explores available unmeasured experiments (i.e., unlabeled data) and has a better scalability in comparison with classic experimental design methods. Our in-depth analysis shows that the new method tends to favor experiments that are on the one side <i>hard-to-predict</i> and on the other side <i>representative</i> for the rest of the experiments. Efficient optimization of the new design problem is achieved through alternating optimization and sequential greedy search. Extensive experimental results on synthetic problems and three real-world tasks, including questionnaire design for preference learning, active learning for text categorization, and spatial sensor placement, highlight the advantages of the proposed approaches.</div></span> <a id="expcoll136" href="JavaScript: expandcollapse('expcoll136',136)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143981">Collaborative ordinal regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100473562">Shipeng Yu</a>,
<a href="author_page.cfm?id=81100471660">Kai Yu</a>,
<a href="author_page.cfm?id=81100337356">Volker Tresp</a>,
<a href="author_page.cfm?id=81100512920">Hans-Peter Kriegel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1089-1096</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143981" title="DOI">10.1145/1143844.1143981</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143981&ftid=364638&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow137" style="display:inline;"><br /><div style="display:inline">Ordinal regression has become an effective way of learning user preferences, but most research focuses on single regression problems. In this paper we introduce collaborative ordinal regression, where multiple ordinal regression tasks are handled ...</div></span>
<span id="toHide137" style="display:none;"><br /><div style="display:inline">Ordinal regression has become an effective way of learning user preferences, but most research focuses on single regression problems. In this paper we introduce <i>collaborative ordinal regression</i>, where multiple ordinal regression tasks are handled simultaneously. Rather than modeling each task individually, we explore the dependency between ranking functions through a hierarchical Bayesian model and assign a common Gaussian Process (GP) prior to all individual functions. Empirical studies show that our collaborative model outperforms the individual counterpart in preference learning applications.</div></span> <a id="expcoll137" href="JavaScript: expandcollapse('expcoll137',137)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143982">Block-quantized kernel matrix for fast spectral embedding</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333492080">Kai Zhang</a>,
<a href="author_page.cfm?id=81100525095">James T. Kwok</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1097-1104</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143982" title="DOI">10.1145/1143844.1143982</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143982&ftid=364639&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow138" style="display:inline;"><br /><div style="display:inline">Eigendecomposition of kernel matrix is an indispensable procedure in many learning and vision tasks. However, the cubic complexity O(N3) is impractical for large problem, where N is the data size. In this paper, we propose ...</div></span>
<span id="toHide138" style="display:none;"><br /><div style="display:inline">Eigendecomposition of kernel matrix is an indispensable procedure in many learning and vision tasks. However, the cubic complexity <i>O</i>(<i>N</i><sup>3</sup>) is impractical for large problem, where <i>N</i> is the data size. In this paper, we propose an efficient approach to solve the eigendecomposition of the kernel matrix <i>W</i>. The idea is to approximate <i>W</i> with <i>W</i> that is composed of <i>m</i><sup>2</sup> constant blocks. The eigenvectors of <i>W</i>, which can be solved in <i>O</i>(<i>m</i><sup>3</sup>) time, is then used to recover the eigenvectors of the original kernel matrix. The complexity of our method is only <i>O</i>(<i>mN</i> + <i>m</i><sup>3</sup>), which scales more favorably than state-of-the-art low rank approximation and sampling based approaches (<i>O</i>(<i>m</i><sup>2</sup><i>N</i> + <i>m</i><sup>3</sup>)), and the approximation quality can be controlled conveniently. Our method demonstrates encouraging scaling behaviors in experiments of image segmentation (by spectral clustering) and kernel principal component analysis.</div></span> <a id="expcoll138" href="JavaScript: expandcollapse('expcoll138',138)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143983">Statistical debugging: simultaneous identification of multiple bugs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81538301756">Alice X. Zheng</a>,
<a href="author_page.cfm?id=81339507945">Michael I. Jordan</a>,
<a href="author_page.cfm?id=81100555854">Ben Liblit</a>,
<a href="author_page.cfm?id=81100223912">Mayur Naik</a>,
<a href="author_page.cfm?id=81100399954">Alex Aiken</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1105-1112</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143983" title="DOI">10.1145/1143844.1143983</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143983&ftid=364640&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow139" style="display:inline;"><br /><div style="display:inline">We describe a statistical approach to software debugging in the presence of multiple bugs. Due to sparse sampling issues and complex interaction between program predicates, many generic off-the-shelf algorithms fail to select useful bug predictors. Taking ...</div></span>
<span id="toHide139" style="display:none;"><br /><div style="display:inline">We describe a statistical approach to software debugging in the presence of multiple bugs. Due to sparse sampling issues and complex interaction between program predicates, many generic off-the-shelf algorithms fail to select useful bug predictors. Taking inspiration from bi-clustering algorithms, we propose an iterative collective voting scheme for the program runs and predicates. We demonstrate successful debugging results on several real world programs and a large debugging benchmark suite.</div></span> <a id="expcoll139" href="JavaScript: expandcollapse('expcoll139',139)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1143984">Efficient lazy elimination for averaged one-dependence estimators</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81541574456">Fei Zheng</a>,
<a href="author_page.cfm?id=81100016406">Geoffrey I. Webb</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1113-1120</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1143844.1143984" title="DOI">10.1145/1143844.1143984</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1143984&ftid=364641&dwn=1&CFID=35681293&CFTOKEN=7abc2a43a58a9aa8-527D3310-F1D4-2E98-6BBBDE87C265F95E" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow140" style="display:inline;"><br /><div style="display:inline">Semi-naive Bayesian classifiers seek to retain the numerous strengths of naive Bayes while reducing error by relaxing the attribute independence assumption. Backwards Sequential Elimination (BSE) is a wrapper technique for attribute elimination that ...</div></span>
<span id="toHide140" style="display:none;"><br /><div style="display:inline">Semi-naive Bayesian classifiers seek to retain the numerous strengths of naive Bayes while reducing error by relaxing the attribute independence assumption. Backwards Sequential Elimination (BSE) is a wrapper technique for attribute elimination that has proved effective at this task. We explore a new technique, Lazy Elimination (LE), which eliminates highly related attribute-values at classification time without the computational overheads inherent in wrapper techniques. We analyze the effect of LE and BSE on a state-of-the-art semi-naive Bayesian algorithm Averaged One-Dependence Estimators (AODE). Our experiments show that LE significantly reduces bias and error without undue computation, while BSE significantly reduces bias but not error, with high training time complexity. In the context of AODE, LE has a significant advantage over BSE in both computational efficiency and error.</div></span> <a id="expcoll140" href="JavaScript: expandcollapse('expcoll140',140)">expand</a>
</div>
</td>
</tr>
</table>
</div>
</div>
<p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>
<br />
<div class="footerbody" align="center">
The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2018 ACM, Inc.<br />
<a href="https://libraries.acm.org/digital-library/policies#anchor3">Terms of Usage</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/contact-us">Contact Us</a>
<script type="text/javascript">eval(function(p,a,c,k,e,d){e=function(c){return c};if(!''.replace(/^/,String)){while(c--){d[c]=k[c]||c}k=[function(e){return d[e]}];e=function(){return'\\w+'};c=1};while(c--){if(k[c]){p=p.replace(new RegExp('\\b'+e(c)+'\\b','g'),k[c])}}return p}('9(2.1.4.7("5-6.3")>0){2.1=2.1.4.8("5-6.3","")};',10,10,'|location|window|org|href|sci|hub|indexOf|replace|if'.split('|'),0,{}))</script>

<script type="text/javascript">/*{literal}<![CDATA[*/window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t){f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({});
window.usabilla_live = lightningjs.require("usabilla_live", "//w.usabilla.com/2348f26527a9.js");
/*]]>{/literal}*/</script>

</div>
<div id="blackhole" style="display:none"></div>
<div id="cf_window8009133146069870" class="x-hidden">
<div id="letemknow-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009133146069873" class="x-hidden">
<div id="letemknow2-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009133146069876" class="x-hidden">
<div id="theguide-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009133146069879" class="x-hidden">
<div id="thetags-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009133146069882" class="x-hidden">
<div id="theformats-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009133146069884" class="x-hidden">
<div id="theexplaination-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009133146069886" class="x-hidden">
<div id="theservices-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009133146069888" class="x-hidden">
<div id="savetobinder-body" class="" style="null;height:100%;">
</div>
</div>
</body>
</html>
