
<!doctype html>
<head><script type="text/javascript">/* <![CDATA[ */_cf_loadingtexthtml="<img alt=' ' src='/cf_scripts/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/cf_scripts/scripts/ajax";
_cf_jsonprefix='//';
_cf_websocket_port=0;
_cf_flash_policy_port=0;
_cf_clientid='9AA3116A90B2C8364D594A5FFACA344C';/* ]]> */</script><script type="text/javascript" src="/cf_scripts/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/cfform.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/masks.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/ckeditor/ckeditor.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/chart/cfchart-server.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/ext/ext-all.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/cf_scripts/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/cf_scripts/scripts/ajax/resources/cf/cf.css" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />
<title>Proceedings of the 24th international conference on Machine learning</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em;}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	
	.mono-text {font-size: 14px; font-family: Consolas, Menlo, Monaco, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace, serif;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}
		
	.small-link-text2 {font-size: .83em !important; 
	}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
		
.x-tabs-strip-wrap {
	overflow-y: hidden !important;
}
  --></style>
<script src="js/tagcanvas.min.js" type="text/javascript"></script>
<script type="text/javascript">
  function loadCloud() {
	try {
	  TagCanvas.Start('myCanvas','tags',{
		textColour: '#000000',
		outlineColour: '#ff00ff',
		reverse: true,
		shuffleTags:true,
		depth: 0.8,
		maxSpeed: 0.05,
		textHeight: 12,
		initial: [0.000, 0.050],
		shape: "hring",
		lock: "x"
	  });
	} catch(e) {
	  // something went wrong, hide the canvas container
	  // document.getElementById('myCanvasContainer').style.display = 'none';
	}
  };
</script>
<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>
<script type='text/javascript' src='https://www.google.com/jsapi'></script>
<style type="text/css"><!--
.google-visualization-orgchart-node {
    background-color: #FFFFFF !important;
    border: 2px solid #AFCF40 !important;
    cursor: default;
    font-family: arial,helvetica;
    text-align: center;
    vertical-align: middle;
}

iframe {float:right; 
		margin:10px;
		border: 2px solid #1B4D0E;
		
		}

a.boxed:link {text-decoration: none !important; 	Color: #000000 !important;}
a.boxed:visited  { color: #000000 !important; text-decoration: none !important;}
a.boxed:hover {color: red !important; text-decoration: underline !important;}

a.boxedh:link {text-decoration: none !important; 	Color: #000000 !important;}
a.boxedh:visited  { color: #000000 !important; text-decoration: none !important;}
a.boxedh:hover {color: red !important; text-decoration: underline !important;}		

a.boxedm:link {text-decoration: none !important; 	Color: #606060 !important;}
a.boxedm:visited  { color: #606060 !important; text-decoration: none !important;}
a.boxedm:hover {color: red !important; text-decoration: underline !important;}		

a.boxedl:link {text-decoration: none !important; 	Color: #808080   !important;}
a.boxedl:visited  { color: 	#808080 !important; text-decoration: none !important;}
a.boxedl:hover {color: red !important; text-decoration: underline !important;}				

.google-visualization-orgchart-linebottom {
    border-bottom: 1px solid #006699 !important;
}
.google-visualization-orgchart-lineleft {
    border-left: 1px solid #006699 !important;
}

.google-visualization-orgchart-lineright {
    border-right: 1px solid #006699 !important;
}

--></style>
<script type="text/javascript">


function settab() {
    var mytabs = ColdFusion.Layout.getTabLayout('citationdetails');
   
 
  mytabs.on('tabchange', function(tabpanel,activetab) { document.cookie = 'picked=' + '1273496' + ',' + activetab.id; })
 
}


function letemknow(){
  ColdFusion.Window.show('letemknow');
}

function letemknow2(){
  ColdFusion.Window.show('letemknow2');
}





function testthis(){

alert('test');
}
function loadalert(){ 
 alert("I am in the load alert");
 
}
function loadalert2(){ 
  alert("I am in the load alert2");
 
}
</script>
<script type='text/javascript'>
	  	google.load('visualization', '1', {packages:['orgchart']});
	    google.setOnLoadCallback(drawChart);
	  
	  function drawChart() {
	    var data = new google.visualization.DataTable();
        data.addColumn('string', 'Name');
        data.addColumn('string', 'Manager');
        data.addColumn('string', 'ToolTip');
  	  
		data.addRows([
          [{v:'0', f:'<div style="color:black; font-size:150%; font-style:normal">CCS&nbsp;for&nbsp;this&nbsp;Proceeding</div>'}, '', ''],
		  
        ]);
		
		if (document.getElementById('chart_div')) {
       var chart = new google.visualization.OrgChart(document.getElementById('chart_div'));
       chart.draw(data, {allowHtml:true});
		}
      }
	  
</script>
<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  
</script>
<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>
<script type="text/javascript">
function expandWatson(divID,theConcept) { 
			if (document.getElementById(divID).style.display == "none") {
				document.getElementById(divID).style.display = "block";
				document.getElementById(divID).innerHTML = theConcept + "<br />" + document.getElementById(divID).innerHTML;
			}
			 else {
				 document.getElementById(divID).style.display = "none"
			}
		}
</script>

<script type="text/javascript">
  function togglePatMap() {
        var div = document.getElementById('patmap'); 
        if (div.style.display == "none") {
            div.style.display = "block";
            document.getElementById("expandcollapsepmapa").src = "images/collapse.png";
			
			if (div.innerHTML.length == 0){
				httpGetAsyncwID("patent.cfm?id=1273496",'patmap');
				httpGetAsyncwID("simmap_track.cfm?id=1273496&how=live",'blackhole');
			}
			else {
				httpGetAsyncwID("simmap_track.cfm?id=1273496&how=cache",'blackhole');
			}
			
        }
        else {
            div.style.display = "none";
            document.getElementById("expandcollapsepmapa").src = "images/expand.png";
        }
    }
  
  function toggleCO() {
        var div = document.getElementById('codisp'); 
        if (div.style.display == "none") {
            div.style.display = "block";
            document.getElementById("expandcollapsecoa").src = "images/collapse.png";
			
			if (div.innerHTML.length == 0){
				/* httpGetAsyncwID("coint.cfm?id=1273496",'codisp'); */
				/*httpGetAsyncwID("simmap_track.cfm?id=1273496&how=live",'blackhole'); */
			}
			else {
				/* httpGetAsyncwID("simmap_track.cfm?id=1273496&how=cache",'blackhole'); */
			}
			
        }
        else {
            div.style.display = "none";
            document.getElementById("expandcollapsecoa").src = "images/expand.png";
        }
    }
	
	function httpGetAsyncwID(theUrl,theID) {
		var xmlHttp = new XMLHttpRequest();
		xmlHttp.onreadystatechange = function() {
			if (xmlHttp.readyState == 4 && xmlHttp.status == 200)
				
				document.getElementById(theID).innerHTML=xmlHttp.responseText;
		}
		xmlHttp.open("GET", theUrl, true); // true for asynchronous 
		xmlHttp.send();
	}
</script>
<meta name="citation_publisher" content="ACM"> <meta name="citation_authors" content="Ghahramani, Zoubin"> <meta name="citation_conference_title" content="Proceedings of the 24th international conference on Machine learning"> <meta name="citation_date" content="06/20/2007"> <meta name="citation_isbn" content="978-1-59593-793-3"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=1273496">
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFFORM');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFDIV');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFTEXTAREA');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Event.registerOnLoad(drawChart,null,false,true);
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFWINDOW');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009140783999500=function()
	{
		_cf_bind_init_8009140783999501=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'letemknow-body','bindExpr':['letemknow.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009140783999501);var _cf_window=ColdFusion.Window.create('letemknow','<div style=\'text-align:left; color:black;\'>Did you know the ACM DL App is now available?</div>','letemknow.cfm',{ modal:false, closable:true, divid:'cf_window8009140783999499', draggable:true, resizable:true, fixedcenter:false, width:600, height:275, shadow:true, callfromtag:true, minwidth:600, minheight:275, initshow:false, destroyonclose:false, x:75, y:125});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009140783999500);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009140783999503=function()
	{
		_cf_bind_init_8009140783999504=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'letemknow2-body','bindExpr':['letemknow_recomm.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009140783999504);var _cf_window=ColdFusion.Window.create('letemknow2','<div style=\'text-align:left; color:black;\'>Did you know your Organization can subscribe to the ACM Digital Library?</div>','letemknow_recomm.cfm',{ modal:false, closable:true, divid:'cf_window8009140783999502', draggable:true, resizable:true, fixedcenter:false, width:600, height:275, shadow:true, callfromtag:true, minwidth:600, minheight:275, initshow:false, destroyonclose:false, x:70, y:175});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009140783999503);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009140783999506=function()
	{
		_cf_bind_init_8009140783999507=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide-body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009140783999507);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window8009140783999505', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009140783999506);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009140783999509=function()
	{
		_cf_bind_init_8009140783999510=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags-body','bindExpr':['showthetags.cfm?id=1273496']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009140783999510);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=1273496',{ modal:false, closable:true, divid:'cf_window8009140783999508', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009140783999509);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009140783999512=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window8009140783999511', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009140783999512);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009140783999514=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window8009140783999513', draggable:true, resizable:true, fixedcenter:false, width:600, height:600, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009140783999514);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009140783999516=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window8009140783999515', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009140783999516);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009140783999518=function()
	{
		_cf_bind_init_8009140783999519=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder-body','bindExpr':['savetobinder.cfm?id=1273496']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009140783999519);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=1273496',{ modal:false, closable:true, divid:'cf_window8009140783999517', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009140783999518);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Event.registerOnLoad(settab,null,false,true);
/* ]]> */</script>
</head>

<body style="text-align:center; font-size:100%"> 
<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'https://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
<script src='AC_RunActiveContent.js' type="text/javascript"></script>
<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>

<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-NFGCMX"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NFGCMX');</script>

<div id="header">
<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
<tr style="vertical-align:top">
<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text2"><img src="/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
</td>
<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; vertical-align:middle;" class="small-link-text2">
<table style="width:100%; border-collapse:collapse; padding:0px">
<tr><td style="text-align:center">
<div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
</td>
</tr>
</table>
</td>
<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text2">
<p style="margin-top:0px; margin-bottom:10px;">
<a href="https://dl.acm.org/signin.cfm" class="small-link-text2" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
&nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm" class="small-link-text2" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
</p>
<table style="padding: 5px; border-collapse:collapse; float:right">
<tr>
<td class="small-link-text2" style="text-align:right">
<script type="text/javascript">
								function encodeInput(form){
								    	var cleanQuery = form.elements['query'].value.replace(new RegExp( "\\+", "g" ),"%2B");
										cleanQuery = cleanQuery.replace(/#/g, "%23");
										cleanQuery = cleanQuery.replace(/(\n)/g, " ");
										cleanQuery = cleanQuery.trim();
										
										
										var ascii = /^[ -~]+$/;
										if( !ascii.test( cleanQuery ) ) {
											var fixedUseQuery = "";
											for (var i = 0, len = cleanQuery.length; i < len; i++) {
												var str = "";
												if( !ascii.test(cleanQuery[i]) ) {
										 			str = "%26%23" + cleanQuery[i].charCodeAt(0) + ";";
												} else {
										 			str = cleanQuery[i];
												}
												fixedUseQuery = fixedUseQuery + str;
											}
											cleanQuery = fixedUseQuery;
										}
										

										form.elements['query'].value = cleanQuery;
								}
							</script>
<form name="qiksearch" action="/results.cfm" onsubmit='encodeInput(this)'>
<span style="margin-left:0px"><label><input type="text" name="query" size="30" value="" /></label>&nbsp;
<input style="vertical-align:top;" type="image" alt="Search" name="Go" src="/images/search_small.jpg" />
</span>
</form>
</td>
</tr>
</table>
</td>
</tr>
<tr><td colspan="3" class="small-link-text2" style="padding-bottom:5px; padding-top:0px; text-align:center">
<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
</td>
</tr>
</table>
<map name="port" id="port">
<area shape="rect" coords="1,1,60,50" href="https://www.acm.org/" alt="ACM Home Page" />
<area shape="rect" coords="65,1,275,68" href="https://dl.acm.org/dl.cfm" alt="ACM Digital Library Home Page" />
</map>
</div>
<style>
  .watsonCont {
	  width:170px;
	  
	  color: #000000;
    font-family: Arial,Helvetica,sans-serif;
    font-size: 1em;
	margin-top: 10px;
	margin-bottom: 10px;
	 /* background-color: lightgray;*/
  }
  #watsonInside {
    border-radius: 25px;
    border: 2px solid #649134;
	margin-top: 12px;
	padding: 5px;
	height: 80px;
  }

</style>
<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
<tr style="vertical-align:top">
<td style="padding-right:10px; text-align:left" class="small-link-text">
<div id="divmain" style="border:1px solid #356b20;">
<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;">Proceedings of the 24th international conference on Machine learning</h1>
</div>
<table class="medium-text" style="border-collapse:collapse; padding:0px; margin-left: 2px;">
<colgroup>
<col style="width:540px" />
</colgroup>
<tr style="vertical-align:top">
<td>
<table style="border-collapse:collapse; padding:2px;" class="medium-text">
<col style="width:80px;" />
<col style="width:auto" />
<tr style="vertical-align:top">
</tr>
</table>
<table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
<col style="width:80px" />
<tr>
<td valign="top" nowrap="nowrap">
Editor:
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81100572858&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">Zoubin Ghahramani</a>
</td>
<td valign="bottom">
<a href="inst_page.cfm?id=60031101" title="Institutional Profile Page"><small>University of Cambridge, United Kingdom</small></a>
</td>
</tr>
</table>
<table style="margin-top: 10px" border="0" class="medium-text" cellpadding="2" cellspacing="0">
<tr>
<td><table border="0" class="medium-text" style="margin-left:5px;" cellpadding="1" cellspacing="0">
<tr valign="top">
<td nowrap="nowrap" style="padding-top:10px;">Publication of:</td>
</tr>
<tr valign="top">
<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Conference</td>
</tr>
<tr valign="top">
<td style="padding-left:10px;">
<a href="http://oregonstate.edu/conferences/icml2007/" title="Conference Website" target="_self" class="link-text">ICML '07 & ILP '07</a> The 24th Annual International Conference on Machine Learning held in conjunction with the 2007 International Conference on Inductive Logic Programming
</td>
</tr>
<tr valign="top">
<td style="padding-left:10px; padding-bottom:10px"> Corvallis, OR, USA &mdash; June 20 - 24, 2007
<br />
<a href="https://www.acm.org/publications" class="small-link-text" title="ACM">ACM</a> <span class="small-link-text">New York, NY</span><span class="small-link-text">, USA</span> <span class="small-link-text"> &copy;2007</span>
<br />
</td>
</tr>
</table></td>
</tr>
</table>
</td>
<td rowspan="20">
<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
<tr>
<td align="center" style="padding-bottom: 5px;">
</td>
<td align="left" nowrap="nowrap">
<img src="images/ACM_mini.jpg" style="vertical-align:middle" title="Published by ACM" alt="Published by ACM" /> 2007 Proceeding<br />
</td>
</tr>
<tr>
<td colspan="2" valign="baseline" style="padding-bottom:5px; padding-top:5px;">
<img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
<a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
</td>
</tr>
 <tr>
<td class="small-text" colspan="2" valign="top" style="padding-left:30px;">
&middot;&nbsp;Citation Count: 4,995<br />
&middot;&nbsp;Downloads (cumulative): 84,514<br />
&middot;&nbsp;Downloads (12 Months): 13,118<br />
&middot;&nbsp;Downloads (6 Weeks): 5,840<br />
</td>
</tr>

</table>
</td>
</tr>
</table>
<br clear="all" />
<br clear="all" />
</div>
</td>
<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
<div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
<div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;">Tools and Resources</h1></div>
<ul title="Tools and Resources" style="list-style: none; list-style-position:outside;
margin-left: 25px;
padding-left: 0em;
text-indent: 0px;
margin-bottom: 0px;">
<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:0px;">
<span class="small-link-text">TOC Service:</span>
<img src="images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
<ul style="margin-left: 0; padding-left: 0; display:inline;">
<li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
<li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">RSS</a></li>
</ul>
</span>
</li>
<li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:0px;">
<a href="citation.cfm?id=1273496&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
</span></li>
<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:0px; margin-bottom:0px">
<span class="small-link-text">Export Formats:</span>
<ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1273496&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1273496&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1273496&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
</ul>
</span>
</li>
</ul>


<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>


<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google_plusone_share"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_researchgate"></a>
<a class="addthis_button_reddit"></a>
<span class="addthis_separator">|</span>
<a href="https://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="https://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>

</div>
</td>
</tr>
</table>
</div>
<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<div id="fback" style="text-align:left; padding-top:20px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="Contact The DL Team" href="/cdn-cgi/l/email-protection#7606190402171a5b101313121417151d361e075817151b58190411" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="Contact The DL Team" border="0" /></a>
<a title="Contact The DL Team" href="/cdn-cgi/l/email-protection#f5859a87819499d8939090919794969eb59d84db949698db9a8792"><strong>Contact Us</strong></a>
<span style="padding:10px;">|</span>
<span>Switch to <a href="citation.cfm?id=1273496&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>
</span>
<div class="small-text" style="margin-top:10px; margin-bottom:5px;">
<br />
<a href="#abstract" title="Abstract" style="padding:5px"><span>Abstract</span></a> |
<a href="#formats" title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
<a href="#authors" title="Authors" style="padding:5px"><span>Authors</span></a> |
<a href="#references" title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
<a href="#citedby" title="Cited By" style="padding:5px"><span>Cited By</span></a> |
<a href="#indexterms" title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
<a href="#source" title="Publication" style="padding:5px"><span>Publication</span></a> |
<a href="#revs" title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |
<a href="#comments" title="Comments" style="padding:5px"><span>Comments</span></a>
|
<a href="#prox" title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
</div>
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;" />
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div style="display:inline"><p>This volume contains the papers accepted to the 24th International Conference on Machine Learning (ICML 2007), which was held at Oregon State University in Corvalis, Oregon, from June 20th to 24th, 2007. ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at: http://www.machinelearning.org.</p> <p>This year there were 522 submissions to ICML. There was a very thorough review process, in which each paper was reviewed by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. For the first time this year there were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Also for the first time, authors were asked to submit a list of changes with their final accepted papers, which was checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 150 papers were accepted to ICML this year, including a very small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 29%.</p> <p>ICML attracts submissions from machine learning researchers around the globe. The 150 accepted papers this year were geographically distributed as follows: 66 papers had a first author from the US, 32 from Europe, 19 from China or Hong Kong, 11 from Canada, 6 from India, 5 each from Australia and Japan, 3 from Israel, and 1 each from Korea, Russia and Taiwan.</p> <p>In addition to the main program of accepted papers, which includes both a talk and poster presentation for each paper, the ICML program included 3 workshops and 8 tutorials on machine learning topics which are currently of broad interest. We were also extremely pleased to have David Heckerman (Microsoft Research), Joshua Tenenbaum (Massachussetts Institute of Technology), and Bernhard Sch&ouml;lkopf (Max Planck Institute for Biological Cybernetics) as the invited speakers this year. Thanks to sponsorship by the Machine Learning Journal, we were able to award a number of outstanding student paper prizes.</p> <p>We were fortunate this year that ICML was co-located with the International Conference on Inductive Logic Programming (ILP 2007). ICML and ILP held joint sessions on the first day of ICML 2007.</p></div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div class="abstract">
<SPAN style="font-weight:bold">FRONT MATTER</span>
</div>
<div style="margin-left:10px; line-height:180%;">
<A NAME="FullText" HREF="https://portalparts.acm.org/1280000/1273496/fm/frontmatter.pdf?ip=173.16.22.104" title="PDF" target="_blank">
<img src="imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
&nbsp;Front matter (Title page, TOC, Preface, Committees, Miscellaneous material)
</div>
<div style="margin-top: 10px;" class="abstract">
<SPAN style="font-weight:bold">BACK MATTER</span>
</div>
<div style="margin-left:10px; line-height:180%;">
<A NAME="FullText" HREF="https://portalparts.acm.org/1280000/1273496/bm/backmatter.pdf?ip=173.16.22.104" title="PDF" target="_blank">
<img src="imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
&nbsp;Back matter (Author index)
</div>
<div style="margin-top: 10px; height: auto; padding: 5px; ">
<div style="margin-top:20px;" class="abstract">
<SPAN style="font-weight:bold">APPEARS IN</span>
</div>
<div>
<a href="/icps.cfm" title="ICPS"><img src="images/ACM_ICPS.jpg" alt="ICPS" style="padding-right:10px; vertical-align:middle" border="0" /></a> ICPS: <a href="/icps.cfm" title="ICPS" target="_blank">ACM International Conference Proceeding Series</a>
</div>
</div>
<br clear="all" />
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<dl title="Authors" style="margin-top:0px">
<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
<strong>
Editor
</strong>
</dt>
<dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
<span>
<br><br />
<table border="0" cellspacing="10">
 <tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of Zoubin Ghahramani" href="author_page.cfm?id=81100572858">Zoubin Ghahramani</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1993-2017</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">181</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">3,158</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">30</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">186</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">1,089</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">11,967</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">398.90</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">17.45</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of Zoubin Ghahramani" href="author_page.cfm?id=81100572858&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of Zoubin Ghahramani
</td>
</tr>
</table>
</span>
</dd>
</dl>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
References are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<P class="abstract">2 Citations</p>
<table cellpadding="5">
<tr valign="top">
<td valign="top">
&nbsp;
</td>
<td style="padding: 10px;">
<div>
<a href="citation.cfm?id=2283651">
Chang Wan , Rong Pan , Jiefei Li, Bi-weighting domain adaptation for cross-language text classification, Proceedings of the Twenty-Second international joint conference on Artificial Intelligence, July 16-22, 2011, Barcelona, Catalonia, Spain
</a>
</div>
</td>
</tr>
<tr valign="top">
<td valign="top">
&nbsp;
</td>
<td style="padding: 10px;">
<div>
<a href="citation.cfm?id=2946687">
Andreas C. Damianou , Michalis K. Titsias , Neil D. Lawrence, Variational inference for latent variables and uncertain inputs in Gaussian processes, The Journal of Machine Learning Research, v.17 n.1, p.1425-1486, January 2016
</a>
</div>
</td>
</tr>
</table>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 0px;" class="flatbody">
Index Terms are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<table border="0" class="medium-text" cellpadding="5" cellspacing="5">
<tr valign="top">
<td style="padding: 10px;">Title</td>
<td style="padding: 10px;">
<a href="http://oregonstate.edu/conferences/icml2007/" title="Conference Website" target="_self" class="link-text">ICML '07 & ILP '07</a> The 24th Annual International Conference on Machine Learning held in conjunction with the 2007 International Conference on Inductive Logic Programming
</td>
</tr>
<tr><td style="padding: 10px;"></td><td style="padding: 10px;">Corvallis, OR, USA &mdash; June 20 - 24, 2007</td></tr> <tr><td style="padding: 10px;">Pages</td><td style="padding: 10px;">1233</td></tr>
<tr>
<td style="padding: 10px;">Sponsor</td>
<td style="padding: 10px;">
<a name="sponsor"> Machine Learning Journal</a>
</td>
</tr>
<tr><td style="padding: 10px;">Publisher</td><td style="padding: 10px;"><a href="https://www.acm.org/publications">ACM</a> New York, NY, USA</td>
</tr>
<tr><td style="padding: 10px;">ISBN</td><td style="padding: 10px;"> 978-1-59593-793-3</td></tr>
<tr valign="top">
<td style="padding-left: 10px;">Conference</td>
<td valign="top" align="left" style="padding-bottom: 25px; padding-left:10px;">
<strong style="padding-right:10px">ICML</strong><a href="event.cfm?id=RE548" title="International Conference on Machine Learning">International Conference on Machine Learning</a>
</td>
</tr>
<tr><td colspan="2">Paper Acceptance Rate 150 of 522 submissions, 29%</td></tr> <tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 448 of 1,653 submissions, 27%</td></tr>
<tr valign="top">
<td colspan="2" style="padding-left:25px;">
<table>
<tr><td style="padding: 10px;">
<img border="0" class="chart" id="3439026860132198-img" src="/CFFileServlet/_cf_chart/3439026860132198.jpg" usemap="#3439026860132198-map" />
<div id="3439026860132198-tooltip" style="position:fixed;display:none;"></div>
<map id="3439026860132198-map" name="3439026860132198-map">
<area style="cursor:auto" shape="rect" id="3439026860132198-graph-id0-plotset-plot-0-node-0" coords="37,24,57,214" />
<area style="cursor:auto" shape="rect" id="3439026860132198-graph-id0-plotset-plot-0-node-1" coords="95,33,115,214" />
<area style="cursor:auto" shape="rect" id="3439026860132198-graph-id0-plotset-plot-0-node-2" coords="152,12,173,214" />
<area style="cursor:auto" shape="rect" id="3439026860132198-graph-id0-plotset-plot-1-node-0" coords="64,167,85,214" />
<area style="cursor:auto" shape="rect" id="3439026860132198-graph-id0-plotset-plot-1-node-1" coords="122,164,142,214" />
<area style="cursor:auto" shape="rect" id="3439026860132198-graph-id0-plotset-plot-1-node-2" coords="180,161,200,214" />
</map>
<script data-cfasync="false" src="/cdn-cgi/scripts/f2bf09f8/cloudflare-static/email-decode.min.js"></script><script>
if (!CFCHART) {var CFCHART={};};if (!CFCHART.nodes) {CFCHART.nodes={};}
CFCHART.nodes["3439026860132198"]={};
CFCHART.nodes["3439026860132198"]["3439026860132198-graph-id0-plotset-plot-0-node-0"]={text:"548",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["3439026860132198"]["3439026860132198-graph-id0-plotset-plot-0-node-1"]={text:"522",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["3439026860132198"]["3439026860132198-graph-id0-plotset-plot-0-node-2"]={text:"583",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["3439026860132198"]["3439026860132198-graph-id0-plotset-plot-1-node-0"]={text:"140",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["3439026860132198"]["3439026860132198-graph-id0-plotset-plot-1-node-1"]={text:"150",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["3439026860132198"]["3439026860132198-graph-id0-plotset-plot-1-node-2"]={text:"158",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
</script>
</td>
<td style="padding-left:20px;">
<table style="border-width: 1px; border-style: solid; width:100%;  border-spacing: 6px;" class="text12">
<tr bgcolor="#ffffff">
<th style="width:50%">Year</th>
<th align="right" style="width:15%">Submitted</th>
<th align="right" style="width:15%">Accepted</th>
<th align="center">Rate</th>
</tr>
<tr bgcolor="#f0f0f0">
<td style="padding: 10px;">ICML '06</td>
<td align="right">548</td>
<td align="right">140</td>
<td align="center">26%</td>
</tr>
<tr bgcolor="#ffffff">
<td style="padding: 10px;">ICML '07</td>
<td align="right">522</td>
<td align="right">150</td>
<td align="center">29%</td>
</tr>
<tr bgcolor="#f0f0f0">
<td style="padding: 10px;">ICML '08</td>
<td align="right">583</td>
<td align="right">158</td>
<td align="center">27%</td>
</tr>
<tr bgcolor="#ffffff">
<td style="padding: 10px;"><strong>Overall</strong></td>
<td align="right">1,653</td>
<td align="right">448</td>
<td align="center">27%</td>
</tr>
</table>
</td>
</tr>
</table>
</td>
</tr>
</table>
</table>
<br />
<div class="abstract" style="margin-bottom:10px;">
<SPAN><strong>APPEARS IN</strong></span>
</div>
<div>
<a href="/icps.cfm" title="ICPS"><img src="images/ACM_ICPS.jpg" alt="ICPS" style="padding-right:10px; vertical-align:middle" border="0" /></a> ICPS: <a href="/icps.cfm" title="ICPS" target="_blank">ACM International Conference Proceeding Series</a>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<br />Reviews are not available for this item
<div align="left" style="margin-top:30px">
<a title="Computing Reviews" href="ocr_review_main.cfm">
<img src="images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
<ul style="list-style:disc; display:inline-block">
<li>Access <a href="ocr_review_main.cfm" target="_blank">critical reviews</a> of computing literature.</li>
<li><a href="http://www.computingreviews.com/Reviewer/" target="_blank">Become a reviewer</a> for Computing Reviews</li>
</ul>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div>
<div>
<p style="margin-left:5px;">
<strong>Be the first to comment</strong>
To Post a comment please <a href="signin.cfm">sign in or create</a> a free Web account</a>
</p>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;">
<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the 24th international conference on Machine learning</h5>
<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>
<div style="clear:both">
<div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1143844&picked=prox" title="previous: ICML '06"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><a href="citation.cfm?id=1390156&picked=prox" title="Next: ICML '08">next proceeding <img align="absmiddle" hspace="5" border="0" src="img/next.gif" width="19" height="11" alt="next" /></a></div>
</div>
<table class="text12" border="0">
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273497">Quantum clustering algorithms</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100241645">Esma A&#239;meur</a>,
<a href="author_page.cfm?id=81410593851">Gilles Brassard</a>,
<a href="author_page.cfm?id=81100178724">S&#233;bastien Gambs</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1-8</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273497" title="DOI">10.1145/1273496.1273497</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273497&ftid=425349&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow1" style="display:inline;"><br /><div style="display:inline">By the term "quantization", we refer to the process of using quantum mechanics in order to improve a classical algorithm, usually by making it go faster. In this paper, we initiate the idea of quantizing clustering algorithms by using variations on a ...</div></span>
<span id="toHide1" style="display:none;"><br /><div style="display:inline"><p>By the term "quantization", we refer to the process of using quantum mechanics in order to improve a classical algorithm, usually by making it go faster. In this paper, we initiate the idea of quantizing clustering algorithms by using variations on a celebrated quantum algorithm due to Grover. After having introduced this novel approach to unsupervised learning, we illustrate it with a quantized version of three standard algorithms: divisive clustering, <i>k</i>-medians and an algorithm for the construction of a neighbourhood graph. We obtain a significant speedup compared to the classical approach.</p></div></span> <a id="expcoll1" href="JavaScript: expandcollapse('expcoll1',1)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273498">Learning random walks to rank nodes in graphs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317487833">Alekh Agarwal</a>,
<a href="author_page.cfm?id=81100424876">Soumen Chakrabarti</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 9-16</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273498" title="DOI">10.1145/1273496.1273498</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273498&ftid=425350&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow2" style="display:inline;"><br /><div style="display:inline">Ranking nodes in graphs is of much recent interest. Edges, via the graph Laplacian, are used to encourage local smoothness of node scores in SVM-like formulations with generalization guarantees. In contrast, Page-rank variants are based on Markovian ...</div></span>
<span id="toHide2" style="display:none;"><br /><div style="display:inline"><p>Ranking nodes in graphs is of much recent interest. Edges, via the graph Laplacian, are used to encourage local smoothness of node scores in SVM-like formulations with generalization guarantees. In contrast, Page-rank variants are based on Markovian random walks. For directed graphs, there is no simple known correspondence between these views of scoring/ranking. Recent scalable algorithms for learning the Pagerank transition probabilities do not have generalization guarantees. In this paper we show some correspondence results between the Laplacian and the Pagerank approaches, and give new generalization guarantees for the latter. We enhance the Pagerank-learning approaches to use an additive margin. We also propose a general framework for rank-sensitive score-learning, and apply it to Laplacian smoothing. Experimental results are promising.</p></div></span> <a id="expcoll2" href="JavaScript: expandcollapse('expcoll2',2)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273499">Uncovering shared structures in multiclass classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333487504">Yonatan Amit</a>,
<a href="author_page.cfm?id=81336489159">Michael Fink</a>,
<a href="author_page.cfm?id=81100005542">Nathan Srebro</a>,
<a href="author_page.cfm?id=81331505935">Shimon Ullman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 17-24</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273499" title="DOI">10.1145/1273496.1273499</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273499&ftid=425351&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow3" style="display:inline;"><br /><div style="display:inline">This paper suggests a method for multiclass learning with many classes by simultaneously learning shared characteristics common to the classes, and predictors for the classes in terms of these characteristics. We cast this as a convex optimization problem, ...</div></span>
<span id="toHide3" style="display:none;"><br /><div style="display:inline"><p>This paper suggests a method for multiclass learning with many classes by simultaneously learning shared characteristics common to the classes, and predictors for the classes in terms of these characteristics. We cast this as a convex optimization problem, using <i>trace-norm</i> regularization and study gradient-based optimization both for the linear case and the kernelized setting.</p></div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273500">Two-view feature generation model for semi-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100486025">Rie Kubota Ando</a>,
<a href="author_page.cfm?id=81100114170">Tong Zhang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 25-32</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273500" title="DOI">10.1145/1273496.1273500</a></span></td>
</tr>
 <tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273500&ftid=425352&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow4" style="display:inline;"><br /><div style="display:inline">We consider a setting for discriminative semi-supervised learning where unlabeled data are used with a generative model to learn effective feature representations for discriminative training. Within this framework, we revisit the two-view feature generation ...</div></span>
<span id="toHide4" style="display:none;"><br /><div style="display:inline"><p>We consider a setting for discriminative semi-supervised learning where unlabeled data are used with a generative model to learn effective feature representations for discriminative training. Within this framework, we revisit the two-view feature generation model of co-training and prove that the optimum predictor can be expressed as a linear combination of a few features constructed from unlabeled data. From this analysis, we derive methods that employ two views but are very different from co-training. Experiments show that our approach is more robust than co-training and EM, under various data generation conditions.</p></div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273501">Scalable training of <i>L</i><sup>1</sup>-regularized log-linear models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333487476">Galen Andrew</a>,
<a href="author_page.cfm?id=81537909456">Jianfeng Gao</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 33-40</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273501" title="DOI">10.1145/1273496.1273501</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273501&ftid=425353&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow5" style="display:inline;"><br /><div style="display:inline">The L-BFGS limited-memory quasi-Newton method is the algorithm of choice for optimizing the parameters of large-scale log-linear models with L2 regularization, but it cannot be used for an L1-regularized loss due to ...</div></span>
<span id="toHide5" style="display:none;"><br /><div style="display:inline"><p>The L-BFGS limited-memory quasi-Newton method is the algorithm of choice for optimizing the parameters of large-scale log-linear models with <i>L</i><inf>2</inf> regularization, but it cannot be used for an <i>L</i><inf>1</inf>-regularized loss due to its non-differentiability whenever some parameter is zero. Efficient algorithms have been proposed for this task, but they are impractical when the number of parameters is very large. We present an algorithm Orthant-Wise Limited-memory Quasi-Newton (OWL-QN), based on L-BFGS, that can efficiently optimize the <i>L</i><inf>1</inf>-regularized log-likelihood of log-linear models with millions of parameters. In our experiments on a parse reranking task, our algorithm was several orders of magnitude faster than an alternative algorithm, and substantially faster than L-BFGS on the analogous <i>L</i><inf>2</inf>-regularized problem. We also present a proof that OWL-QN is guaranteed to converge to a globally optimal parameter vector.</p></div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273502">Multiclass core vector machine</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323487810">S. Asharaf</a>,
<a href="author_page.cfm?id=81100199654">M. Narasimha Murty</a>,
<a href="author_page.cfm?id=81310499556">S. K. Shevade</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 41-48</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273502" title="DOI">10.1145/1273496.1273502</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273502&ftid=425354&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow6" style="display:inline;"><br /><div style="display:inline">Even though several techniques have been proposed in the literature for achieving multiclass classification using Support Vector Machine(SVM), the scalability aspect of these approaches to handle large data sets still needs much of exploration. Core ...</div></span>
<span id="toHide6" style="display:none;"><br /><div style="display:inline"><p>Even though several techniques have been proposed in the literature for achieving multiclass classification using Support Vector Machine(SVM), the scalability aspect of these approaches to handle large data sets still needs much of exploration. Core Vector Machine(CVM) is a technique for scaling up a two class SVM to handle large data sets. In this paper we propose a Multiclass Core Vector Machine(MCVM). Here we formulate the multiclass SVM problem as a Quadratic Programming(QP) problem defining an SVM with vector valued output. This QP problem is then solved using the CVM technique to achieve scalability to handle large data sets. Experiments done with several large synthetic and real world data sets show that the proposed MCVM technique gives good generalization performance as that of SVM at a much lesser computational expense. Further, it is observed that MCVM scales well with the size of the data set.</p></div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273503">The rendezvous algorithm: multiclass semi-supervised learning with Markov random walks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315487826">Arik Azran</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 49-56</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273503" title="DOI">10.1145/1273496.1273503</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273503&ftid=425355&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow7" style="display:inline;"><br /><div style="display:inline">We consider the problem of multiclass classification where both labeled and unlabeled data points are given. We introduce and demonstrate a new approach for estimating a distribution over the missing labels where data points are viewed as nodes of a ...</div></span>
<span id="toHide7" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of multiclass classification where both labeled and unlabeled data points are given. We introduce and demonstrate a new approach for estimating a distribution over the missing labels where data points are viewed as nodes of a graph, and pairwise similarities are used to derive a transition probability matrix <i>P</i> for a Markov random walk between them. The algorithm associates each point with a particle which moves between points according to <i>P</i>. Labeled points are set to be absorbing states of the Markov random walk, and the probability of each particle to be absorbed by the different labeled points, as the number of steps increases, is then used to derive a distribution over the associated missing label. A computationally efficient algorithm to implement this is derived and demonstrated on both real and artificial data sets, including a numerical comparison with other methods.</p></div></span> <a id="expcoll7" href="JavaScript: expandcollapse('expcoll7',7)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273504">Focused crawling with scalable ordinal regression solvers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333487479">Rashmin Babaria</a>,
<a href="author_page.cfm?id=81317489516">J. Saketha Nath</a>,
<a href="author_page.cfm?id=81333489546">Krishnan S</a>,
<a href="author_page.cfm?id=81333491180">Sivaramakrishnan K R</a>,
<a href="author_page.cfm?id=81100050056">Chiranjib Bhattacharyya</a>,
<a href="author_page.cfm?id=81100199654">M. N. Murty</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 57-64</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273504" title="DOI">10.1145/1273496.1273504</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273504&ftid=425356&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow8" style="display:inline;"><br /><div style="display:inline">In this paper we propose a novel, scalable, clustering based Ordinal Regression formulation, which is an instance of a Second Order Cone Program (SOCP) with one Second Order Cone (SOC) constraint. The main contribution of the paper is a fast algorithm, ...</div></span>
<span id="toHide8" style="display:none;"><br /><div style="display:inline"><p>In this paper we propose a novel, scalable, clustering based Ordinal Regression formulation, which is an instance of a Second Order Cone Program (SOCP) with one Second Order Cone (SOC) constraint. The main contribution of the paper is a fast algorithm, CB-OR, which solves the proposed formulation more eficiently than general purpose solvers. Another main contribution of the paper is to pose the problem of focused crawling as a large scale Ordinal Regression problem and solve using the proposed CB-OR. Focused crawling is an efficient mechanism for discovering resources of interest on the web. Posing the problem of focused crawling as an Ordinal Regression problem avoids the need for a negative class and topic hierarchy, which are the main drawbacks of the existing focused crawling methods. Experiments on large synthetic and benchmark datasets show the scalability of CB-OR. Experiments also show that the proposed focused crawler outperforms the state-of-the-art.</p></div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273505">Learning distance function by coding similarity</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81300336101">Aharon Bar Hillel</a>,
<a href="author_page.cfm?id=81100572427">Daphna Weinshall</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 65-72</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273505" title="DOI">10.1145/1273496.1273505</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273505&ftid=425357&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow9" style="display:inline;"><br /><div style="display:inline">We consider the problem of learning a similarity function from a set of positive equivalence constraints, i.e. 'similar' point pairs. We define the similarity in information theoretic terms, as the gain in coding length when shifting from independent ...</div></span>
<span id="toHide9" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of learning a similarity function from a set of positive equivalence constraints, i.e. 'similar' point pairs. We define the similarity in information theoretic terms, as the gain in coding length when shifting from independent encoding of the pair to joint encoding. Under simple Gaussian assumptions, this formulation leads to a non-Mahalanobis similarity function which is efficient and simple to learn. This function can be viewed as a likelihood ratio test, and we show that the optimal similarity-preserving projection of the data is a variant of Fisher Linear Discriminant. We also show that under some naturally occurring sampling conditions of equivalence constraints, this function converges to a known Mahalanobis distance (RCA). The suggested similarity function exhibits superior performance over alternative Mahalanobis distances learnt from the same data. Its superiority is demonstrated in the context of image retrieval and graph based clustering, using a large number of data sets.</p></div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273506">Structural alignment based kernels for protein structure classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
 <a href="author_page.cfm?id=81333487532">Sourangshu Bhattacharya</a>,
<a href="author_page.cfm?id=81100050056">Chiranjib Bhattacharyya</a>,
<a href="author_page.cfm?id=81333487998">Nagasuma Chandra</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 73-80</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273506" title="DOI">10.1145/1273496.1273506</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273506&ftid=425358&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow10" style="display:inline;"><br /><div style="display:inline">Structural alignments are the most widely used tools for comparing proteins with low sequence similarity. The main contribution of this paper is to derive various kernels on proteins from structural alignments, which do not use sequence information. ...</div></span>
<span id="toHide10" style="display:none;"><br /><div style="display:inline"><p>Structural alignments are the most widely used tools for comparing proteins with low sequence similarity. The main contribution of this paper is to derive various kernels on proteins from structural alignments, which do not use sequence information. Central to the kernels is a novel alignment algorithm which matches substructures of fixed size using spectral graph matching techniques. We derive positive semi-definite kernels which capture the notion of similarity between substructures. Using these as base more sophisticated kernels on protein structures are proposed. To empirically evaluate the kernels we used a 40% sequence non-redundant structures from 15 different SCOP superfamilies. The kernels when used with SVMs show competitive performance with CE, a state of the art structure comparison program.</p></div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273507">Discriminative learning for differing training and test distributions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333487432">Steffen Bickel</a>,
<a href="author_page.cfm?id=81333487810">Michael Br&#252;ckner</a>,
<a href="author_page.cfm?id=81100180901">Tobias Scheffer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 81-88</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273507" title="DOI">10.1145/1273496.1273507</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273507&ftid=425359&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow11" style="display:inline;"><br /><div style="display:inline">We address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution---problems also referred to as classification under covariate shift. We derive a solution ...</div></span>
<span id="toHide11" style="display:none;"><br /><div style="display:inline"><p>We address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution---problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. We formulate the general problem of learning under covariate shift as an integrated optimization problem. We derive a kernel logistic regression classifier for differing training and test distributions.</p></div></span> <a id="expcoll11" href="JavaScript: expandcollapse('expcoll11',11)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273508">Solving multiclass support vector machines with LaRank</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81321489328">Antoine Bordes</a>,
<a href="author_page.cfm?id=81100263096">L&#233;on Bottou</a>,
<a href="author_page.cfm?id=81100169573">Patrick Gallinari</a>,
<a href="author_page.cfm?id=81100015405">Jason Weston</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 89-96</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273508" title="DOI">10.1145/1273496.1273508</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273508&ftid=425360&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow12" style="display:inline;"><br /><div style="display:inline">Optimization algorithms for large margin multiclass recognizers are often too costly to handle ambitious problems with structured outputs and exponential numbers of classes. Optimization algorithms that rely on the full gradient are not effective because, ...</div></span>
<span id="toHide12" style="display:none;"><br /><div style="display:inline"><p>Optimization algorithms for large margin multiclass recognizers are often too costly to handle ambitious problems with structured outputs and exponential numbers of classes. Optimization algorithms that rely on the full gradient are not effective because, unlike the solution, the gradient is not sparse and is very large. The LaRank algorithm sidesteps this difficulty by relying on a randomized exploration inspired by the perceptron algorithm. We show that this approach is competitive with gradient based optimizers on simple multiclass problems. Furthermore, a single LaRank pass over the training examples delivers test error rates that are nearly as good as those of the final solution.</p></div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273509">Efficiently computing minimax expected-size confidence regions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365592321">Brent Bryan</a>,
<a href="author_page.cfm?id=81100140036">H. Brendan McMahan</a>,
<a href="author_page.cfm?id=81100656569">Chad M. Schafer</a>,
<a href="author_page.cfm?id=81100155456">Jeff Schneider</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 97-104</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273509" title="DOI">10.1145/1273496.1273509</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273509&ftid=425361&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow13" style="display:inline;"><br /><div style="display:inline">Given observed data and a collection of parameterized candidate models, a 1 -- &alpha; confidence region in parameter space provides useful insight as to those models which are a good fit to the data, all while keeping the probability of incorrect exclusion ...</div></span>
<span id="toHide13" style="display:none;"><br /><div style="display:inline"><p>Given observed data and a collection of parameterized candidate models, a 1 -- &alpha; confidence region in parameter space provides useful insight as to those models which are a good fit to the data, all while keeping the probability of incorrect exclusion below &alpha;. With complex models, optimally precise procedures (those with small expected size) are, in practice, difficult to derive; one solution is the Minimax Expected-Size (MES) confidence procedure. The key computational problem of MES is computing a minimax equilibria to a certain zero-sum game. We show that this game is convex with bilinear payoffs, allowing us to apply any convex game solver, including linear programming. Exploiting the sparsity of the matrix, along with using fast linear programming software, allows us to compute approximate minimax expected-size confidence regions orders of magnitude faster than previously published methods. We test these approaches by estimating parameters for a cosmological model.</p></div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273510">Multiple instance learning for sparse positive bags</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100589066">Razvan C. Bunescu</a>,
<a href="author_page.cfm?id=81100539345">Raymond J. Mooney</a>
 </span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 105-112</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273510" title="DOI">10.1145/1273496.1273510</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273510&ftid=425362&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow14" style="display:inline;"><br /><div style="display:inline">We present a new approach to multiple instance learning (MIL) that is particularly effective when the positive bags are sparse (i.e. contain few positive instances). Unlike other SVM-based MIL methods, our approach more directly enforces the desired ...</div></span>
<span id="toHide14" style="display:none;"><br /><div style="display:inline"><p>We present a new approach to <i>multiple instance learning</i> (MIL) that is particularly effective when the positive bags are sparse (i.e. contain few positive instances). Unlike other SVM-based MIL methods, our approach more directly enforces the desired constraint that <i>at least one</i> of the instances in a positive bag is positive. Using both artificial and real-world data, we experimentally demonstrate that our approach achieves greater accuracy than state-of-the-art MIL methods when positive bags are sparse, and performs competitively when they are not. In particular, our approach is the best performing method for image region classification.</p></div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273511">Cluster analysis of heterogeneous rank data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333487785">Ludwig M. Busse</a>,
<a href="author_page.cfm?id=81333490164">Peter Orbanz</a>,
<a href="author_page.cfm?id=81331489168">Joachim M. Buhmann</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 113-120</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273511" title="DOI">10.1145/1273496.1273511</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273511&ftid=425363&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow15" style="display:inline;"><br /><div style="display:inline">Cluster analysis of ranking data, which occurs in consumer questionnaires, voting forms or other inquiries of preferences, attempts to identify typical groups of rank choices. Empirically measured rankings are often incomplete, i.e. different numbers ...</div></span>
<span id="toHide15" style="display:none;"><br /><div style="display:inline"><p>Cluster analysis of ranking data, which occurs in consumer questionnaires, voting forms or other inquiries of preferences, attempts to identify typical groups of rank choices. Empirically measured rankings are often incomplete, i.e. different numbers of filled rank positions cause heterogeneity in the data. We propose a mixture approach for clustering of heterogeneous rank data. Rankings of different lengths can be described and compared by means of a single probabilistic model. A maximum entropy approach avoids hidden assumptions about missing rank positions. Parameter estimators and an efficient EM algorithm for unsupervised inference are derived for the ranking mixture model. Experiments on both synthetic data and real-world data demonstrate significantly improved parameter estimates on heterogeneous data when the incomplete rankings are included in the inference process.</p></div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273512">Feature selection in a kernel space</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100130445">Bin Cao</a>,
<a href="author_page.cfm?id=81313483930">Dou Shen</a>,
<a href="author_page.cfm?id=81351592034">Jian-Tao Sun</a>,
<a href="author_page.cfm?id=81372591186">Qiang Yang</a>,
<a href="author_page.cfm?id=81416601059">Zheng Chen</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 121-128</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273512" title="DOI">10.1145/1273496.1273512</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273512&ftid=425364&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow16" style="display:inline;"><br /><div style="display:inline">We address the problem of feature selection in a kernel space to select the most discriminative and informative features for classification and data analysis. This is a difficult problem because the dimension of a kernel space may be infinite. In the ...</div></span>
<span id="toHide16" style="display:none;"><br /><div style="display:inline"><p>We address the problem of feature selection in a kernel space to select the most discriminative and informative features for classification and data analysis. This is a difficult problem because the dimension of a kernel space may be infinite. In the past, little work has been done on feature selection in a kernel space. To solve this problem, we derive a basis set in the kernel space as a first step for feature selection. Using the basis set, we then extend the margin-based feature selection algorithms that are proven effective even when many features are dependent. The selected features form a subspace of the kernel space, in which different state-of-the-art classification algorithms can be applied for classification. We conduct extensive experiments over real and simulated data to compare our proposed method with four baseline algorithms. Both theoretical analysis and experimental results validate the effectiveness of our proposed method.</p></div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273513">Learning to rank: from pairwise approach to listwise approach</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81545787556">Zhe Cao</a>,
<a href="author_page.cfm?id=81100549718">Tao Qin</a>,
<a href="author_page.cfm?id=81350580267">Tie-Yan Liu</a>,
<a href="author_page.cfm?id=81335498595">Ming-Feng Tsai</a>,
<a href="author_page.cfm?id=81350598903">Hang Li</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 129-136</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273513" title="DOI">10.1145/1273496.1273513</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273513&ftid=425365&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow17" style="display:inline;"><br /><div style="display:inline">The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative filtering, and many other applications. Several methods for learning to rank ...</div></span>
<span id="toHide17" style="display:none;"><br /><div style="display:inline"><p>The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative filtering, and many other applications. Several methods for learning to rank have been proposed, which take object pairs as 'instances' in learning. We refer to them as the pairwise approach in this paper. Although the pairwise approach offers advantages, it ignores the fact that ranking is a prediction task on list of objects. The paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as 'instances' in learning. The paper proposes a new probabilistic method for the approach. Specifically it introduces two probability models, respectively referred to as permutation probability and top <i>k</i> probability, to define a listwise loss function for learning. Neural Network and Gradient Descent are then employed as model and algorithm in the learning method. Experimental results on information retrieval show that the proposed listwise approach performs better than the pairwise approach.</p></div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273514">Local similarity discriminant analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81350591806">Luca Cazzanti</a>,
<a href="author_page.cfm?id=81100021030">Maya R. Gupta</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 137-144</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273514" title="DOI">10.1145/1273496.1273514</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273514&ftid=425366&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow18" style="display:inline;"><br /><div style="display:inline">We propose a local, generative model for similarity-based classification. The method is applicable to the case that only pairwise similarities between samples are available. The classifier models the local class-conditional distribution using a maximum ...</div></span>
<span id="toHide18" style="display:none;"><br /><div style="display:inline"><p>We propose a local, generative model for similarity-based classification. The method is applicable to the case that only pairwise similarities between samples are available. The classifier models the local class-conditional distribution using a maximum entropy estimate and empirical moment constraints. The resulting exponential class conditional-distributions are combined with class prior probabilities and misclassification costs to form the <i>local similarity discriminant analysis</i> (local SDA) classifier. We compare the performance of local SDA to a non-local version, to the local nearest centroid classifier, the nearest centroid classifier, k-NN, and to the recently-developed potential support vector machine (PSVM). Results show that local SDA is competitive with k-NN and the computationally-demanding PSVM while offering the advantages of a generative classifier.</p></div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273515">Direct convex relaxations of sparse SVM</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81538887256">Antoni B. Chan</a>,
<a href="author_page.cfm?id=81100239272">Nuno Vasconcelos</a>,
<a href="author_page.cfm?id=81100118766">Gert R. G. Lanckriet</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 145-153</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273515" title="DOI">10.1145/1273496.1273515</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273515&ftid=425367&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow19" style="display:inline;"><br /><div style="display:inline">Although support vector machines (SVMs) for binary classification give rise to a decision rule that only relies on a subset of the training data points (support vectors), it will in general be based on all available features in the input space. We propose ...</div></span>
<span id="toHide19" style="display:none;"><br /><div style="display:inline"><p>Although support vector machines (SVMs) for binary classification give rise to a decision rule that only relies on a subset of the training data points (support vectors), it will in general be based on all available features in the input space. We propose two direct, novel convex relaxations of a non-convex sparse SVM formulation that explicitly constrains the cardinality of the vector of feature weights. One relaxation results in a quadratically-constrained quadratic program (QCQP), while the second is based on a semidefinite programming (SDP) relaxation. The QCQP formulation can be interpreted as applying an adaptive soft-threshold on the SVM hyperplane, while the SDP formulation learns a weighted inner-product (i.e. a kernel) that results in a sparse hyperplane. Experimental results show an increase in sparsity while conserving the generalization performance compared to a standard as well as a linear programming SVM.</p></div></span> <a id="expcoll19" href="JavaScript: expandcollapse('expcoll19',19)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273516">Minimum reference set based feature selection for small sample classifications</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81320488854">Xue-wen Chen</a>,
<a href="author_page.cfm?id=81343495943">Jong Cheol Jeong</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 153-160</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273516" title="DOI">10.1145/1273496.1273516</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273516&ftid=425368&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow20" style="display:inline;"><br /><div style="display:inline">We address feature selection problems for classification of small samples and high dimensionality. A practical example is microarray-based cancer classification problems, where sample size is typically less than 100 and number of features is several ...</div></span>
<span id="toHide20" style="display:none;"><br /><div style="display:inline"><p>We address feature selection problems for classification of small samples and high dimensionality. A practical example is microarray-based cancer classification problems, where sample size is typically less than 100 and number of features is several thousands or higher. One of the commonly used methods in addressing this problem is recursive feature elimination (RFE) method, which utilizes the generalization capability embedded in support vector machines and is thus suitable for small samples problems. We propose a novel method using minimum reference set (MRS) generated by the nearest neighbor rule. MRS is the set of minimum number of samples that correctly classify all the training samples. It is related to structural risk minimization principle and thus leads to good generalization. The proposed MRS based method is compared to RFE method with several real datasets, and experimental results show that the MRS method produces better classification performance.</p></div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273517">Learning to compress images and videos</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100098097">Li Cheng</a>,
<a href="author_page.cfm?id=81100528901">S. V. N. Vishwanathan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 161-168</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273517" title="DOI">10.1145/1273496.1273517</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273517&ftid=425369&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow21" style="display:inline;"><br /><div style="display:inline">We present an intuitive scheme for lossy color-image compression: Use the color information from a few representative pixels to learn a model which predicts color on the rest of the pixels. Now, storing the representative pixels and the image in grayscale ...</div></span>
<span id="toHide21" style="display:none;"><br /><div style="display:inline"><p>We present an intuitive scheme for lossy color-image compression: Use the color information from a few representative pixels to learn a model which predicts color on the rest of the pixels. Now, storing the representative pixels and the image in grayscale suffice to recover the original image. A similar scheme is also applicable for compressing videos, where a single model can be used to predict color on many consecutive frames, leading to better compression. Existing algorithms for colorization -- the process of adding color to a grayscale image or video sequence -- are tedious, and require intensive human-intervention. We bypass these limitations by using a graph-based inductive semi-supervised learning module for colorization, and a simple active learning strategy to choose the representative pixels. Experiments on a wide variety of images and video sequences demonstrate the efficacy of our algorithm.</p></div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273518">Magnitude-preserving ranking algorithms</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81332494270">Corinna Cortes</a>,
<a href="author_page.cfm?id=81100197439">Mehryar Mohri</a>,
<a href="author_page.cfm?id=81333490588">Ashish Rastogi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 169-176</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273518" title="DOI">10.1145/1273496.1273518</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273518&ftid=425370&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow22" style="display:inline;"><br /><div style="display:inline">This paper studies the learning problem of ranking when one wishes not just to accurately predict pairwise ordering but also preserve the magnitude of the preferences or the difference between ratings, a problem motivated by its key importance in the ...</div></span>
<span id="toHide22" style="display:none;"><br /><div style="display:inline"><p>This paper studies the learning problem of ranking when one wishes not just to accurately predict pairwise ordering but also preserve the magnitude of the preferences or the difference between ratings, a problem motivated by its key importance in the design of search engines, movie recommendation, and other similar ranking systems. We describe and analyze several algorithms for this problem and give stability bounds for their generalization error, extending previously known stability results to non-bipartite ranking and magnitude of preference-preserving algorithms. We also report the results of experiments comparing these algorithms on several datasets and compare these results with those obtained using an algorithm minimizing the pairwise misranking error and standard regression.</p></div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273519">Full regularization path for sparse principal component analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315488475">Alexandre d'Aspremont</a>,
<a href="author_page.cfm?id=81100328355">Francis R. Bach</a>,
<a href="author_page.cfm?id=81100540027">Laurent El Ghaoui</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 177-184</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273519" title="DOI">10.1145/1273496.1273519</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273519&ftid=425371&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow23" style="display:inline;"><br /><div style="display:inline">Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse ...</div></span>
<span id="toHide23" style="display:none;"><br /><div style="display:inline"><p>Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a <i>full set</i> of good solutions for all numbers of non zero coefficients, with complexity <i>O(n</i><sup>3</sup>), where <i>n</i> is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in <i>O(n</i><sup>3</sup>). We show on toy examples and biological data that our algorithm does provide globally optimal solutions in many cases.</p></div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273520">Kernel selection forl semi-supervised kernel machines</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100172115">Guang Dai</a>,
<a href="author_page.cfm?id=81100614347">Dit-Yan Yeung</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 185-192</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273520" title="DOI">10.1145/1273496.1273520</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273520&ftid=425372&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow24" style="display:inline;"><br /><div style="display:inline">Existing semi-supervised learning methods are mostly based on either the cluster assumption or the manifold assumption. In this paper, we propose an integrated regularization framework for semi-supervised kernel machines by incorporating both the cluster ...</div></span>
<span id="toHide24" style="display:none;"><br /><div style="display:inline"><p>Existing semi-supervised learning methods are mostly based on either the cluster assumption or the manifold assumption. In this paper, we propose an integrated regularization framework for semi-supervised kernel machines by incorporating both the cluster assumption and the manifold assumption. Moreover, it supports kernel learning in the form of kernel selection. The optimization problem involves joint optimization over all the labeled and unlabeled data points, a convex set of basic kernels, and a discrete space of unknown labels for the unlabeled data. When the manifold assumption is incorporated, graph Laplacian kernels are used as the basic kernels for learning an optimal convex combination of graph Laplacian kernels. Comparison with related methods on the USPS data set shows very promising results.</p></div></span> <a id="expcoll24" href="JavaScript: expandcollapse('expcoll24',24)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273521">Boosting for transfer learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333488189">Wenyuan Dai</a>,
<a href="author_page.cfm?id=81372591186">Qiang Yang</a>,
<a href="author_page.cfm?id=81100142932">Gui-Rong Xue</a>,
<a href="author_page.cfm?id=81363594294">Yong Yu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 193-200</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273521" title="DOI">10.1145/1273496.1273521</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273521&ftid=425373&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow25" style="display:inline;"><br /><div style="display:inline">Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task ...</div></span>
<span id="toHide25" style="display:none;"><br /><div style="display:inline"><p>Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this <i>identical-distribution</i> assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away <i>all</i> the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund & Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model.</p></div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273522">Intractability and clustering with constraints</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100099431">Ian Davidson</a>,
<a href="author_page.cfm?id=81452611706">S. S. Ravi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 201-208</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273522" title="DOI">10.1145/1273496.1273522</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273522&ftid=425374&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow26" style="display:inline;"><br /><div style="display:inline">Clustering with constraints is a developing area of machine learning. Various papers have used constraints to enforce particular clusterings, seed clustering algorithms and even learn distance functions which are then used for clustering. We present ...</div></span>
<span id="toHide26" style="display:none;"><br /><div style="display:inline"><p>Clustering with constraints is a developing area of machine learning. Various papers have used constraints to enforce particular clusterings, seed clustering algorithms and even learn distance functions which are then used for clustering. We present intractability results for some constraint combinations and illustrate both formally and experimentally the implications of these results for using constraints with clustering.</p></div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273523">Information-theoretic metric learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317487671">Jason V. Davis</a>,
<a href="author_page.cfm?id=81100083449">Brian Kulis</a>,
<a href="author_page.cfm?id=81547334756">Prateek Jain</a>,
<a href="author_page.cfm?id=81100115933">Suvrit Sra</a>,
<a href="author_page.cfm?id=81100098715">Inderjit S. Dhillon</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 209-216</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273523" title="DOI">10.1145/1273496.1273523</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273523&ftid=425375&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow27" style="display:inline;"><br /><div style="display:inline">In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance ...</div></span>
<span id="toHide27" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function. We express this problem as a particular Bregman optimization problem---that of minimizing the LogDet divergence subject to linear constraints. Our resulting algorithm has several advantages over existing methods. First, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. Second, it is fast and scalable. Unlike most existing methods, no eigenvalue computations or semi-definite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of metric learning for nearest neighbor classification, as well as on standard data sets.</p></div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273524">An integrated approach to feature invention and model construction for drug activity prediction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100603602">Jesse Davis</a>,
<a href="author_page.cfm?id=81100281490">V&#237;tor Santos Costa</a>,
<a href="author_page.cfm?id=81100552505">Soumya Ray</a>,
<a href="author_page.cfm?id=81405593471">David Page</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 217-224</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273524" title="DOI">10.1145/1273496.1273524</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273524&ftid=425376&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow28" style="display:inline;"><br /><div style="display:inline">We present a new machine learning approach for 3D-QSAR, the task of predicting binding affinities of molecules to target proteins based on 3D structure. Our approach predicts binding affinity by using regression on substructures discovered by relational ...</div></span>
<span id="toHide28" style="display:none;"><br /><div style="display:inline"><p>We present a new machine learning approach for 3D-QSAR, the task of predicting binding affinities of molecules to target proteins based on 3D structure. Our approach predicts binding affinity by using regression on substructures discovered by relational learning. We make two contributions to the state-of-the-art. First, we use multiple-instance (MI) regression, which represents a molecule as a set of 3D conformations, to model activity. Second, the relational learning component employs the "Score As You Use" (SAYU) method to select substructures for their ability to improve the regression model. This is the first application of SAYU to multiple-instance, real-valued prediction. We evaluate our approach on three tasks and demonstrate that (i) SAYU outperforms standard coverage measures when selecting features for regression, (ii) the MI representation improves accuracy over standard single feature-vector encodings and (iii) combining SAYU with MI regression is more accurate for 3D-QSAR than either approach by itself.</p></div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273525">Percentile optimization in uncertain Markov decision processes with application to efficient exploration</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81436594796">Erick Delage</a>,
<a href="author_page.cfm?id=81100515533">Shie Mannor</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 225-232</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273525" title="DOI">10.1145/1273496.1273525</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273525&ftid=425377&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow29" style="display:inline;"><br /><div style="display:inline">Markov decision processes are an effective tool in modeling decision-making in uncertain dynamic environments. Since the parameters of these models are typically estimated from data, learned from experience, or designed by hand, it is not surprising ...</div></span>
<span id="toHide29" style="display:none;"><br /><div style="display:inline"><p>Markov decision processes are an effective tool in modeling decision-making in uncertain dynamic environments. Since the parameters of these models are typically estimated from data, learned from experience, or designed by hand, it is not surprising that the actual performance of a chosen strategy often significantly differs from the designer's initial expectations due to unavoidable model uncertainty. In this paper, we present a percentile criterion that captures the trade-off between optimistic and pessimistic points of view on MDP with parameter uncertainty. We describe tractable methods that take parameter uncertainty into account in the process of decision making. Finally, we propose a cost-effective exploration strategy when it is possible to invest (money, time or computation efforts) in actions that will reduce the uncertainty in the parameters.</p></div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273526">Unsupervised prediction of citation influences</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333488165">Laura Dietz</a>,
<a href="author_page.cfm?id=81333487432">Steffen Bickel</a>,
<a href="author_page.cfm?id=81100180901">Tobias Scheffer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 233-240</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273526" title="DOI">10.1145/1273496.1273526</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273526&ftid=425378&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow30" style="display:inline;"><br /><div style="display:inline">Publication repositories contain an abundance of information about the evolution of scientific research areas. We address the problem of creating a visualization of a research area that describes the flow of topics between papers, quantifies the impact ...</div></span>
<span id="toHide30" style="display:none;"><br /><div style="display:inline"><p>Publication repositories contain an abundance of information about the evolution of scientific research areas. We address the problem of creating a visualization of a research area that describes the flow of topics between papers, quantifies the impact that papers have on each other, and helps to identify key contributions. To this end, we devise a probabilistic topic model that explains the generation of documents; the model incorporates the aspects of topical innovation and topical inheritance via citations. We evaluate the model's ability to predict the strength of influence of citations against manually rated citations.</p></div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273527">Non-isometric manifold learning: analysis and an algorithm</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81387591919">Piotr Doll&#225;r</a>,
<a href="author_page.cfm?id=81309489852">Vincent Rabaud</a>,
<a href="author_page.cfm?id=81100352084">Serge Belongie</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 241-248</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273527" title="DOI">10.1145/1273496.1273527</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273527&ftid=425379&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow31" style="display:inline;"><br /><div style="display:inline">In this work we take a novel view of nonlinear manifold learning. Usually, manifold learning is formulated in terms of finding an embedding or 'unrolling' of a manifold into a lower dimensional space. Instead, we treat it as the problem of learning a ...</div></span>
<span id="toHide31" style="display:none;"><br /><div style="display:inline"><p>In this work we take a novel view of nonlinear manifold learning. Usually, manifold learning is formulated in terms of finding an embedding or 'unrolling' of a manifold into a lower dimensional space. Instead, we treat it as the problem of learning a representation of a nonlinear, possibly non-isometric manifold that allows for the manipulation of novel points. Central to this view of manifold learning is the concept of generalization beyond the training data. Drawing on concepts from supervised learning, we establish a framework for studying the problems of model assessment, model complexity, and model selection for manifold learning. We present an extension of a recent algorithm, Locally Smooth Manifold Learning (LSML), and show it has good generalization properties. LSML learns a representation of a manifold or family of related manifolds and can be used for computing geodesic distances, finding the projection of a point onto a manifold, recovering a manifold from points corrupted by noise, generating novel points on a manifold, and more.</p></div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273528">Hierarchical maximum entropy density estimation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100493879">Miroslav Dudik</a>,
<a href="author_page.cfm?id=81100028344">David M. Blei</a>,
<a href="author_page.cfm?id=81100083454">Robert E. Schapire</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 249-256</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273528" title="DOI">10.1145/1273496.1273528</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273528&ftid=425380&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow32" style="display:inline;"><br /><div style="display:inline">We study the problem of simultaneously estimating several densities where the datasets are organized into overlapping groups, such as a hierarchy. For this problem, we propose a maximum entropy formulation, which systematically incorporates the groups ...</div></span>
<span id="toHide32" style="display:none;"><br /><div style="display:inline"><p>We study the problem of simultaneously estimating several densities where the datasets are organized into overlapping groups, such as a hierarchy. For this problem, we propose a maximum entropy formulation, which systematically incorporates the groups and allows us to share the strength of prediction across similar datasets. We derive general performance guarantees, and show how some previous approaches, such as hierarchical shrinkage and hierarchical priors, can be derived as special cases. We demonstrate the proposed technique on synthetic data and in a real-world application to modeling the geographic distributions of species hierarchically grouped in a taxonomy. Specifically, we model the geographic distributions of species in the Australian wet tropics and Northeast New South Wales. In these regions, small numbers of samples per species significantly hinder effective prediction. Substantial benefits are obtained by combining information across taxonomic groups.</p></div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273529">CarpeDiem: an algorithm for the fast evaluation of SSL classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100065716">Roberto Esposito</a>,
<a href="author_page.cfm?id=81333490420">Daniele P. Radicioni</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 257-264</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273529" title="DOI">10.1145/1273496.1273529</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273529&ftid=425720&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow33" style="display:inline;"><br /><div style="display:inline">In this paper we present a novel algorithm, CarpeDiem. It significantly improves on the time complexity of Viterbi algorithm, preserving the optimality of the result. This fact has consequences on Machine Learning systems that use Viterbi algorithm during ...</div></span>
<span id="toHide33" style="display:none;"><br /><div style="display:inline"><p>In this paper we present a novel algorithm, CarpeDiem. It significantly improves on the time complexity of Viterbi algorithm, preserving the optimality of the result. This fact has consequences on Machine Learning systems that use Viterbi algorithm during learning or classification. We show how the algorithm applies to the Supervised Sequential Learning task and, in particular, to the HMPerceptron algorithm. We illustrate CarpeDiem in full details, and provide experimental results that support the proposed approach.</p></div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a> 
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273530">Manifold-adaptive dimension estimation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81388597470">Amir massoud Farahmand</a>,
<a href="author_page.cfm?id=81100538163">Csaba Szepesv&#225;ri</a>,
<a href="author_page.cfm?id=81309481887">Jean-Yves Audibert</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 265-272</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273530" title="DOI">10.1145/1273496.1273530</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273530&ftid=425721&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow34" style="display:inline;"><br /><div style="display:inline">Intuitively, learning should be easier when the data points lie on a low-dimensional submanifold of the input space. Recently there has been a growing interest in algorithms that aim to exploit such geometrical properties of the data. Oftentimes these ...</div></span>
<span id="toHide34" style="display:none;"><br /><div style="display:inline"><p>Intuitively, learning should be easier when the data points lie on a low-dimensional submanifold of the input space. Recently there has been a growing interest in algorithms that aim to exploit such geometrical properties of the data. Oftentimes these algorithms require estimating the dimension of the manifold first. In this paper we propose an algorithm for dimension estimation and study its finite-sample behaviour. The algorithm estimates the dimension locally around the data points using nearest neighbor techniques and then combines these local estimates. We show that the rate of convergence of the resulting estimate is independent of the dimension of the input space and hence the algorithm is "manifold-adaptive". Thus, when the manifold supporting the data is low dimensional, the algorithm can be exponentially more efficient than its counterparts that are not exploiting this property. Our computer experiments confirm the obtained theoretical results.</p></div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273531">Combining online and offline knowledge in UCT</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333488555">Sylvain Gelly</a>,
<a href="author_page.cfm?id=81100336678">David Silver</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 273-280</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273531" title="DOI">10.1145/1273496.1273531</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273531&ftid=425722&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow35" style="display:inline;"><br /><div style="display:inline">The UCT algorithm learns a value function online using sample-based search. The TD(&lambda;) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions ...</div></span>
<span id="toHide35" style="display:none;"><br /><div style="display:inline"><p>The UCT algorithm learns a value function online using sample-based search. The <i>TD</i>(&lambda;) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions in the UCT algorithm. First, the offline value function is used as a default policy during Monte-Carlo simulation. Second, the UCT value function is combined with a rapid online estimate of action values. Third, the offline value function is used as prior knowledge in the UCT search tree. We evaluate these algorithms in 9 x 9 Go against GnuGo 3.7.10. The first algorithm performs better than UCT with a random simulation policy, but surprisingly, worse than UCT with a weaker, handcrafted simulation policy. The second algorithm outperforms UCT altogether. The third algorithm outperforms UCT with handcrafted prior knowledge. We combine these algorithms in <i>MoGo</i>, the world's strongest 9 x 9 Go program. Each technique significantly improves <i>MoGo's</i> playing strength.</p></div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273532">Robust non-linear dimensionality reduction using successive 1-dimensional Laplacian Eigenmaps</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333488387">Samuel Gerber</a>,
<a href="author_page.cfm?id=81100204789">Tolga Tasdizen</a>,
<a href="author_page.cfm?id=81100483298">Ross Whitaker</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 281-288</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273532" title="DOI">10.1145/1273496.1273532</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273532&ftid=425723&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow36" style="display:inline;"><br /><div style="display:inline">Non-linear dimensionality reduction of noisy data is a challenging problem encountered in a variety of data analysis applications. Recent results in the literature show that spectral decomposition, as used for example by the Laplacian Eigenmaps algorithm, ...</div></span>
<span id="toHide36" style="display:none;"><br /><div style="display:inline"><p>Non-linear dimensionality reduction of noisy data is a challenging problem encountered in a variety of data analysis applications. Recent results in the literature show that spectral decomposition, as used for example by the Laplacian Eigenmaps algorithm, provides a powerful tool for non-linear dimensionality reduction and manifold learning. In this paper, we discuss a significant shortcoming of these approaches, which we refer to as the <i>repeated eigendirections problem</i>. We propose a novel approach that combines successive 1-dimensional spectral embeddings with a data advection scheme that allows us to address this problem. The proposed method does not depend on a non-linear optimization scheme; hence, it is not prone to local minima. Experiments with artificial and real data illustrate the advantages of the proposed method over existing approaches. We also demonstrate that the approach is capable of correctly learning manifolds corrupted by significant amounts of noise.</p></div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273533">Gradient boosting for kernelized output spaces</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100360352">Pierre Geurts</a>,
<a href="author_page.cfm?id=81100172542">Louis Wehenkel</a>,
<a href="author_page.cfm?id=81100133327">Florence d'Alch&#233;-Buc</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 289-296</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273533" title="DOI">10.1145/1273496.1273533</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273533&ftid=425724&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow37" style="display:inline;"><br /><div style="display:inline">A general framework is proposed for gradient boosting in supervised learning problems where the loss function is defined using a kernel over the output space. It extends boosting in a principled way to complex output spaces (images, text, graphs etc.) ...</div></span>
<span id="toHide37" style="display:none;"><br /><div style="display:inline"><p>A general framework is proposed for gradient boosting in supervised learning problems where the loss function is defined using a kernel over the output space. It extends boosting in a principled way to complex output spaces (images, text, graphs etc.) and can be applied to a general class of base learners working in kernelized output spaces. Empirical results are provided on three problems: a regression problem, an image completion task and a graph prediction problem. In these experiments, the framework is combined with tree-based base learners, which have interesting algorithmic properties. The results show that gradient boosting significantly improves these base learners and provides competitive results with other tree-based ensemble methods based on randomization.</p></div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273534">Bayesian actor-critic algorithms</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100211551">Mohammad Ghavamzadeh</a>,
<a href="author_page.cfm?id=81100183155">Yaakov Engel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 297-304</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273534" title="DOI">10.1145/1273496.1273534</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273534&ftid=425725&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow38" style="display:inline;"><br /><div style="display:inline">We present a new actor-critic learning model in which a Bayesian class of non-parametric critics, using Gaussian process temporal difference learning is used. Such critics model the state-action value function as a Gaussian process, allowing Bayes' rule ...</div></span>
<span id="toHide38" style="display:none;"><br /><div style="display:inline"><p>We present a new actor-critic learning model in which a Bayesian class of non-parametric critics, using Gaussian process temporal difference learning is used. Such critics model the state-action value function as a Gaussian process, allowing Bayes' rule to be used in computing the posterior distribution over state-action value functions, conditioned on the observed data. Appropriate choices of the prior covariance (kernel) between state-action values and of the parametrization of the policy allow us to obtain closed-form expressions for the posterior distribution of the gradient of the average discounted return with respect to the policy parameters. The posterior mean, which serves as our estimate of the policy gradient, is used to update the policy, while the posterior covariance allows us to gauge the reliability of the update.</p></div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273535">Exponentiated gradient algorithms for log-linear structured prediction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100607997">Amir Globerson</a>,
<a href="author_page.cfm?id=81309498429">Terry Y. Koo</a>,
<a href="author_page.cfm?id=81100282019">Xavier Carreras</a>,
<a href="author_page.cfm?id=81406599089">Michael Collins</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 305-312</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273535" title="DOI">10.1145/1273496.1273535</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273535&ftid=425726&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow39" style="display:inline;"><br /><div style="display:inline">Conditional log-linear models are a commonly used method for structured prediction. Efficient learning of parameters in these models is therefore an important problem. This paper describes an exponentiated gradient (EG) algorithm for training such models. ...</div></span>
<span id="toHide39" style="display:none;"><br /><div style="display:inline"><p>Conditional log-linear models are a commonly used method for structured prediction. Efficient learning of parameters in these models is therefore an important problem. This paper describes an exponentiated gradient (EG) algorithm for training such models. EG is applied to the convex dual of the maximum likelihood objective; this results in both sequential and parallel update algorithms, where in the sequential algorithm parameters are updated in an <i>online</i> fashion. We provide a convergence proof for both algorithms. Our analysis also simplifies previous results on EG for max-margin models, and leads to a tighter bound on convergence rates. Experiments on a large-scale parsing task show that the proposed algorithm converges much faster than conjugate-gradient and L-BFGS approaches both in terms of optimization objective and test error.</p></div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273536">Best of both: a hybridized centroid-medoid clustering heuristic</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489002">Nizar Grira</a>,
<a href="author_page.cfm?id=81100299673">Michael E. Houle</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 313-320</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273536" title="DOI">10.1145/1273496.1273536</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273536&ftid=425727&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow40" style="display:inline;"><br /><div style="display:inline">Although each iteration of the popular k-Means clustering heuristic scales well to larger problem sizes, it often requires an unacceptably-high number of iterations to converge to a solution. This paper introduces an enhancement of k-Means ...</div></span>
<span id="toHide40" style="display:none;"><br /><div style="display:inline"><p>Although each iteration of the popular <i>k</i>-Means clustering heuristic scales well to larger problem sizes, it often requires an unacceptably-high number of iterations to converge to a solution. This paper introduces an enhancement of <i>k</i>-Means in which local search is used to accelerate convergence without greatly increasing the average computational cost of the iterations. The local search involves a carefully-controlled number of swap operations resembling those of the more robust <i>k</i>-Medoids clustering heuristic. We show empirically that the proposed method improves convergence results when compared to standard <i>k</i>-Means.</p></div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273537">Recovering temporally rewiring networks: a model-based approach</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489042">Fan Guo</a>,
<a href="author_page.cfm?id=81315488801">Steve Hanneke</a>,
<a href="author_page.cfm?id=81539354956">Wenjie Fu</a>,
<a href="author_page.cfm?id=81407592503">Eric P. Xing</a>
</span>
</td>
</tr>
<tr>
<td></td> 
<td> <span style="padding-left:0">Pages: 321-328</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273537" title="DOI">10.1145/1273496.1273537</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273537&ftid=425728&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow41" style="display:inline;"><br /><div style="display:inline">A plausible representation of relational information among entities in dynamic systems such as a living cell or a social community is a stochastic network which is topologically rewiring and semantically evolving over time. While there is a rich literature ...</div></span>
<span id="toHide41" style="display:none;"><br /><div style="display:inline"><p>A plausible representation of relational information among entities in dynamic systems such as a living cell or a social community is a stochastic network which is topologically rewiring and semantically evolving over time. While there is a rich literature on modeling static or temporally invariant networks, much less has been done toward modeling the dynamic processes underlying rewiring networks, and on recovering such networks when they are not observable. We present a class of <i>hidden temporal exponential random graph models</i> (htERGMs) to study the yet unexplored topic of modeling and recovering temporally rewiring networks from time series of node attributes such as activities of social actors or expression levels of genes. We show that one can reliably infer the latent time-specific topologies of the evolving networks from the observation. We report empirical results on both synthetic data and a <i>Drosophila</i> lifecycle gene expression data set, in comparison with a static counterpart of htERGM.</p></div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273538">Efficient inference with cardinality-based clique potentials</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81337489552">Rahul Gupta</a>,
<a href="author_page.cfm?id=81100202873">Ajit A. Diwan</a>,
<a href="author_page.cfm?id=81100043079">Sunita Sarawagi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 329-336</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273538" title="DOI">10.1145/1273496.1273538</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273538&ftid=425729&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow42" style="display:inline;"><br /><div style="display:inline">Many collective labeling tasks require inference on graphical models where the clique potentials depend only on the number of nodes that get a particular label. We design efficient inference algorithms for various families of such potentials. Our algorithms ...</div></span>
<span id="toHide42" style="display:none;"><br /><div style="display:inline"><p>Many collective labeling tasks require inference on graphical models where the clique potentials depend only on the number of nodes that get a particular label. We design efficient inference algorithms for various families of such potentials. Our algorithms are exact for arbitrary cardinality-based clique potentials on binary labels and for max-like and majority-like clique potentials on multiple labels. Moving towards more complex potentials, we show that inference becomes NP-hard even on cliques with homogeneous Potts potentials. We present a 13/15-approximation algorithm with runtime sub-quadratic in the clique size. In contrast, the best known previous guarantee for graphs with Potts potentials is only 0.5. We perform empirical comparisons on real and synthetic data, and show that our proposed methods are an order of magnitude faster than the well-known Tree-based re-parameterization (TRW) and graph-cut algorithms.</p></div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273539">Sparse probabilistic classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333488795">Romain H&#233;rault</a>,
<a href="author_page.cfm?id=81100345039">Yves Grandvalet</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 337-344</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273539" title="DOI">10.1145/1273496.1273539</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273539&ftid=425730&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow43" style="display:inline;"><br /><div style="display:inline">The scores returned by support vector machines are often used as a confidence measures in the classification of new examples. However, there is no theoretical argument sustaining this practice. Thus, when classification uncertainty has to be assessed, ...</div></span>
<span id="toHide43" style="display:none;"><br /><div style="display:inline"><p>The scores returned by support vector machines are often used as a confidence measures in the classification of new examples. However, there is no theoretical argument sustaining this practice. Thus, when classification uncertainty has to be assessed, it is safer to resort to classifiers estimating conditional probabilities of class labels. Here, we focus on the ambiguity in the vicinity of the boundary decision. We propose an adaptation of maximum likelihood estimation, instantiated on logistic regression. The model outputs proper conditional probabilities into a user-defined interval and is less precise elsewhere. The model is also sparse, in the sense that few examples contribute to the solution. The computational efficiency is thus improved compared to logistic regression. Furthermore, preliminary experiments show improvements over standard logistic regression and performances similar to support vector machines.</p></div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273540">Supervised clustering of streaming data for email batch detection</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81310486073">Peter Haider</a>,
<a href="author_page.cfm?id=81100150852">Ulf Brefeld</a>,
<a href="author_page.cfm?id=81100180901">Tobias Scheffer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 345-352</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273540" title="DOI">10.1145/1273496.1273540</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273540&ftid=425731&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow44" style="display:inline;"><br /><div style="display:inline">We address the problem of detecting batches of emails that have been created according to the same template. This problem is motivated by the desire to filter spam more effectively by exploiting collective information about entire batches of jointly ...</div></span>
<span id="toHide44" style="display:none;"><br /><div style="display:inline"><p>We address the problem of detecting batches of emails that have been created according to the same template. This problem is motivated by the desire to filter spam more effectively by exploiting collective information about entire batches of jointly generated messages. The application matches the problem setting of supervised clustering, because examples of correct clusterings can be collected. Known decoding procedures for supervised clustering are cubic in the number of instances. When decisions cannot be reconsidered once they have been made --- owing to the streaming nature of the data --- then the decoding problem can be solved in linear time. We devise a sequential decoding procedure and derive the corresponding optimization problem of supervised clustering. We study the impact of collective attributes of email batches on the effectiveness of recognizing spam emails.</p></div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273541">A bound on the label complexity of agnostic active learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315488801">Steve Hanneke</a>
</span>
</td>
 </tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 353-360</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273541" title="DOI">10.1145/1273496.1273541</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273541&ftid=425732&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow45" style="display:inline;"><br /><div style="display:inline">We study the label complexity of pool-based active learning in the agnostic PAC model. Specifically, we derive general bounds on the number of label requests made by the A2 algorithm proposed by Balcan, Beygelzimer & Langford (Balcan ...</div></span>
<span id="toHide45" style="display:none;"><br /><div style="display:inline"><p>We study the label complexity of pool-based active learning in the agnostic PAC model. Specifically, we derive general bounds on the number of label requests made by the <i>A</i><sup>2</sup> algorithm proposed by Balcan, Beygelzimer & Langford (Balcan et al., 2006). This represents the first nontrivial general-purpose upper bound on label complexity in the agnostic PAC model.</p></div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273542">Learning nonparametric kernel matrices from pairwise constraints</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100337419">Steven C. H. Hoi</a>,
<a href="author_page.cfm?id=81100054575">Rong Jin</a>,
<a href="author_page.cfm?id=81100033051">Michael R. Lyu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 361-368</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273542" title="DOI">10.1145/1273496.1273542</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273542&ftid=425733&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow46" style="display:inline;"><br /><div style="display:inline">Many kernel learning methods have to assume parametric forms for the target kernel functions, which significantly limits the capability of kernels in fitting diverse patterns. Some kernel learning methods assume the target kernel matrix to be a linear ...</div></span>
<span id="toHide46" style="display:none;"><br /><div style="display:inline"><p>Many kernel learning methods have to assume parametric forms for the target kernel functions, which significantly limits the capability of kernels in fitting diverse patterns. Some kernel learning methods assume the target kernel matrix to be a linear combination of parametric kernel matrices. This assumption again importantly limits the flexibility of the target kernel matrices. The key challenge with nonparametric kernel learning arises from the difficulty in linking the nonparametric kernels to the input patterns. In this paper, we resolve this problem by introducing the graph Laplacian of the observed data as a regularizer when optimizing the kernel matrix with respect to the pairwise constraints. We formulate the problem into Semi-Definite Programs (SDP), and propose an efficient algorithm to solve the SDP problem. The extensive evaluation on clustering with pairwise constraints shows that the proposed nonparametric kernel learning method is more effective than other state-of-the-art kernel learning techniques.</p></div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273543">Parameter learning for relational Bayesian networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100654844">Manfred Jaeger</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 369-376</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273543" title="DOI">10.1145/1273496.1273543</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273543&ftid=425734&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow47" style="display:inline;"><br /><div style="display:inline">We present a method for parameter learning in relational Bayesian networks (RBNs). Our approach consists of compiling the RBN model into a computation graph for the likelihood function, and to use this likelihood graph to perform the necessary computations ...</div></span>
<span id="toHide47" style="display:none;"><br /><div style="display:inline"><p>We present a method for parameter learning in relational Bayesian networks (RBNs). Our approach consists of compiling the RBN model into a computation graph for the likelihood function, and to use this likelihood graph to perform the necessary computations for a gradient ascent likelihood optimization procedure. The method can be applied to all RBN models that only contain differentiable combining rules. This includes models with non-decomposable combining rules, as well as models with weighted combinations or nested occurrences of combining rules. Experimental results on artificial random graph data explores the feasibility of the approach both for complete and incomplete data.</p></div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273544">Bayesian compressive sensing and projection optimization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81310484748">Shihao Ji</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 377-384</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273544" title="DOI">10.1145/1273496.1273544</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273544&ftid=425735&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow48" style="display:inline;"><br /><div style="display:inline">This paper introduces a new problem for which machine-learning tools may make an impact. The problem considered is termed "compressive sensing", in which a real signal of dimension N is measured accurately based on K &lt;&lt; N real measurements. ...</div></span>
<span id="toHide48" style="display:none;"><br /><div style="display:inline"><p>This paper introduces a new problem for which machine-learning tools may make an impact. The problem considered is termed "compressive sensing", in which a real signal of dimension <i>N</i> is measured accurately based on <i>K &lt;&lt; N</i> real measurements. This is achieved under the assumption that the underlying signal has a sparse representation in some basis (e.g., wavelets). In this paper we demonstrate how techniques developed in machine learning, specifically sparse Bayesian regression and active learning, may be leveraged to this new problem. We also point out future research directions in compressive sensing of interest to the machine-learning community.</p></div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273545">Constructing basis functions from directed graphs for value function approximation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489301">Jeff Johns</a>,
<a href="author_page.cfm?id=81100132345">Sridhar Mahadevan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 385-392</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273545" title="DOI">10.1145/1273496.1273545</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273545&ftid=425736&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow49" style="display:inline;"><br /><div style="display:inline">Basis functions derived from an undirected graph connecting nearby samples from a Markov decision process (MDP) have proven useful for approximating value functions. The success of this technique is attributed to the smoothness of the basis functions ...</div></span>
<span id="toHide49" style="display:none;"><br /><div style="display:inline"><p>Basis functions derived from an undirected graph connecting nearby samples from a Markov decision process (MDP) have proven useful for approximating value functions. The success of this technique is attributed to the smoothness of the basis functions with respect to the state space geometry. This paper explores the properties of bases created from <i>directed</i> graphs which are a more natural fit for expressing state connectivity. Digraphs capture the effect of non-reversible MDPs whose value functions may not be smooth across adjacent states. We provide an analysis using the Dirichlet sum of the directed graph Laplacian to show how the smoothness of the basis functions is affected by the graph's invariant distribution. Experiments in discrete and continuous MDPs with non-reversible actions demonstrate a significant improvement in the policies learned using directed graph bases.</p></div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273546">Most likely heteroscedastic Gaussian process regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81337490510">Kristian Kersting</a>,
<a href="author_page.cfm?id=81381593564">Christian Plagemann</a>,
<a href="author_page.cfm?id=81324492561">Patrick Pfaff</a>,
<a href="author_page.cfm?id=81100565843">Wolfram Burgard</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 393-400</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273546" title="DOI">10.1145/1273496.1273546</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273546&ftid=425737&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow50" style="display:inline;"><br /><div style="display:inline">This paper presents a novel Gaussian process (GP) approach to regression with input-dependent noise rates. We follow Goldberg et al.'s approach and model the noise variance using a second GP in addition to the GP governing the noise-free output value. ...</div></span>
<span id="toHide50" style="display:none;"><br /><div style="display:inline"><p>This paper presents a novel Gaussian process (GP) approach to regression with input-dependent noise rates. We follow Goldberg et al.'s approach and model the noise variance using a second GP in addition to the GP governing the noise-free output value. In contrast to Goldberg et al., however, we do not use a Markov chain Monte Carlo method to approximate the posterior noise variance but a most likely noise approach. The resulting model is easy to implement and can directly be used in combination with various existing extensions of the standard GPs such as sparse approximations. Extensive experiments on both synthetic and real-world data, including a challenging perception problem in robotics, show the effectiveness of most likely heteroscedastic GP regression.</p></div></span> <a id="expcoll50" href="JavaScript: expandcollapse('expcoll50',50)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273547">Neighbor search with global geometry: a minimax message passing algorithm</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489267">Kye-Hyeon Kim</a>,
<a href="author_page.cfm?id=81452599277">Seungjin Choi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 401-408</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273547" title="DOI">10.1145/1273496.1273547</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273547&ftid=425738&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow51" style="display:inline;"><br /><div style="display:inline">Neighbor search is a fundamental task in machine learning, especially in classification and retrieval. Efficient nearest neighbor search methods have been widely studied, with their emphasis on data structures but most of them did not consider the underlying ...</div></span>
<span id="toHide51" style="display:none;"><br /><div style="display:inline"><p>Neighbor search is a fundamental task in machine learning, especially in classification and retrieval. Efficient nearest neighbor search methods have been widely studied, with their emphasis on data structures but most of them did not consider the underlying global geometry of a data set. Recent graph-based semi-supervised learning methods capture the global geometry, but suffer from scalability and parameter tuning problems. In this paper we present a (nearest) neighbor search method where the underlying global geometry is incorporated and the parameter tuning is not required. To this end, we introduce <i>deterministic walks</i> as a deterministic counterpart of Markov random walks, leading us to use the minimax distance as a global dissimilarity measure. Then we develop a message passing algorithm for efficient minimax distance calculation, which scales linearly in both time and space. Empirical study reveals the useful behavior of the method in image retrieval and semi-supervised learning.</p></div></span> <a id="expcoll51" href="JavaScript: expandcollapse('expcoll51',51)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273548">A recursive method for discriminative mixture learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317495337">Minyoung Kim</a>,
<a href="author_page.cfm?id=81100453089">Vladimir Pavlovic</a>
</span>
 </td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 409-416</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273548" title="DOI">10.1145/1273496.1273548</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273548&ftid=425739&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow52" style="display:inline;"><br /><div style="display:inline">We consider the problem of learning density mixture models for classification. Traditional learning of mixtures for density estimation focuses on models that correctly represent the density at all points in the sample space. Discriminative learning, ...</div></span>
<span id="toHide52" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of learning density mixture models for classification. Traditional learning of mixtures for density estimation focuses on models that correctly represent the density at all points in the sample space. Discriminative learning, on the other hand, aims at representing the density at the decision boundary. We introduce a novel discriminative learning method for mixtures of generative models. Unlike traditional discriminative learning methods that often resort to computationally demanding gradient search optimization, the proposed method is highly efficient as it reduces to generative learning of individual mixture components on weighted data. Hence it is particularly suited to domains with complex component models, such as hidden Markov models or Bayesian networks in general, that are usually too complex for effective gradient search. We demonstrate the benefits of the proposed method in a comprehensive set of evaluations on time-series sequence classification problems.</p></div></span> <a id="expcoll52" href="JavaScript: expandcollapse('expcoll52',52)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273549">Infinite mixtures of trees</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100279386">Sergey Kirshner</a>,
<a href="author_page.cfm?id=81100305321">Padhraic Smyth</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 417-423</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273549" title="DOI">10.1145/1273496.1273549</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273549&ftid=425740&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
 <span id="toShow53" style="display:inline;"><br /><div style="display:inline">Finite mixtures of tree-structured distributions have been shown to be efficient and effective in modeling multivariate distributions. Using Dirichlet processes, we extend this approach to allow countably many tree-structured mixture components. The ...</div></span>
<span id="toHide53" style="display:none;"><br /><div style="display:inline"><p>Finite mixtures of tree-structured distributions have been shown to be efficient and effective in modeling multivariate distributions. Using Dirichlet processes, we extend this approach to allow countably many tree-structured mixture components. The resulting Bayesian framework allows us to deal with the problem of selecting the number of mixture components by computing the posterior distribution over the number of components and integrating out the components by Bayesian model averaging. We apply the proposed framework to identify the number and the properties of predominant precipitation patterns in historical archives of climate data.</p></div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273550">Local dependent components</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100103324">Arto Klami</a>,
<a href="author_page.cfm?id=81100348810">Samuel Kaski</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 425-432</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273550" title="DOI">10.1145/1273496.1273550</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273550&ftid=425741&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow54" style="display:inline;"><br /><div style="display:inline">We introduce a mixture of probabilistic canonical correlation analyzers model for analyzing local correlations, or more generally mutual statistical dependencies, in cooccurring data pairs. The model extends the traditional canonical correlation analysis ...</div></span>
<span id="toHide54" style="display:none;"><br /><div style="display:inline"><p>We introduce a mixture of probabilistic canonical correlation analyzers model for analyzing local correlations, or more generally mutual statistical dependencies, in cooccurring data pairs. The model extends the traditional canonical correlation analysis and its probabilistic interpretation in three main ways. First, a full Bayesian treatment enables analysis of small samples (large <i>p</i>, small <i>n</i>, a crucial problem in bioinformatics, for instance), and rigorous estimation of the degree of dependency and independency. Secondly, the mixture formulation generalizes the method from global linearity to the more reasonable assumption of different kinds of dependencies for different kinds of data. As a third novel extension the method decomposes the variation in the data into shared and data set-specific components.</p></div></span> <a id="expcoll54" href="JavaScript: expandcollapse('expcoll54',54)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273551">Statistical predicate invention</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309513190">Stanley Kok</a>,
<a href="author_page.cfm?id=81100205908">Pedro Domingos</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 433-440</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273551" title="DOI">10.1145/1273496.1273551</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273551&ftid=425742&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow55" style="display:inline;"><br /><div style="display:inline">We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models ...</div></span>
<span id="toHide55" style="display:none;"><br /><div style="display:inline"><p>We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.</p></div></span> <a id="expcoll55" href="JavaScript: expandcollapse('expcoll55',55)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273552">Kernelizing PLS, degrees of freedom, and efficient model selection</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81485650471">Nicole Kr&#228;mer</a>,
<a href="author_page.cfm?id=81339491463">Mikio L. Braun</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 441-448</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273552" title="DOI">10.1145/1273496.1273552</a></span></td>
</tr>
<tr>
 <td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273552&ftid=425743&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow56" style="display:inline;"><br /><div style="display:inline">Kernelizing partial least squares (PLS), an algorithm which has been particularly popular in chemometrics, leads to kernel PLS which has several interesting properties, including a sub-cubic runtime for learning, and an iterative construction of directions ...</div></span>
<span id="toHide56" style="display:none;"><br /><div style="display:inline"><p>Kernelizing partial least squares (PLS), an algorithm which has been particularly popular in chemometrics, leads to kernel PLS which has several interesting properties, including a sub-cubic runtime for learning, and an iterative construction of directions which are relevant for predicting the outputs. We show that the kernelization of PLS introduces interesting properties not found in ordinary PLS, giving novel insights into the workings of kernel PLS and the connections to kernel ridge regression and conjugate gradient descent methods. Furthermore, we show how to correctly define the degrees of freedom for kernel PLS and how to efficiently compute an unbiased estimate. Finally, we address the practical problem of model selection. We demonstrate how to use the degrees of freedom estimate to perform effective model selection, and discuss how to implement crossvalidation schemes efficiently.</p></div></span> <a id="expcoll56" href="JavaScript: expandcollapse('expcoll56',56)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273553">Nonmyopic active learning of Gaussian processes: an exploration-exploitation approach</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100460663">Andreas Krause</a>,
<a href="author_page.cfm?id=81100629945">Carlos Guestrin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 449-456</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273553" title="DOI">10.1145/1273496.1273553</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273553&ftid=425744&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow57" style="display:inline;"><br /><div style="display:inline">When monitoring spatial phenomena, such as the ecological condition of a river, deciding where to make observations is a challenging task. In these settings, a fundamental question is when an active learning, or sequential design, strategy, where locations ...</div></span>
<span id="toHide57" style="display:none;"><br /><div style="display:inline"><p>When monitoring spatial phenomena, such as the ecological condition of a river, deciding where to make observations is a challenging task. In these settings, a fundamental question is when an active learning, or sequential design, strategy, where locations are selected based on previous measurements, will perform significantly better than sensing at an a priori specified set of locations. For Gaussian Processes (GPs), which often accurately model spatial phenomena, we present an analysis and efficient algorithms that address this question. Central to our analysis is a theoretical bound which quantifies the performance difference between active and a priori design strategies. We consider GPs with unknown kernel parameters and present a nonmyopic approach for trading off exploration, i.e., decreasing uncertainty about the model parameters, and exploitation, i.e., near-optimally selecting observations when the parameters are (approximately) known. We discuss several exploration strategies, and present logarithmic sample complexity bounds for the exploration phase. We then extend our algorithm to handle nonstationary GPs exploiting local structure in the model. We also present extensive empirical evaluation on several real-world problems.</p></div></span> <a id="expcoll57" href="JavaScript: expandcollapse('expcoll57',57)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273554">On one method of non-diagonal regularization in sparse Bayesian learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81381596903">Dmitry Kropotov</a>,
<a href="author_page.cfm?id=81319503116">Dmitry Vetrov</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 457-464</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273554" title="DOI">10.1145/1273496.1273554</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273554&ftid=425745&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow58" style="display:inline;"><br /><div style="display:inline">In the paper we propose a new type of regularization procedure for training sparse Bayesian methods for classification. Transforming Hessian matrix of log-likelihood function to diagonal form with further regularization of its eigenvectors allows us ...</div></span>
<span id="toHide58" style="display:none;"><br /><div style="display:inline"><p>In the paper we propose a new type of regularization procedure for training sparse Bayesian methods for classification. Transforming Hessian matrix of log-likelihood function to diagonal form with further regularization of its eigenvectors allows us to optimize evidence explicitly as a product of one-dimensional integrals. The process of automatic regularization coefficients determination then converges in one iteration. We show how to use the proposed approach for Gaussian and Laplace priors. Both algorithms show comparable performance with the state-of-the-art Relevance Vector Machines (RVM) but require less time for training and produce more sparse decision rules (in terms of degrees of freedom).</p></div></span> <a id="expcoll58" href="JavaScript: expandcollapse('expcoll58',58)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273555">Online kernel PCA with entropic matrix updates</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489486">Dima Kuzmin</a>,
<a href="author_page.cfm?id=81100102963">Manfred K. Warmuth</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 465-472</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273555" title="DOI">10.1145/1273496.1273555</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273555&ftid=425746&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow59" style="display:inline;"><br /><div style="display:inline">A number of updates for density matrices have been developed recently that are motivated by relative entropy minimization problems. The updates involve a softmin calculation based on matrix logs and matrix exponentials. We show that these updates can ...</div></span>
<span id="toHide59" style="display:none;"><br /><div style="display:inline"><p>A number of updates for density matrices have been developed recently that are motivated by relative entropy minimization problems. The updates involve a softmin calculation based on matrix logs and matrix exponentials. We show that these updates can be kernelized. This is important because the bounds provable for these algorithms are logarithmic in the feature dimension (provided that the 2-norm of feature vectors is bounded by a constant). The main problem we focus on is the kernelization of an online PCA algorithm which belongs to this family of updates.</p></div></span> <a id="expcoll59" href="JavaScript: expandcollapse('expcoll59',59)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273556">An empirical evaluation of deep architectures on problems with many factors of variation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81319495662">Hugo Larochelle</a>,
<a href="author_page.cfm?id=81320489503">Dumitru Erhan</a>,
<a href="author_page.cfm?id=81100464337">Aaron Courville</a>,
<a href="author_page.cfm?id=81320488191">James Bergstra</a>,
<a href="author_page.cfm?id=81100287057">Yoshua Bengio</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 473-480</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273556" title="DOI">10.1145/1273496.1273556</a></span></td>
</tr>
<tr>
<td></td>
<td>
 <span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273556&ftid=425747&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow60" style="display:inline;"><br /><div style="display:inline">Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in ...</div></span>
<span id="toHide60" style="display:none;"><br /><div style="display:inline"><p>Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.</p></div></span> <a id="expcoll60" href="JavaScript: expandcollapse('expcoll60',60)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273557">Hierarchical Gaussian process latent variable models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100574233">Neil D. Lawrence</a>,
<a href="author_page.cfm?id=81333490402">Andrew J. Moore</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 481-488</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273557" title="DOI">10.1145/1273496.1273557</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273557&ftid=425748&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow61" style="display:inline;"><br /><div style="display:inline">The Gaussian process latent variable model (GP-LVM) is a powerful approach for probabilistic modelling of high dimensional data through dimensional reduction. In this paper we extend the GP-LVM through hierarchies. A hierarchical model (such as a tree) ...</div></span>
<span id="toHide61" style="display:none;"><br /><div style="display:inline"><p>The Gaussian process latent variable model (GP-LVM) is a powerful approach for probabilistic modelling of high dimensional data through dimensional reduction. In this paper we extend the GP-LVM through hierarchies. A hierarchical model (such as a tree) allows us to express conditional independencies in the data as well as the manifold structure. We first introduce Gaussian process hierarchies through a simple dynamical model, we then extend the approach to a more complex hierarchy which is applied to the visualisation of human motion data sets.</p></div></span> <a id="expcoll61" href="JavaScript: expandcollapse('expcoll61',61)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273558">Learning a meta-level prior for feature relevance from multiple related tasks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100411172">Su-In Lee</a>,
<a href="author_page.cfm?id=81100204858">Vassil Chatalbashev</a>,
<a href="author_page.cfm?id=81100310298">David Vickrey</a>,
<a href="author_page.cfm?id=81100246010">Daphne Koller</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 489-496</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273558" title="DOI">10.1145/1273496.1273558</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273558&ftid=425749&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow62" style="display:inline;"><br /><div style="display:inline">In many prediction tasks, selecting relevant features is essential for achieving good generalization performance. Most feature selection algorithms consider all features to be a priori equally likely to be relevant. In this paper, we use transfer learning---learning ...</div></span>
<span id="toHide62" style="display:none;"><br /><div style="display:inline"><p>In many prediction tasks, selecting relevant features is essential for achieving good generalization performance. Most feature selection algorithms consider all features to be a priori equally likely to be relevant. In this paper, we use transfer learning---learning on an ensemble of related tasks---to construct an informative prior on feature relevance. We assume that features themselves have meta-features that are predictive of their relevance to the prediction task, and model their relevance as a function of the meta-features using hyperparameters (called <i>meta-priors</i>). We present a convex optimization algorithm for simultaneously learning the meta-priors and feature weights from an ensemble of related prediction tasks which share a similar relevance structure. Our approach transfers the "meta-priors" among different tasks, which makes it possible to deal with settings where tasks have nonoverlapping features or the relevance of the features vary over the tasks. We show that learning feature relevance improves performance on two real data sets which illustrate such settings: (1) predicting ratings in a collaborative filtering task, and (2) distinguishing arguments of a verb in a sentence.</p></div></span> <a id="expcoll62" href="JavaScript: expandcollapse('expcoll62',62)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273559">Scalable modeling of real graphs using Kronecker multiplication</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100096826">Jure Leskovec</a>,
<a href="author_page.cfm?id=81100373169">Christos Faloutsos</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 497-504</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273559" title="DOI">10.1145/1273496.1273559</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273559&ftid=425750&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow63" style="display:inline;"><br /><div style="display:inline">Given a large, real graph, how can we generate a synthetic graph that matches its properties, i.e., it has similar degree distribution, similar (small) diameter, similar spectrum, etc? We propose to use "Kronecker graphs", which naturally obey ...</div></span>
<span id="toHide63" style="display:none;"><br /><div style="display:inline"><p>Given a large, real graph, how can we generate a synthetic graph that matches its properties, <i>i.e.</i>, it has similar degree distribution, similar (small) diameter, similar spectrum, etc? We propose to use "Kronecker graphs", which naturally obey all of the above properties, and we present KronFit, a fast and scalable algorithm for fitting the Kronecker graph generation model to real networks. A naive approach to fitting would take super-exponential time. In contrast, KronFit takes <i>linear</i> time, by exploiting the structure of Kronecker product and by using sampling. Experiments on large real and synthetic graphs show that KronFit indeed mimics very well the patterns found in the target graphs. Once fitted, the model parameters and the resulting synthetic graphs can be used for anonymization, extrapolations, and graph summarization.</p></div></span> <a id="expcoll63" href="JavaScript: expandcollapse('expcoll63',63)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273560">Support cluster machine</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81350587874">Bin Li</a>,
<a href="author_page.cfm?id=81333487902">Mingmin Chi</a>,
<a href="author_page.cfm?id=81455605372">Jianping Fan</a>,
<a href="author_page.cfm?id=81100142607">Xiangyang Xue</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 505-512</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273560" title="DOI">10.1145/1273496.1273560</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273560&ftid=425751&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow64" style="display:inline;"><br /><div style="display:inline">For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, ...</div></span>
<span id="toHide64" style="display:none;"><br /><div style="display:inline"><p>For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.</p></div></span> <a id="expcoll64" href="JavaScript: expandcollapse('expcoll64',64)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273561">A transductive framework of distance metric learning by spectral dimensionality reduction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81414609899">Fuxin Li</a>,
<a href="author_page.cfm?id=81361591444">Jian Yang</a>,
<a href="author_page.cfm?id=81100233470">Jue Wang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 513-520</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273561" title="DOI">10.1145/1273496.1273561</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273561&ftid=425752&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow65" style="display:inline;"><br /><div style="display:inline">Distance metric learning and nonlinear dimensionality reduction are two interesting and active topics in recent years. However, the connection between them is not thoroughly studied yet. In this paper, a transductive framework of distance metric learning ...</div></span>
<span id="toHide65" style="display:none;"><br /><div style="display:inline"><p>Distance metric learning and nonlinear dimensionality reduction are two interesting and active topics in recent years. However, the connection between them is not thoroughly studied yet. In this paper, a transductive framework of distance metric learning is proposed and its close connection with many nonlinear spectral dimensionality reduction methods is elaborated. Furthermore, we prove a representer theorem for our framework, linking it with function estimation in an RKHS, and making it possible for generalization to unseen test samples. In our framework, it suffices to solve a sparse eigenvalue problem, thus datasets with 10<sup>5</sup> samples can be handled. Finally, experiment results on synthetic data, several UCI databases and the MNIST handwritten digit database are shown.</p></div></span> <a id="expcoll65" href="JavaScript: expandcollapse('expcoll65',65)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273562">Adaptive dimension reduction using discriminant analysis and <i>K</i>-means clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100136610">Chris Ding</a>,
<a href="author_page.cfm?id=81100475528">Tao Li</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 521-528</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273562" title="DOI">10.1145/1273496.1273562</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273562&ftid=425753&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow66" style="display:inline;"><br /><div style="display:inline">We combine linear discriminant analysis (LDA) and K-means clustering into a coherent framework to adaptively select the most discriminative subspace. We use K-means clustering to generate class labels and use LDA to do subspace selection. ...</div></span>
<span id="toHide66" style="display:none;"><br /><div style="display:inline"><p>We combine linear discriminant analysis (LDA) and <i>K</i>-means clustering into a coherent framework to adaptively select the most discriminative subspace. We use <i>K</i>-means clustering to generate class labels and use LDA to do subspace selection. The clustering process is thus integrated with the subspace selection process and the data are then simultaneously clustered while the feature subspaces are selected. We show the rich structure of the general LDA-Km framework by examining its variants and their relationships to earlier approaches. Relations among PCA, LDA, <i>K</i>-means are clarified. Extensive experimental results on real-world datasets show the effectiveness of our approach.</p></div></span> <a id="expcoll66" href="JavaScript: expandcollapse('expcoll66',66)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273563">Large-scale RLSC learning without agony</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489647">Wenye Li</a>,
<a href="author_page.cfm?id=81409592806">Kin-Hong Lee</a>,
<a href="author_page.cfm?id=81451595717">Kwong-Sak Leung</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 529-536</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273563" title="DOI">10.1145/1273496.1273563</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273563&ftid=425754&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow67" style="display:inline;"><br /><div style="display:inline">The advances in kernel-based learning necessitate the study on solving a large-scale non-sparse positive definite linear system. To provide a deterministic approach, recent researches focus on designing fast matrix-vector multiplication techniques coupled ...</div></span>
<span id="toHide67" style="display:none;"><br /><div style="display:inline"><p>The advances in kernel-based learning necessitate the study on solving a large-scale non-sparse positive definite linear system. To provide a deterministic approach, recent researches focus on designing fast matrix-vector multiplication techniques coupled with a conjugate gradient method. Instead of using the conjugate gradient method, our paper proposes to use a domain decomposition approach in solving such a linear system. Its convergence property and speed can be understood within von Neumann's alternating projection framework. We will report signi ficant and consistent improvements in convergence speed over the conjugate gradient method when the approach is applied to recent machine learning problems.</p></div></span> <a id="expcoll67" href="JavaScript: expandcollapse('expcoll67',67)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273564">A novel orthogonal NMF-based belief compression for POMDPs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81414606102">Xin Li</a>,
<a href="author_page.cfm?id=81416597690">William K. W. Cheung</a>,
<a href="author_page.cfm?id=81408596722">Jiming Liu</a>,
<a href="author_page.cfm?id=81452592497">Zhili Wu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 537-544</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273564" title="DOI">10.1145/1273496.1273564</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273564&ftid=425755&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;"> 
<div style="padding-left:0">
<span id="toShow68" style="display:inline;"><br /><div style="display:inline">High dimensionality of POMDP's belief state space is one major cause that makes the underlying optimal policy computation intractable. Belief compression refers to the methodology that projects the belief state space to a low-dimensional one to alleviate ...</div></span>
<span id="toHide68" style="display:none;"><br /><div style="display:inline"><p>High dimensionality of POMDP's belief state space is one major cause that makes the underlying optimal policy computation intractable. Belief compression refers to the methodology that projects the belief state space to a low-dimensional one to alleviate the problem. In this paper, we propose a novel orthogonal non-negative matrix factorization (<i>O</i>-NMF) for the projection. The proposed <i>O</i>-NMF not only factors the belief state space by minimizing the reconstruction error, but also allows the compressed POMDP formulation to be efficiently computed (due to its orthogonality) in a value-directed manner so that the value function will take same values for corresponding belief states in the original and compressed state spaces. We have tested the proposed approach using a number of benchmark problems and the empirical results confirms its effectiveness in achieving substantial computational cost saving in policy computation.</p></div></span> <a id="expcoll68" href="JavaScript: expandcollapse('expcoll68',68)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273565">A permutation-augmented sampler for DP mixture models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323492904">Percy Liang</a>,
<a href="author_page.cfm?id=81339507945">Michael I. Jordan</a>,
<a href="author_page.cfm?id=81100453688">Ben Taskar</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 545-552</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273565" title="DOI">10.1145/1273496.1273565</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273565&ftid=425756&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow69" style="display:inline;"><br /><div style="display:inline">We introduce a new inference algorithm for Dirichlet process mixture models. While Gibbs sampling and variational methods focus on local moves, the new algorithm makes more global moves. This is done by introducing a permutation of the data points as ...</div></span>
<span id="toHide69" style="display:none;"><br /><div style="display:inline"><p>We introduce a new inference algorithm for Dirichlet process mixture models. While Gibbs sampling and variational methods focus on local moves, the new algorithm makes more global moves. This is done by introducing a permutation of the data points as an auxiliary variable. The algorithm is a blocked sampler which alternates between sampling the clustering and sampling the permutation. The key to the efficiency of this approach is that it is possible to use dynamic programming to consider all exponentially many clusterings consistent with a given permutation. We also show that random projections can be used to effectively sample the permutation. The result is a stochastic hill-climbing algorithm that yields burn-in times significantly smaller than those of collapsed Gibbs sampling.</p></div></span> <a id="expcoll69" href="JavaScript: expandcollapse('expcoll69',69)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273566">Quadratically gated mixture of experts for incomplete data classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309499575">Xuejun Liao</a>,
<a href="author_page.cfm?id=81315489927">Hui Li</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 553-560</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273566" title="DOI">10.1145/1273496.1273566</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273566&ftid=425757&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow70" style="display:inline;"><br /><div style="display:inline">We introduce quadratically gated mixture of experts (QGME), a statistical model for multi-class nonlinear classification. The QGME is formulated in the setting of incomplete data, where the data values are partially observed. We show that the ...</div></span>
<span id="toHide70" style="display:none;"><br /><div style="display:inline"><p>We introduce <i>quadratically gated mixture of experts</i> (QGME), a statistical model for multi-class nonlinear classification. The QGME is formulated in the setting of incomplete data, where the data values are partially observed. We show that the missing values entail joint estimation of the data manifold and the classifier, which allows <i>adaptive</i> imputation during classifier learning. The expectation maximization (EM) algorithm is derived for joint likelihood maximization, with adaptive imputation performed analytically in the E-step. The performance of QGME is evaluated on three benchmark data sets and the results show that the QGME yields significant improvements over competing methods.</p></div></span> <a id="expcoll70" href="JavaScript: expandcollapse('expcoll70',70)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273567">Trust region Newton methods for large-scale logistic regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384612017">Chih-Jen Lin</a>,
<a href="author_page.cfm?id=81330500504">Ruby C. Weng</a>,
<a href="author_page.cfm?id=81100176197">S. Sathiya Keerthi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 561-568</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273567" title="DOI">10.1145/1273496.1273567</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273567&ftid=425758&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow71" style="display:inline;"><br /><div style="display:inline">Large-scale logistic regression arises in many applications such as document classification and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed ...</div></span>
<span id="toHide71" style="display:none;"><br /><div style="display:inline"><p>Large-scale logistic regression arises in many applications such as document classification and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also compare it with linear SVM implementations.</p></div></span> <a id="expcoll71" href="JavaScript: expandcollapse('expcoll71',71)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273568">Relational clustering by symmetric convex coding</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81375607652">Bo Long</a>,
<a href="author_page.cfm?id=81451593853">Zhongfei (Mark) Zhang</a>,
<a href="author_page.cfm?id=81452596593">Xiaoyun Wu</a>,
<a href="author_page.cfm?id=81350576309">Philip S. Yu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 569-576</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273568" title="DOI">10.1145/1273496.1273568</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273568&ftid=425759&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow72" style="display:inline;"><br /><div style="display:inline">Relational data appear frequently in many machine learning applications. Relational data consist of the pairwise relations (similarities or dissimilarities) between each pair of implicit objects, and are usually stored in relation matrices and typically ...</div></span>
<span id="toHide72" style="display:none;"><br /><div style="display:inline"><p>Relational data appear frequently in many machine learning applications. Relational data consist of the pairwise relations (similarities or dissimilarities) between each pair of implicit objects, and are usually stored in relation matrices and typically no other knowledge is available. Although relational clustering can be formulated as graph partitioning in some applications, this formulation is not adequate for general relational data. In this paper, we propose a general model for relational clustering based on symmetric convex coding. The model is applicable to all types of relational data and unifies the existing graph partitioning formulation. Under this model, we derive two alternative bound optimization algorithms to solve the symmetric convex coding under two popular distance functions, Euclidean distance and generalized I-divergence. Experimental evaluation and theoretical analysis show the effectiveness and great potential of the proposed model and algorithms.</p></div></span> <a id="expcoll72" href="JavaScript: expandcollapse('expcoll72',72)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273569">Discriminant analysis in correlation similarity measure space</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81319496532">Yong Ma</a>,
<a href="author_page.cfm?id=81100483727">Shihong Lao</a>,
<a href="author_page.cfm?id=81320495568">Erina Takikawa</a>,
<a href="author_page.cfm?id=81320491155">Masato Kawade</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 577-584</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273569" title="DOI">10.1145/1273496.1273569</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273569&ftid=425760&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow73" style="display:inline;"><br /><div style="display:inline">Correlation is one of the most widely used similarity measures in machine learning like Euclidean and Mahalanobis distances. However, compared with proposed numerous discriminant learning algorithms in distance metric space, only a very little work has ...</div></span>
<span id="toHide73" style="display:none;"><br /><div style="display:inline"><p>Correlation is one of the most widely used similarity measures in machine learning like Euclidean and Mahalanobis distances. However, compared with proposed numerous discriminant learning algorithms in distance metric space, only a very little work has been conducted on this topic using correlation similarity measure. In this paper, we propose a novel discriminant learning algorithm in correlation measure space, Correlation Discriminant Analysis (CDA). In this framework, based on the definitions of within-class correlation and between-class correlation, the optimum transformation can be sought for to maximize the difference between them, which is in accordance with good classification performance empirically. Under different cases of the transformation, different implementations of the algorithm are given. Extensive empirical evaluations of CDA demonstrate its advantage over alternative methods.</p></div></span> <a id="expcoll73" href="JavaScript: expandcollapse('expcoll73',73)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273570">Adaptive mesh compression in 3D computer graphics using multiscale manifold learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100132345">Sridhar Mahadevan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 585-592</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273570" title="DOI">10.1145/1273496.1273570</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273570&ftid=425761&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow74" style="display:inline;"><br /><div style="display:inline">This paper investigates compression of 3D objects in computer graphics using manifold learning. Spectral compression uses the eigenvectors of the graph Laplacian of an object's topology to adaptively compress 3D objects. 3D compression is a challenging ...</div></span>
<span id="toHide74" style="display:none;"><br /><div style="display:inline"><p>This paper investigates compression of 3D objects in computer graphics using manifold learning. <i>Spectral compression</i> uses the eigenvectors of the graph Laplacian of an object's topology to adaptively compress 3D objects. 3D compression is a challenging application domain: object models can have &gt; 10<sup>5</sup> vertices, and reliably computing the basis functions on large graphs is numerically challenging. In this paper, we introduce a novel multiscale manifold learning approach to 3D mesh compression using <i>diffusion wavelets</i>, a general extension of wavelets to graphs with arbitrary topology. Unlike the "global" nature of Laplacian bases, diffusion wavelet bases are compact, and multiscale in nature. We decompose large graphs using a fast graph partitioning method, and combine local multiscale wavelet bases computed on each subgraph. We present results showing that multiscale diffusion wavelets bases are superior to the Laplacian bases for adaptive compression of large 3D objects.</p></div></span> <a id="expcoll74" href="JavaScript: expandcollapse('expcoll74',74)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273571">Simple, robust, scalable semi-supervised learning via expectation regularization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81540463356">Gideon S. Mann</a>,
<a href="author_page.cfm?id=81100553872">Andrew McCallum</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 593-600</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273571" title="DOI">10.1145/1273496.1273571</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273571&ftid=425762&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow75" style="display:inline;"><br /><div style="display:inline">Although semi-supervised learning has been an active area of research, its use in deployed applications is still relatively rare because the methods are often difficult to implement, fragile in tuning, or lacking in scalability. This paper presents expectation ...</div></span>
<span id="toHide75" style="display:none;"><br /><div style="display:inline"><p>Although semi-supervised learning has been an active area of research, its use in deployed applications is still relatively rare because the methods are often difficult to implement, fragile in tuning, or lacking in scalability. This paper presents <i>expectation regularization</i>, a semi-supervised learning method for exponential family parametric models that augments the traditional conditional label-likelihood objective function with an additional term that encourages model predictions on unlabeled data to match certain expectations---such as label priors. The method is extremely easy to implement, scales as well as logistic regression, and can handle non-independent features. We present experiments on five different data sets, showing accuracy improvements over other semi-supervised methods.</p></div></span> <a id="expcoll75" href="JavaScript: expandcollapse('expcoll75',75)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273572">Automatic shaping and decomposition of reward functions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333490008">Bhaskara Marthi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 601-608</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273572" title="DOI">10.1145/1273496.1273572</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273572&ftid=425763&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow76" style="display:inline;"><br /><div style="display:inline">This paper investigates the problem of automatically learning how to restructure the reward function of a Markov decision process so as to speed up reinforcement learning. We begin by describing a method that learns a shaped reward function given a set ...</div></span>
<span id="toHide76" style="display:none;"><br /><div style="display:inline"><p>This paper investigates the problem of automatically learning how to restructure the reward function of a Markov decision process so as to speed up reinforcement learning. We begin by describing a method that learns a shaped reward function given a set of state and temporal abstractions. Next, we consider decomposition of the per-timestep reward in multieffector problems, in which the overall agent can be decomposed into multiple units that are concurrently carrying out various tasks. We show by example that to find a good reward decomposition, it is often necessary to first shape the rewards appropriately. We then give a function approximation algorithm for solving both problems together. Standard reinforcement learning algorithms can be augmented with our methods, and we show experimentally that in each case, significantly faster learning results.</p></div></span> <a id="expcoll76" href="JavaScript: expandcollapse('expcoll76',76)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273573">Asymmetric boosting</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489809">Hamed Masnadi-Shirazi</a>,
<a href="author_page.cfm?id=81100239272">Nuno Vasconcelos</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 609-619</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273573" title="DOI">10.1145/1273496.1273573</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273573&ftid=425764&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow77" style="display:inline;"><br /><div style="display:inline">A cost-sensitive extension of boosting, denoted as asymmetric boosting, is presented. Unlike previous proposals, the new algorithm is derived from sound decision-theoretic principles, which exploit the statistical interpretation of boosting to determine ...</div></span>
<span id="toHide77" style="display:none;"><br /><div style="display:inline"><p>A cost-sensitive extension of boosting, denoted as asymmetric boosting, is presented. Unlike previous proposals, the new algorithm is derived from sound decision-theoretic principles, which exploit the statistical interpretation of boosting to determine a principled extension of the boosting loss. Similarly to AdaBoost, the cost-sensitive extension minimizes this loss by gradient descent on the functional space of convex combinations of weak learners, and produces large margin detectors. It is shown that asymmetric boosting is fully compatible with AdaBoost, in the sense that it becomes the latter when errors are weighted equally. Experimental evidence is provided to demonstrate the claims of cost-sensitivity and large margin. The algorithm is also applied to the computer vision problem of face detection, where it is shown to outperform a number of previous heuristic proposals for cost-sensitive boosting (AdaCost, CSB0, CSB1, CSB2, asymmetric-AdaBoost, AdaC1, AdaC2 and AdaC3).</p></div></span> <a id="expcoll77" href="JavaScript: expandcollapse('expcoll77',77)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273574">Linear and nonlinear generative probabilistic class models for shape contours</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317497684">Graham McNeill</a>,
<a href="author_page.cfm?id=81100170884">Sethu Vijayakumar</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 617-624</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273574" title="DOI">10.1145/1273496.1273574</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273574&ftid=425765&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow78" style="display:inline;"><br /><div style="display:inline">We introduce a robust probabilistic approach to modeling shape contours based on a lowdimensional, nonlinear latent variable model. In contrast to existing techniques that use objective functions in data space without explicit noise models, we are able ...</div></span>
<span id="toHide78" style="display:none;"><br /><div style="display:inline"><p>We introduce a robust probabilistic approach to modeling shape contours based on a lowdimensional, nonlinear latent variable model. In contrast to existing techniques that use objective functions in data space without explicit noise models, we are able to extract complex shape variation from noisy data. Most approaches to learning shape models slide observed data points around fixed contours and hence, require a correctly labeled 'reference shape' to prevent degenerate solutions. In our method, unobserved curves are reparameterized to explain the fixed data points, so this problem does not arise. The proposed algorithms are suitable for use with arbitrary basis functions and are applicable to both open and closed shapes; their effectiveness is demonstrated through illustrative examples, quantitative assessment on benchmark data sets and a visualization task.</p></div></span> <a id="expcoll78" href="JavaScript: expandcollapse('expcoll78',78)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273575">Bottom-up learning of Markov logic network structure</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489791">Lilyana Mihalkova</a>,
<a href="author_page.cfm?id=81100539345">Raymond J. Mooney</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 625-632</span></td>
</tr>
<tr>
<td></td>
 <td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273575" title="DOI">10.1145/1273496.1273575</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273575&ftid=425766&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow79" style="display:inline;"><br /><div style="display:inline">Markov logic networks (MLNs) are a statistical relational model that consists of weighted firstorder clauses and generalizes first-order logic and Markov networks. The current state-of-the-art algorithm for learning MLN structure follows a top-down paradigm ...</div></span>
<span id="toHide79" style="display:none;"><br /><div style="display:inline"><p>Markov logic networks (MLNs) are a statistical relational model that consists of weighted firstorder clauses and generalizes first-order logic and Markov networks. The current state-of-the-art algorithm for learning MLN structure follows a top-down paradigm where many potential candidate structures are systematically generated without considering the data and then evaluated using a statistical measure of their fit to the data. Even though this existing algorithm outperforms an impressive array of benchmarks, its greedy search is susceptible to local maxima or plateaus. We present a novel algorithm for learning MLN structure that follows a more bottom-up approach to address this problem. Our algorithm uses a "propositional" Markov network learning method to construct "template" networks that guide the construction of candidate clauses. Our algorithm significantly improves accuracy and learning time over the existing topdown approach in three real-world domains.</p></div></span> <a id="expcoll79" href="JavaScript: expandcollapse('expcoll79',79)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273576">Mixtures of hierarchical topics with Pachinko allocation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81332516082">David Mimno</a>,
<a href="author_page.cfm?id=81537247056">Wei Li</a>,
<a href="author_page.cfm?id=81100553872">Andrew McCallum</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 633-640</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273576" title="DOI">10.1145/1273496.1273576</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273576&ftid=425767&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow80" style="display:inline;"><br /><div style="display:inline">The four-level pachinko allocation model (PAM) (Li & McCallum, 2006) represents correlations among topics using a DAG structure. It does not, however, represent a nested hierarchy of topics, with some topical word distributions representing the ...</div></span>
<span id="toHide80" style="display:none;"><br /><div style="display:inline"><p>The four-level <i>pachinko allocation model</i> (PAM) (Li & McCallum, 2006) represents correlations among topics using a DAG structure. It does not, however, represent a nested hierarchy of topics, with some topical word distributions representing the vocabulary that is shared among several more specific topics. This paper presents <i>hierarchical PAM</i>---an enhancement that explicitly represents a topic hierarchy. This model can be seen as combining the advantages of hLDA's topical hierarchy representation with PAM's ability to mix multiple leaves of the topic hierarchy. Experimental results show improvements in likelihood of held-out documents, as well as mutual information between automatically-discovered topics and humangenerated categories such as journals.</p></div></span> <a id="expcoll80" href="JavaScript: expandcollapse('expcoll80',80)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273577">Three new graphical models for statistical language modelling</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489759">Andriy Mnih</a>,
<a href="author_page.cfm?id=81100505762">Geoffrey Hinton</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 641-648</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273577" title="DOI">10.1145/1273496.1273577</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273577&ftid=425768&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow81" style="display:inline;"><br /><div style="display:inline">The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic ...</div></span>
<span id="toHide81" style="display:none;"><br /><div style="display:inline"><p>The supremacy of <i>n</i>-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best <i>n</i>-gram models.</p></div></span> <a id="expcoll81" href="JavaScript: expandcollapse('expcoll81',81)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273578">Fast and effective kernels for relational learning from texts</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100511871">Alessandro Moschitti</a>,
<a href="author_page.cfm?id=81100175367">Fabio Massimo Zanzotto</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 649-656</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273578" title="DOI">10.1145/1273496.1273578</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273578&ftid=425769&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow82" style="display:inline;"><br /><div style="display:inline">In this paper, we define a family of syntactic kernels for automatic relational learning from pairs of natural language sentences. We provide an efficient computation of such models by optimizing the dynamic programming algorithm of the kernel evaluation. ...</div></span>
<span id="toHide82" style="display:none;"><br /><div style="display:inline"><p>In this paper, we define a family of syntactic kernels for automatic relational learning from pairs of natural language sentences. We provide an efficient computation of such models by optimizing the dynamic programming algorithm of the kernel evaluation. Experiments with Support Vector Machines and the above kernels show the effectiveness and efficiency of our approach on two very important natural language tasks, Textual Entailment Recognition and Question Answering.</p></div></span> <a id="expcoll82" href="JavaScript: expandcollapse('expcoll82',82)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273579">Dimensionality reduction and generalization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333490183">Sofia Mosci</a>,
<a href="author_page.cfm?id=81100149718">Lorenzo Rosasco</a>,
<a href="author_page.cfm?id=81100339278">Alessandro Verri</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 657-664</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273579" title="DOI">10.1145/1273496.1273579</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273579&ftid=425770&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow83" style="display:inline;"><br /><div style="display:inline">In this paper we investigate the regularization property of Kernel Principal Component Analysis (KPCA), by studying its application as a preprocessing step to supervised learning problems. We show that performing KPCA and then ordinary least squares ...</div></span>
<span id="toHide83" style="display:none;"><br /><div style="display:inline"><p>In this paper we investigate the regularization property of Kernel Principal Component Analysis (KPCA), by studying its application as a preprocessing step to supervised learning problems. We show that performing KPCA and then ordinary least squares on the projected data, a procedure known as kernel principal component regression (KPCR), is equivalent to spectral cut-off regularization, the regularization parameter being exactly the number of principal components to keep. Using probabilistic estimates for integral operators we can prove error estimates for KPCR and propose a parameter choice procedure allowing to prove consistency of the algorithm.</p></div></span> <a id="expcoll83" href="JavaScript: expandcollapse('expcoll83',83)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273580">Unsupervised estimation for noisy-channel models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333490335">Markos Mylonakis</a>,
<a href="author_page.cfm?id=81333491139">Khalil Sima'an</a>,
<a href="author_page.cfm?id=81100468944">Rebecca Hwa</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 665-672</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273580" title="DOI">10.1145/1273496.1273580</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273580&ftid=425771&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow84" style="display:inline;"><br /><div style="display:inline">Shannon's Noisy-Channel model, which describes how a corrupted message might be reconstructed, has been the corner stone for much work in statistical language and speech processing. The model factors into two components: a language model to characterize ...</div></span>
<span id="toHide84" style="display:none;"><br /><div style="display:inline"><p>Shannon's Noisy-Channel model, which describes how a corrupted message might be reconstructed, has been the corner stone for much work in statistical language and speech processing. The model factors into two components: a <i>language model</i> to characterize the original message and a <i>channel model</i> to describe the channel's corruptive process. The standard approach for estimating the parameters of the channel model is unsupervised Maximum-Likelihood of the observation data, usually approximated using the Expectation-Maximization (EM) algorithm. In this paper we show that it is better to maximize the joint likelihood of the data <i>at both ends of the noisy-channel</i>. We derive a corresponding bi-directional EM algorithm and show that it gives better performance than standard EM on two tasks: (1) translation using a probabilistic lexicon and (2) adaptation of a part-of-speech tagger between related languages.</p></div></span> <a id="expcoll84" href="JavaScript: expandcollapse('expcoll84',84)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273581">Revisiting probabilistic models for clustering with pair-wise constraints</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81541246156">Blaine Nelson</a>,
<a href="author_page.cfm?id=81100145752">Ira Cohen</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 673-680</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273581" title="DOI">10.1145/1273496.1273581</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273581&ftid=425772&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow85" style="display:inline;"><br /><div style="display:inline">We revisit recently proposed algorithms for probabilistic clustering with pair-wise constraints between data points. We evaluate and compare existing techniques in terms of robustness to misspecified constraints. We show that the technique that strictly ...</div></span>
<span id="toHide85" style="display:none;"><br /><div style="display:inline"><p>We revisit recently proposed algorithms for probabilistic clustering with pair-wise constraints between data points. We evaluate and compare existing techniques in terms of robustness to misspecified constraints. We show that the technique that strictly enforces the given constraints, namely the chunklet model, produces poor results even under a small number of misspecified constraints. We further show that methods that penalize constraint violation are more robust to misspecified constraints but have undesirable local behaviors. Based on this evaluation, we propose a new learning technique, extending the chunklet model to allow soft constraints represented by an intuitive measure of confidence in the constraint.</p></div></span> <a id="expcoll85" href="JavaScript: expandcollapse('expcoll85',85)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273582">Comparisons of sequence labeling algorithms and extensions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100511402">Nam Nguyen</a>,
<a href="author_page.cfm?id=81333488960">Yunsong Guo</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 681-688</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273582" title="DOI">10.1145/1273496.1273582</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273582&ftid=425773&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow86" style="display:inline;"><br /><div style="display:inline">In this paper, we survey the current state-of-art models for structured learning problems, including Hidden Markov Model (HMM), Conditional Random Fields (CRF), Averaged Perceptron (AP), Structured SVMs (SVMstruct), Max Margin Markov ...</div></span>
<span id="toHide86" style="display:none;"><br /><div style="display:inline"><p>In this paper, we survey the current state-of-art models for structured learning problems, including Hidden Markov Model (HMM), Conditional Random Fields (CRF), Averaged Perceptron (AP), Structured SVMs (<i>SVM<sup>struct</sup>)</i>, Max Margin Markov Networks (M<sup>3</sup>N), and an integration of search and learning algorithm (SEARN). With all due tuning efforts of various parameters of each model, on the data sets we have applied the models to, we found that SVM<i><sup>struct</sup></i> enjoys better performance compared with the others. In addition, we also propose a new method which we call the Structured Learning Ensemble (SLE) to combine these structured learning models. Empirical results show that our SLE algorithm provides more accurate solutions compared with the best results of the individual models.</p></div></span> <a id="expcoll86" href="JavaScript: expandcollapse('expcoll86',86)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273583">Multi-task learning for sequential data via iHMMs and the nested Dirichlet process</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333490124">Kai Ni</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>,
<a href="author_page.cfm?id=81333488093">David Dunson</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 689-696</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273583" title="DOI">10.1145/1273496.1273583</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273583&ftid=425774&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow87" style="display:inline;"><br /><div style="display:inline">A new hierarchical nonparametric Bayesian model is proposed for the problem of multitask learning (MTL) with sequential data. Sequential data are typically modeled with a hidden Markov model (HMM), for which one often must choose an appropriate model ...</div></span>
<span id="toHide87" style="display:none;"><br /><div style="display:inline"><p>A new hierarchical nonparametric Bayesian model is proposed for the problem of multitask learning (MTL) with sequential data. Sequential data are typically modeled with a hidden Markov model (HMM), for which one often must choose an appropriate model structure (number of states) before learning. Here we model sequential data from each task with an infinite hidden Markov model (iHMM), avoiding the problem of model selection. The MTL for iHMMs is implemented by imposing a nested Dirichlet process (nDP) prior on the base distributions of the iHMMs. The nDP-iHMM MTL method allows us to perform task-level clustering and data-level clustering simultaneously, with which the learning for individual iHMMs is enhanced and between-task similarities are learned. Learning and inference for the nDP-iHMM MTL are based on a Gibbs sampler. The effectiveness of the framework is demonstrated using synthetic data as well as real music data.</p></div></span> <a id="expcoll87" href="JavaScript: expandcollapse('expcoll87',87)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273584">Regression on manifolds using kernel dimension reduction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81540516656">Jens Nilsson</a>,
<a href="author_page.cfm?id=81100436576">Fei Sha</a>,
<a href="author_page.cfm?id=81339507945">Michael I. Jordan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 697-704</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273584" title="DOI">10.1145/1273496.1273584</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273584&ftid=425775&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow88" style="display:inline;"><br /><div style="display:inline">We study the problem of discovering a manifold that best preserves information relevant to a nonlinear regression. Solving this problem involves extending and uniting two threads of research. On the one hand, the literature on sufficient dimension reduction ...</div></span>
<span id="toHide88" style="display:none;"><br /><div style="display:inline"><p>We study the problem of discovering a manifold that best preserves information relevant to a nonlinear regression. Solving this problem involves extending and uniting two threads of research. On the one hand, the literature on sufficient dimension reduction has focused on methods for finding the best linear subspace for nonlinear regression; we extend this to manifolds. On the other hand, the literature on manifold learning has focused on unsupervised dimensionality reduction; we extend this to the supervised setting. Our approach to solving the problem involves combining the machinery of kernel dimension reduction with Laplacian eigenmaps. Specifically, we optimize cross-covariance operators in kernel feature spaces that are induced by the normalized graph Laplacian. The result is a highly flexible method in which no strong assumptions are made on the regression function or on the distribution of the covariates. We illustrate our methodology on the analysis of global temperature data and image manifolds.</p></div></span> <a id="expcoll88" href="JavaScript: expandcollapse('expcoll88',88)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273585">Learning state-action basis functions for hierarchical MDPs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333490362">Sarah Osentoski</a>,
<a href="author_page.cfm?id=81100132345">Sridhar Mahadevan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 705-712</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273585" title="DOI">10.1145/1273496.1273585</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273585&ftid=425776&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow89" style="display:inline;"><br /><div style="display:inline">This paper introduces a new approach to action-value function approximation by learning basis functions from a spectral decomposition of the state-action manifold. This paper extends previous work on using Laplacian bases for value function approximation ...</div></span>
<span id="toHide89" style="display:none;"><br /><div style="display:inline"><p>This paper introduces a new approach to action-value function approximation by learning basis functions from a spectral decomposition of the state-action manifold. This paper extends previous work on using Laplacian bases for value function approximation by using the actions of the agent as part of the representation when creating basis functions. The approach results in a nonlinear learned representation particularly suited to approximating action-value functions, without incurring the wasteful duplication of state bases in previous work. We discuss two techniques to create state-action graphs: off-policy and on-policy. We show that these graphs have a greater expressive power and have better performance over state-based Laplacian basis functions in domains modeled as Semi-Markov Decision Processes (SMDPs). We present a simple graph partitioning method to scale the approach to large discrete MDPs.</p></div></span> <a id="expcoll89" href="JavaScript: expandcollapse('expcoll89',89)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273586">A fast linear separability test by projection of positive points on subspaces</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333492039">Yogananda A P</a>,
<a href="author_page.cfm?id=81333490319">M Narasimha Murthy</a>,
<a href="author_page.cfm?id=81333488576">Lakshmi Gopal</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 713-720</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273586" title="DOI">10.1145/1273496.1273586</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273586&ftid=425777&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow90" style="display:inline;"><br /><div style="display:inline">A geometric and non parametric procedure for testing if two finite set of points are linearly separable is proposed. The Linear Separability Test is equivalent to a test that determines if a strictly positive point h &gt; 0 exists in the ...</div></span>
<span id="toHide90" style="display:none;"><br /><div style="display:inline"><p>A geometric and non parametric procedure for testing if two finite set of points are linearly separable is proposed. The Linear Separability Test is equivalent to a test that determines if a strictly positive point <i>h</i> &gt; <b>0</b> exists in the range of a matrix <i>A</i> (related to the points in the two finite sets). The algorithm proposed in the paper iteratively checks if a strictly positive point exists in a subspace by projecting a strictly positive vector with equal co-ordinates (<i>p</i>), on the subspace. At the end of each iteration, the subspace is reduced to a lower dimensional subspace. The test is completed within <i>r</i> &le; min(<i>n, d</i> + 1) steps, for both linearly separable and non separable problems (<i>r</i> is the rank of <i>A, n</i> is the number of points and <i>d</i> is the dimension of the space containing the points). The worst case time complexity of the algorithm is <i>O(nr</i><sup>3</sup>) and space complexity of the algorithm is <i>O(nd)</i>. A small review of some of the prominent algorithms and their time complexities is included. The worst case computational complexity of our algorithm is lower than the worst case computational complexity of Simplex, Perceptron, Support Vector Machine and Convex Hull Algorithms, if <i>d&lt;n</i><sup>2/3</sup>.</p></div></span> <a id="expcoll90" href="JavaScript: expandcollapse('expcoll90',90)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273587">Multi-armed bandit problems with dependent arms</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100285459">Sandeep Pandey</a>,
<a href="author_page.cfm?id=81100424530">Deepayan Chakrabarti</a>,
<a href="author_page.cfm?id=81100632698">Deepak Agarwal</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 721-728</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273587" title="DOI">10.1145/1273496.1273587</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273587&ftid=425778&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow91" style="display:inline;"><br /><div style="display:inline">We provide a framework to exploit dependencies among arms in multi-armed bandit problems, when the dependencies are in the form of a generative model on clusters of arms. We find an optimal MDP-based policy for the discounted reward case, and also give ...</div></span>
<span id="toHide91" style="display:none;"><br /><div style="display:inline"><p>We provide a framework to exploit dependencies among arms in multi-armed bandit problems, when the dependencies are in the form of a generative model on clusters of arms. We find an optimal MDP-based policy for the discounted reward case, and also give an approximation of it with formal error guarantee. We discuss lower bounds on regret in the undiscounted reward scenario, and propose a general two-level bandit policy for it. We propose three different instantiations of our general policy and provide theoretical justifications of how the regret of the instantiated policies depend on the characteristics of the clusters. Finally, we empirically demonstrate the efficacy of our policies on large-scale real-world and synthetic data, and show that they significantly outperform classical policies designed for bandits with independent arms.</p></div></span> <a id="expcoll91" href="JavaScript: expandcollapse('expcoll91',91)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273588">Learning for efficient retrieval of structured data with noisy queries</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100170075">Charles Parker</a>,
<a href="author_page.cfm?id=81100360494">Alan Fern</a>,
<a href="author_page.cfm?id=81100642869">Prasad Tadepalli</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 729-736</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273588" title="DOI">10.1145/1273496.1273588</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273588&ftid=425779&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow92" style="display:inline;"><br /><div style="display:inline">Increasingly large collections of structured data necessitate the development of efficient, noise-tolerant retrieval tools. In this work, we consider this issue and describe an approach to learn a similarity function that is not only accurate, but that ...</div></span>
<span id="toHide92" style="display:none;"><br /><div style="display:inline"><p>Increasingly large collections of structured data necessitate the development of efficient, noise-tolerant retrieval tools. In this work, we consider this issue and describe an approach to learn a similarity function that is not only accurate, but that also increases the effectiveness of retrieval data structures. We present an algorithm that uses functional gradient boosting to maximize both retrieval accuracy and the retrieval efficiency of vantage point trees. We demonstrate the effectiveness of our approach on two datasets, including a moderately sized real-world dataset of folk music.</p></div></span> <a id="expcoll92" href="JavaScript: expandcollapse('expcoll92',92)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273589">Analyzing feature generation for value-function approximation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100331696">Ronald Parr</a>,
<a href="author_page.cfm?id=81333490370">Christopher Painter-Wakefield</a>,
<a href="author_page.cfm?id=81331497976">Lihong Li</a>,
<a href="author_page.cfm?id=81406601119">Michael Littman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 737-744</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273589" title="DOI">10.1145/1273496.1273589</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273589&ftid=425780&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow93" style="display:inline;"><br /><div style="display:inline">We analyze a simple, Bellman-error-based approach to generating basis functions for value-function approximation. We show that it generates orthogonal basis functions that provably tighten approximation error bounds. We also illustrate the use of this ...</div></span>
<span id="toHide93" style="display:none;"><br /><div style="display:inline"><p>We analyze a simple, Bellman-error-based approach to generating basis functions for value-function approximation. We show that it generates orthogonal basis functions that provably tighten approximation error bounds. We also illustrate the use of this approach in the presence of noise on some sample problems.</p></div></span> <a id="expcoll93" href="JavaScript: expandcollapse('expcoll93',93)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273590">Reinforcement learning by reward-weighted regression for operational space control</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100524321">Jan Peters</a>,
<a href="author_page.cfm?id=81100472883">Stefan Schaal</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 745-750</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273590" title="DOI">10.1145/1273496.1273590</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273590&ftid=425781&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow94" style="display:inline;"><br /><div style="display:inline">Many robot control problems of practical importance, including operational space control, can be reformulated as immediate reward reinforcement learning problems. However, few of the known optimization or reinforcement learning algorithms can be used ...</div></span>
<span id="toHide94" style="display:none;"><br /><div style="display:inline"><p>Many robot control problems of practical importance, including operational space control, can be reformulated as immediate reward reinforcement learning problems. However, few of the known optimization or reinforcement learning algorithms can be used in online learning control for robots, as they are either prohibitively slow, do not scale to interesting domains of complex robots, or require trying out policies generated by random search, which are infeasible for a physical system. Using a generalization of the EM-base reinforcement learning framework suggested by Dayan & Hinton, we reduce the problem of learning with immediate rewards to a reward-weighted regression problem with an adaptive, integrated reward transformation for faster convergence. The resulting algorithm is efficient, learns smoothly without dangerous jumps in solution space, and works well in applications of complex high degree-of-freedom robots.</p></div></span> <a id="expcoll94" href="JavaScript: expandcollapse('expcoll94',94)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273591">Tracking value function dynamics to improve reinforcement learning with piecewise linear function approximation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333490591">Chee Wee Phua</a>,
<a href="author_page.cfm?id=81100317526">Robert Fitch</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 751-758</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273591" title="DOI">10.1145/1273496.1273591</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273591&ftid=425782&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow95" style="display:inline;"><br /><div style="display:inline">Reinforcement learning algorithms can become unstable when combined with linear function approximation. Algorithms that minimize the mean-square Bellman error are guaranteed to converge, but often do so slowly or are computationally expensive. In this ...</div></span>
<span id="toHide95" style="display:none;"><br /><div style="display:inline"><p>Reinforcement learning algorithms can become unstable when combined with linear function approximation. Algorithms that minimize the mean-square Bellman error are guaranteed to converge, but often do so slowly or are computationally expensive. In this paper, we propose to improve the convergence speed of piecewise linear function approximation by tracking the dynamics of the value function with the Kalman filter using a random-walk model. We cast this as a general framework in which we implement the TD, Q-Learning and MAXQ algorithms for different domains, and report empirical results demonstrating improved learning speed over previous methods.</p></div></span> <a id="expcoll95" href="JavaScript: expandcollapse('expcoll95',95)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273592">Self-taught learning: transfer learning from unlabeled data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315491047">Rajat Raina</a>,
<a href="author_page.cfm?id=81100528919">Alexis Battle</a>,
<a href="author_page.cfm?id=81333489568">Honglak Lee</a>,
<a href="author_page.cfm?id=81430611645">Benjamin Packer</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 759-766</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273592" title="DOI">10.1145/1273496.1273592</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273592&ftid=425783&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow96" style="display:inline;"><br /><div style="display:inline">We present a new machine learning framework called "self-taught learning" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. ...</div></span>
<span id="toHide96" style="display:none;"><br /><div style="display:inline"><p>We present a new machine learning framework called "self-taught learning" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.</p></div></span> <a id="expcoll96" href="JavaScript: expandcollapse('expcoll96',96)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273593">Online discovery of similarity mappings</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333490741">Alexander Rakhlin</a>,
<a href="author_page.cfm?id=81350570005">Jacob Abernethy</a>,
<a href="author_page.cfm?id=81452616577">Peter L. Bartlett</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 767-774</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273593" title="DOI">10.1145/1273496.1273593</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273593&ftid=425784&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow97" style="display:inline;"><br /><div style="display:inline">We consider the problem of choosing, sequentially, a map which assigns elements of a set A to a few elements of a set B. On each round, the algorithm suffers some cost associated with the chosen assignment, and the ...</div></span>
<span id="toHide97" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of choosing, sequentially, a map which assigns elements of a set <b><i>A</i></b> to <i>a few</i> elements of a set <b><i>B</i></b>. On each round, the algorithm suffers some cost associated with the chosen assignment, and the goal is to minimize the cumulative loss of these choices relative to the best map on the entire sequence. Even though the offline problem of finding the best map is provably hard, we show that there is an equivalent online approximation algorithm, Randomized Map Prediction (RMP), that is efficient and performs nearly as well. While drawing upon results from the "Online Prediction with Expert Advice" setting, we show how RMP can be utilized as an online approach to several standard batch problems. We apply RMP to online clustering as well as online feature selection and, surprisingly, RMP often outperforms the standard batch algorithms on these problems.</p></div></span> <a id="expcoll97" href="JavaScript: expandcollapse('expcoll97',97)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273594">More efficiency in multiple kernel learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100280150">Alain Rakotomamonjy</a>,
<a href="author_page.cfm?id=81100328355">Francis Bach</a>,
<a href="author_page.cfm?id=81100333303">St&#233;phane Canu</a>,
<a href="author_page.cfm?id=81100345039">Yves Grandvalet</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 775-782</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273594" title="DOI">10.1145/1273496.1273594</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273594&ftid=425785&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow98" style="display:inline;"><br /><div style="display:inline">An efficient and general multiple kernel learning (MKL) algorithm has been recently proposed by Sonnenburg et al. (2006). This approach has opened new perspectives since it makes the MKL approach tractable for large-scale problems, by iteratively using ...</div></span>
<span id="toHide98" style="display:none;"><br /><div style="display:inline"><p>An efficient and general multiple kernel learning (MKL) algorithm has been recently proposed by Sonnenburg et al. (2006). This approach has opened new perspectives since it makes the MKL approach tractable for large-scale problems, by iteratively using existing support vector machine code. However, it turns out that this iterative algorithm needs several iterations before converging towards a reasonable solution. In this paper, we address the MKL problem through an adaptive 2-norm regularization formulation. Weights on each kernel matrix are included in the standard SVM empirical risk minimization problem with a <i>l</i><inf>1</inf> constraint to encourage sparsity. We propose an algorithm for solving this problem and provide an new insight on MKL algorithms based on block 1-norm regularization by showing that the two approaches are equivalent. Experimental results show that the resulting algorithm converges rapidly and its efficiency compares favorably to other MKL algorithms.</p></div></span> <a id="expcoll98" href="JavaScript: expandcollapse('expcoll98',98)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273595">Graph clustering with network structure indices</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100417672">Matthew J. Rattigan</a>,
<a href="author_page.cfm?id=81317489087">Marc Maier</a>,
<a href="author_page.cfm?id=81100640362">David Jensen</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 783-790</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273595" title="DOI">10.1145/1273496.1273595</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273595&ftid=425786&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow99" style="display:inline;"><br /><div style="display:inline">Graph clustering has become ubiquitous in the study of relational data sets. We examine two simple algorithms: a new graphical adaptation of the k-medoids algorithm and the Girvan-Newman method based on edge betweenness centrality. We show that ...</div></span>
<span id="toHide99" style="display:none;"><br /><div style="display:inline"><p>Graph clustering has become ubiquitous in the study of relational data sets. We examine two simple algorithms: a new graphical adaptation of the <i>k</i>-medoids algorithm and the Girvan-Newman method based on edge betweenness centrality. We show that they can be effective at discovering the latent groups or communities that are defined by the link structure of a graph. However, both approaches rely on prohibitively expensive computations, given the size of modern relational data sets. Network structure indices (NSIs) are a proven technique for indexing network structure and efficiently finding short paths. We show how incorporating NSIs into these graph clustering algorithms can overcome these complexity limitations. We also present promising quantitative and qualitative evaluations of the modified algorithms on synthetic and real data sets.</p></div></span> <a id="expcoll99" href="JavaScript: expandcollapse('expcoll99',99)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273596">Restricted Boltzmann machines for collaborative filtering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100562217">Ruslan Salakhutdinov</a>,
<a href="author_page.cfm?id=81333489759">Andriy Mnih</a>,
<a href="author_page.cfm?id=81100505762">Geoffrey Hinton</a>
</span>
 </td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 791-798</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273596" title="DOI">10.1145/1273496.1273596</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273596&ftid=425787&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow100" style="display:inline;"><br /><div style="display:inline">Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, ...</div></span>
<span id="toHide100" style="display:none;"><br /><div style="display:inline"><p>Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.</p></div></span> <a id="expcoll100" href="JavaScript: expandcollapse('expcoll100',100)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273597">Sample compression bounds for decision trees</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81448595156">Mohak Shah</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 799-806</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273597" title="DOI">10.1145/1273496.1273597</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273597&ftid=425788&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow101" style="display:inline;"><br /><div style="display:inline">We propose a formulation of the Decision Tree learning algorithm in the Compression settings and derive tight generalization error bounds. In particular, we propose Sample Compression and Occam's Razor bounds. We show how such bounds, unlike the VC dimension ...</div></span>
<span id="toHide101" style="display:none;"><br /><div style="display:inline"><p>We propose a formulation of the Decision Tree learning algorithm in the Compression settings and derive tight generalization error bounds. In particular, we propose Sample Compression and Occam's Razor bounds. We show how such bounds, unlike the VC dimension or Rademacher complexities based bounds, are more general and can also perform a margin-sparsity trade-off to obtain better classifers. Potentially, these risk bounds can also guide the model selection process and replace traditional pruning strategies.</p></div></span> <a id="expcoll101" href="JavaScript: expandcollapse('expcoll101',101)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273598">Pegasos: Primal Estimated sub-GrAdient SOlver for SVM</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100489319">Shai Shalev-Shwartz</a>,
<a href="author_page.cfm?id=81100308085">Yoram Singer</a>,
<a href="author_page.cfm?id=81100005542">Nathan Srebro</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 807-814</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273598" title="DOI">10.1145/1273496.1273598</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273598&ftid=425789&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow102" style="display:inline;"><br /><div style="display:inline">We describe and analyze a simple and effective iterative algorithm for solving the optimization problem cast by Support Vector Machines (SVM). Our method alternates between stochastic gradient descent steps and projection steps. We prove that the number ...</div></span>
<span id="toHide102" style="display:none;"><br /><div style="display:inline"><p>We describe and analyze a simple and effective iterative algorithm for solving the optimization problem cast by Support Vector Machines (SVM). Our method alternates between stochastic gradient descent steps and projection steps. We prove that the number of iterations required to obtain a solution of accuracy &epsilon; is &Otilde;(1/&epsilon;). In contrast, previous analyses of stochastic gradient descent methods require &Omega; (1/&epsilon;<sup>2</sup>) iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/&lambda;, where &lambda; is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is &Otilde; (d/(&lambda;&epsilon;)), where <i>d</i> is a bound on the number of non-zero features in each example. Since the run-time does <i>not</i> depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach can seamlessly be adapted to employ non-linear kernels while working solely on the primal objective function. We demonstrate the efficiency and applicability of our approach by conducting experiments on large text classification problems, comparing our solver to existing state-of-the-art SVM solvers. For example, it takes less than 5 seconds for our solver to converge when solving a text classification problem from Reuters Corpus Volume 1 (RCV1) with 800,000 training examples.</p></div></span> <a id="expcoll102" href="JavaScript: expandcollapse('expcoll102',102)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273599">A dependence maximization view of clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100578710">Le Song</a>,
<a href="author_page.cfm?id=81100243402">Alex Smola</a>,
<a href="author_page.cfm?id=81333488991">Arthur Gretton</a>,
<a href="author_page.cfm?id=81100155678">Karsten M. Borgwardt</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 815-822</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273599" title="DOI">10.1145/1273496.1273599</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273599&ftid=425790&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow103" style="display:inline;"><br /><div style="display:inline">We propose a family of clustering algorithms based on the maximization of dependence between the input variables and their cluster labels, as expressed by the Hilbert-Schmidt Independence Criterion (HSIC). Under this framework, we unify the geometric, ...</div></span>
<span id="toHide103" style="display:none;"><br /><div style="display:inline"><p>We propose a family of clustering algorithms based on the maximization of dependence between the input variables and their cluster labels, as expressed by the Hilbert-Schmidt Independence Criterion (HSIC). Under this framework, we unify the geometric, spectral, and statistical dependence views of clustering, and subsume many existing algorithms as special cases (e.g. <i>k</i>-means and spectral clustering). Distinctive to our framework is that kernels can also be applied on the labels, which can endow them with particular structures. We also obtain a perturbation bound on the change in <i>k</i>-means clustering.</p></div></span> <a id="expcoll103" href="JavaScript: expandcollapse('expcoll103',103)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273600">Supervised feature selection via dependence estimation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100578710">Le Song</a>,
<a href="author_page.cfm?id=81100243402">Alex Smola</a>,
<a href="author_page.cfm?id=81333488991">Arthur Gretton</a>,
<a href="author_page.cfm?id=81100155678">Karsten M. Borgwardt</a>,
<a href="author_page.cfm?id=81333487693">Justin Bedo</a>
</span>
</td>
</tr>
 <tr>
<td></td>
<td> <span style="padding-left:0">Pages: 823-830</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273600" title="DOI">10.1145/1273496.1273600</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273600&ftid=425791&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow104" style="display:inline;"><br /><div style="display:inline">We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature ...</div></span>
<span id="toHide104" style="display:none;"><br /><div style="display:inline"><p>We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.</p></div></span> <a id="expcoll104" href="JavaScript: expandcollapse('expcoll104',104)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273601">Sparse eigen methods by D.C. programming</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421594362">Bharath K. Sriperumbudur</a>,
<a href="author_page.cfm?id=81100588169">David A. Torres</a>,
<a href="author_page.cfm?id=81100118766">Gert R. G. Lanckriet</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 831-838</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273601" title="DOI">10.1145/1273496.1273601</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273601&ftid=425792&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow105" style="display:inline;"><br /><div style="display:inline">Eigenvalue problems are rampant in machine learning and statistics and appear in the context of classification, dimensionality reduction, etc. In this paper, we consider a cardinality constrained variational formulation of generalized eigenvalue problem ...</div></span>
<span id="toHide105" style="display:none;"><br /><div style="display:inline"><p>Eigenvalue problems are rampant in machine learning and statistics and appear in the context of classification, dimensionality reduction, etc. In this paper, we consider a cardinality constrained variational formulation of generalized eigenvalue problem with sparse principal component analysis (PCA) as a special case. Using <i>l</i><inf>1</inf>-norm approximation to the cardinality constraint, previous methods have proposed both convex and non-convex solutions to the sparse PCA problem. In contrast, we propose a tighter approximation that is related to the negative log-likelihood of a Student's t-distribution. The problem is then framed as a d.c. (difference of convex functions) program and is solved as a sequence of locally convex programs. We show that the proposed method not only explains more variance with sparse loadings on the principal directions but also has better scalability compared to other methods. We demonstrate these results on a collection of datasets of varying dimensionality, two of which are high-dimensional gene datasets where the goal is to find few relevant genes that explain as much variance as possible.</p></div></span> <a id="expcoll105" href="JavaScript: expandcollapse('expcoll105',105)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273602">Learning to solve game trees</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331504390">David Stern</a>,
<a href="author_page.cfm?id=81100124906">Ralf Herbrich</a>,
<a href="author_page.cfm?id=81100450764">Thore Graepel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 839-846</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273602" title="DOI">10.1145/1273496.1273602</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273602&ftid=425793&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow106" style="display:inline;"><br /><div style="display:inline">We apply probability theory to the task of proving whether a goal can be achieved by a player in an adversarial game. Such problems are solved by searching the game tree. We view this tree as a graphical model which yields a distribution over the (Boolean) ...</div></span>
<span id="toHide106" style="display:none;"><br /><div style="display:inline"><p>We apply probability theory to the task of proving whether a goal can be achieved by a player in an adversarial game. Such problems are solved by searching the game tree. We view this tree as a graphical model which yields a distribution over the (Boolean) outcome of the search before it terminates. Experiments show that a best-first search algorithm guided by this distribution explores a similar number of nodes as Proof-Number Search to solve Go problems. Knowledge is incorporated into search by using domain-specific models to provide prior distributions over the values of leaf nodes of the game tree. These are surrogate for the unexplored parts of the tree. The parameters of these models can be learned from previous search trees. Experiments on Go show that the speed of problem solving can be increased by orders of magnitude by this technique but care must be taken to avoid over-fitting.</p></div></span> <a id="expcoll106" href="JavaScript: expandcollapse('expcoll106',106)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273603">Robust mixtures in the presence of measurement errors</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81381600589">Jianyong Sun</a>,
<a href="author_page.cfm?id=81100556599">Ata Kab&#225;n</a>,
<a href="author_page.cfm?id=81100022221">Somak Raychaudhury</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 847-854</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273603" title="DOI">10.1145/1273496.1273603</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273603&ftid=425794&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow107" style="display:inline;"><br /><div style="display:inline">We develop a mixture-based approach to robust density modeling and outlier detection for experimental multivariate data that includes measurement error information. Our model is designed to infer atypical measurements that are not due to errors, aiming ...</div></span>
<span id="toHide107" style="display:none;"><br /><div style="display:inline"><p>We develop a mixture-based approach to robust density modeling and outlier detection for experimental multivariate data that includes measurement error information. Our model is designed to infer atypical measurements that are not due to errors, aiming to retrieve potentially interesting peculiar objects. Since exact inference is not possible in this model, we develop a tree-structured variational EM solution. This compares favorably against a fully factorial approximation scheme, approaching the accuracy of a Markov-Chain-EM, while maintaining computational simplicity. We demonstrate the benefits of including measurement errors in the model, in terms of improved outlier detection rates in varying measurement uncertainty conditions. We then use this approach for detecting peculiar quasars from an astrophysical survey, given photometric measurements with errors.</p></div></span> <a id="expcoll107" href="JavaScript: expandcollapse('expcoll107',107)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273604">A kernel-based causal learning algorithm</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81351604765">Xiaohai Sun</a>,
<a href="author_page.cfm?id=81100317961">Dominik Janzing</a>,
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>,
<a href="author_page.cfm?id=81100373172">Kenji Fukumizu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 855-862</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273604" title="DOI">10.1145/1273496.1273604</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273604&ftid=425795&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow108" style="display:inline;"><br /><div style="display:inline">We describe a causal learning method, which employs measuring the strength of statistical dependences in terms of the Hilbert-Schmidt norm of kernel-based cross-covariance operators. Following the line of the common faithfulness assumption of constraint-based ...</div></span>
<span id="toHide108" style="display:none;"><br /><div style="display:inline"><p>We describe a causal learning method, which employs measuring the strength of statistical dependences in terms of the Hilbert-Schmidt norm of kernel-based cross-covariance operators. Following the line of the common faithfulness assumption of constraint-based causal learning, our approach assumes that a variable <i>Z</i> is likely to be a common effect of <i>X</i> and <i>Y</i>, if conditioning on <i>Z</i> increases the dependence between <i>X</i> and <i>Y</i>. Based on this assumption, we collect "votes" for hypothetical causal directions and orient the edges by the majority principle. In most experiments with known causal structures, our method provided plausible results and outperformed the conventional constraint-based PC algorithm.</p></div></span> <a id="expcoll108" href="JavaScript: expandcollapse('expcoll108',108)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273605">Piecewise pseudolikelihood for efficient training of conditional random fields</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100539852">Charles Sutton</a>,
<a href="author_page.cfm?id=81100553872">Andrew McCallum</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 863-870</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273605" title="DOI">10.1145/1273496.1273605</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273605&ftid=425796&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow109" style="display:inline;"><br /><div style="display:inline">Discriminative training of graphical models can be expensive if the variables have large cardinality, even if the graphical structure is tractable. In such cases, pseudolikelihood is an attractive alternative, because its running time is linear in the ...</div></span>
<span id="toHide109" style="display:none;"><br /><div style="display:inline"><p>Discriminative training of graphical models can be expensive if the variables have large cardinality, even if the graphical structure is tractable. In such cases, pseudolikelihood is an attractive alternative, because its running time is linear in the variable cardinality, but on some data its accuracy can be poor. Piecewise training (Sutton & McCallum, 2005) can have better accuracy but does not scale as well in the variable cardinality. In this paper, we introduce <i>piecewise pseudolikelihood</i>, which retains the computational efficiency of pseudolikelihood but can have much better accuracy. On several benchmark NLP data sets, piecewise pseudolikelihood has better accuracy than standard pseudolikelihood, and in many cases nearly equivalent to maximum likelihood, with five to ten times less training time than batch CRF training.</p></div></span> <a id="expcoll109" href="JavaScript: expandcollapse('expcoll109',109)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273606">On the role of tracking in stationary environments</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81342513055">Richard S. Sutton</a>,
<a href="author_page.cfm?id=81421600064">Anna Koop</a>,
<a href="author_page.cfm?id=81100336678">David Silver</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 871-878</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273606" title="DOI">10.1145/1273496.1273606</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273606&ftid=425797&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow110" style="display:inline;"><br /><div style="display:inline">It is often thought that learning algorithms that track the best solution, as opposed to converging to it, are important only on nonstationary problems. We present three results suggesting that this is not so. First we illustrate in a simple concrete ...</div></span>
<span id="toHide110" style="display:none;"><br /><div style="display:inline"><p>It is often thought that learning algorithms that track the best solution, as opposed to converging to it, are important only on nonstationary problems. We present three results suggesting that this is not so. First we illustrate in a simple concrete example, the Black and White problem, that tracking can perform better than any converging algorithm on a stationary problem. Second, we show the same point on a larger, more realistic problem, an application of temporal difference learning to computer Go. Our third result suggests that tracking in stationary problems could be important for metalearning research (e.g., learning to learn, feature selection, transfer). We apply a metalearning algorithm for step-size adaptation, IDBD (Sutton, 1992a), to the Black and White problem, showing that meta-learning has a dramatic long-term effect on performance whereas, on an analogous converging problem, meta-learning has only a small second-order effect. This small result suggests a way of eventually overcoming a major obstacle to meta-learning research: the lack of an independent methodology for task selection.</p></div></span> <a id="expcoll110" href="JavaScript: expandcollapse('expcoll110',110)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273607">Cross-domain transfer for reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81339531794">Matthew E. Taylor</a>,
<a href="author_page.cfm?id=81100388010">Peter Stone</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 879-886</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273607" title="DOI">10.1145/1273496.1273607</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273607&ftid=425798&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow111" style="display:inline;"><br /><div style="display:inline">A typical goal for transfer learning algorithms is to utilize knowledge gained in a source task to learn a target task faster. Recently introduced transfer methods in reinforcement learning settings have shown considerable promise, but they typically ...</div></span>
<span id="toHide111" style="display:none;"><br /><div style="display:inline"><p>A typical goal for transfer learning algorithms is to utilize knowledge gained in a source task to learn a target task faster. Recently introduced transfer methods in reinforcement learning settings have shown considerable promise, but they typically transfer between pairs of very similar tasks. This work introduces <i>Rule Transfer</i>, a transfer algorithm that first learns rules to summarize a source task policy and then leverages those rules to learn faster in a target task. This paper demonstrates that Rule Transfer can effectively speed up learning in Keepaway, a benchmark RL problem in the robot soccer domain, based on experience from source tasks in the gridworld domain. We empirically show, through the use of three distinct transfer metrics, that Rule Transfer is effective across these domains.</p></div></span> <a id="expcoll111" href="JavaScript: expandcollapse('expcoll111',111)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273608">Incremental Bayesian networks for structure prediction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323497046">Ivan Titov</a>,
<a href="author_page.cfm?id=81100433513">James Henderson</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 887-894</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273608" title="DOI">10.1145/1273496.1273608</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273608&ftid=425799&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow112" style="display:inline;"><br /><div style="display:inline">We propose a class of graphical models appropriate for structure prediction problems where the model structure is a function of the output structure. Incremental Sigmoid Belief Networks (ISBNs) avoid the need to sum over the possible model structures ...</div></span>
<span id="toHide112" style="display:none;"><br /><div style="display:inline"><p>We propose a class of graphical models appropriate for structure prediction problems where the model structure is a function of the output structure. Incremental Sigmoid Belief Networks (ISBNs) avoid the need to sum over the possible model structures by using directed arcs and incrementally specifying the model structure. Exact inference in such directed models is not tractable, but we derive two efficient approximations based on mean field methods, which prove effective in artificial experiments. We then demonstrate their effectiveness on a benchmark natural language parsing task, where they achieve state-of-the-art accuracy. Also, the model which is a closer approximation to an ISBN has better parsing accuracy, suggesting that ISBNs are an appropriate abstract model of structure prediction tasks.</p></div></span> <a id="expcoll112" href="JavaScript: expandcollapse('expcoll112',112)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273609">Classifying matrices with a spectral regularization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333491249">Ryota Tomioka</a>,
<a href="author_page.cfm?id=81100253376">Kazuyuki Aihara</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 895-902</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273609" title="DOI">10.1145/1273496.1273609</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273609&ftid=425800&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow113" style="display:inline;"><br /><div style="display:inline">We propose a method for the classification of matrices. We use a linear classifier with a novel regularization scheme based on the spectral l1-norm of its coefficient matrix. The spectral regularization not only provides a principled ...</div></span>
<span id="toHide113" style="display:none;"><br /><div style="display:inline"><p>We propose a method for the classification of matrices. We use a linear classifier with a novel regularization scheme based on the spectral <i>l</i><inf>1</inf>-norm of its coefficient matrix. The spectral regularization not only provides a principled way of complexity control but also enables automatic determination of the rank of the coefficient matrix. Using the Linear Matrix Inequality technique, we formulate the inference task as a single convex optimization problem. We apply our method to the motor-imagery EEG classification problem. The method not only improves upon conventional methods in the classification performance but also determines a subspace in the signal that concentrates discriminative information without any additional feature extraction step. The method can be easily generalized to regression problems by changing the loss function. Connections to other methods are also discussed.</p></div></span> <a id="expcoll113" href="JavaScript: expandcollapse('expcoll113',113)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273610">Approximate maximum margin algorithms with rules controlled by the number of mistakes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333491422">Petroula Tsampouka</a>,
<a href="author_page.cfm?id=81310500608">John Shawe-Taylor</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 903-910</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273610" title="DOI">10.1145/1273496.1273610</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273610&ftid=425801&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow114" style="display:inline;"><br /><div style="display:inline">We present a family of incremental Perceptron-like algorithms (PLAs) with margin in which both the "effective" learning rate, defined as the ratio of the learning rate to the length of the weight vector, and the misclassification condition are entirely ...</div></span>
<span id="toHide114" style="display:none;"><br /><div style="display:inline"><p>We present a family of incremental Perceptron-like algorithms (PLAs) with margin in which both the "effective" learning rate, defined as the ratio of the learning rate to the length of the weight vector, and the misclassification condition are entirely controlled by rules involving (powers of) the number of mistakes. We examine the convergence of such algorithms in a finite number of steps and show that under some rather mild conditions there exists a limit of the parameters involved in which convergence leads to classification with maximum margin. An experimental comparison of algorithms belonging to this family with other large margin PLAs and decomposition SVMs is also presented.</p></div></span> <a id="expcoll114" href="JavaScript: expandcollapse('expcoll114',114)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273611">Simpler core vector machines with enclosing balls</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309487444">Ivor W. Tsang</a>,
<a href="author_page.cfm?id=81100568777">Andras Kocsor</a>,
<a href="author_page.cfm?id=81100525095">James T. Kwok</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 911-918</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273611" title="DOI">10.1145/1273496.1273611</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273611&ftid=425802&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow115" style="display:inline;"><br /><div style="display:inline">The core vector machine (CVM) is a recent approach for scaling up kernel methods based on the notion of minimum enclosing ball (MEB). Though conceptually simple, an efficient implementation still requires a sophisticated numerical solver. In this paper, ...</div></span>
<span id="toHide115" style="display:none;"><br /><div style="display:inline"><p>The core vector machine (CVM) is a recent approach for scaling up kernel methods based on the notion of minimum enclosing ball (MEB). Though conceptually simple, an efficient implementation still requires a sophisticated numerical solver. In this paper, we introduce the enclosing ball (EB) problem where the ball's radius is fixed and thus does not have to be minimized. We develop efficient (1 + e)-approximation algorithms that are simple to implement and do not require any numerical solver. For the Gaussian kernel in particular, a suitable choice of this (fixed) radius is easy to determine, and the center obtained from the (1 + e)-approximation of this EB problem is close to the center of the corresponding MEB. Experimental results show that the proposed algorithm has accuracies comparable to the other large-scale SVM implementations, but can handle very large data sets and is even faster than the CVM in general.</p></div></span> <a id="expcoll115" href="JavaScript: expandcollapse('expcoll115',115)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273612">Entire regularization paths for graph data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100030766">Koji Tsuda</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 919-926</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273612" title="DOI">10.1145/1273496.1273612</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273612&ftid=425803&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow116" style="display:inline;"><br /><div style="display:inline">Graph data such as chemical compounds and XML documents are getting more common in many application domains. A main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary ...</div></span>
<span id="toHide116" style="display:none;"><br /><div style="display:inline"><p>Graph data such as chemical compounds and XML documents are getting more common in many application domains. A main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of indicators of all possible subgraph patterns, the dimensionality gets too large for usual statistical methods. We propose an efficient method to select a small number of salient patterns by regularization path tracking. The generation of useless patterns is minimized by progressive extension of the search space. In experiments, it is shown that our technique is considerably more efficient than a simpler approach based on frequent substructure mining.</p></div></span> <a id="expcoll116" href="JavaScript: expandcollapse('expcoll116',116)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273613">Discriminative Gaussian process latent variable model for classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100176326">Raquel Urtasun</a>,
<a href="author_page.cfm?id=81100537374">Trevor Darrell</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 927-934</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273613" title="DOI">10.1145/1273496.1273613</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273613&ftid=425831&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow117" style="display:inline;"><br /><div style="display:inline">Supervised learning is difficult with high dimensional input spaces and very small training sets, but accurate classification may be possible if the data lie on a low-dimensional manifold. Gaussian Process Latent Variable Models can discover low dimensional ...</div></span>
<span id="toHide117" style="display:none;"><br /><div style="display:inline"><p>Supervised learning is difficult with high dimensional input spaces and very small training sets, but accurate classification may be possible if the data lie on a low-dimensional manifold. Gaussian Process Latent Variable Models can discover low dimensional manifolds given only a small number of examples, but learn a latent space without regard for class labels. Existing methods for discriminative manifold learning (e.g., LDA, GDA) do constrain the class distribution in the latent space, but are generally deterministic and may not generalize well with limited training data. We introduce a method for Gaussian Process Classification using latent variable models trained with discriminative priors over the latent space, which can learn a discriminative latent space from a small training set.</p></div></span> <a id="expcoll117" href="JavaScript: expandcollapse('expcoll117',117)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273614">Experimental perspectives on learning from imbalanced data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81312484395">Jason Van Hulse</a>,
<a href="author_page.cfm?id=81100596670">Taghi M. Khoshgoftaar</a>,
<a href="author_page.cfm?id=81361598707">Amri Napolitano</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 935-942</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273614" title="DOI">10.1145/1273496.1273614</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273614&ftid=425804&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow118" style="display:inline;"><br /><div style="display:inline">We present a comprehensive suite of experimentation on the subject of learning from imbalanced data. When classes are imbalanced, many learning algorithms can suffer from the perspective of reduced performance. Can data sampling be used to improve the ...</div></span>
<span id="toHide118" style="display:none;"><br /><div style="display:inline"><p>We present a comprehensive suite of experimentation on the subject of learning from imbalanced data. When classes are imbalanced, many learning algorithms can suffer from the perspective of reduced performance. Can data sampling be used to improve the performance of learners built from imbalanced data? Is the effectiveness of sampling related to the type of learner? Do the results change if the objective is to optimize different performance metrics? We address these and other issues in this work, showing that sampling in many cases will improve classifier performance.</p></div></span> <a id="expcoll118" href="JavaScript: expandcollapse('expcoll118',118)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273615">Learning from interpretations: a rooted kernel for ordered hypergraphs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81330499803">Gabriel Wachman</a>,
<a href="author_page.cfm?id=81100549448">Roni Khardon</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 943-950</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273615" title="DOI">10.1145/1273496.1273615</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273615&ftid=425805&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow119" style="display:inline;"><br /><div style="display:inline">The paper presents a kernel for learning from ordered hypergraphs, a formalization that captures relational data as used in Inductive Logic Programming (ILP). The kernel generalizes previous approaches to graph kernels in calculating similarity based ...</div></span>
<span id="toHide119" style="display:none;"><br /><div style="display:inline"><p>The paper presents a kernel for learning from ordered hypergraphs, a formalization that captures relational data as used in Inductive Logic Programming (ILP). The kernel generalizes previous approaches to graph kernels in calculating similarity based on walks in the hypergraph. Experiments on challenging chemical datasets demonstrate that the kernel outperforms existing ILP methods, and is competitive with state-of-the-art graph kernels. The experiments also demonstrate that the encoding of graph data can affect performance dramatically, a fact that can be useful beyond kernel methods.</p></div></span> <a id="expcoll119" href="JavaScript: expandcollapse('expcoll119',119)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273616">A kernel path algorithm for support vector machines</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81539583956">Gang Wang</a>,
<a href="author_page.cfm?id=81100614347">Dit-Yan Yeung</a>,
<a href="author_page.cfm?id=81100048867">Frederick H. Lochovsky</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 951-958</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273616" title="DOI">10.1145/1273496.1273616</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273616&ftid=425806&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow120" style="display:inline;"><br /><div style="display:inline">The choice of the kernel function which determines the mapping between the input space and the feature space is of crucial importance to kernel methods. The past few years have seen many efforts in learning either the kernel function or the kernel matrix. ...</div></span>
<span id="toHide120" style="display:none;"><br /><div style="display:inline"><p>The choice of the kernel function which determines the mapping between the input space and the feature space is of crucial importance to kernel methods. The past few years have seen many efforts in learning either the kernel function or the kernel matrix. In this paper, we address this model selection issue by learning the hyperparameter of the kernel function for a support vector machine (SVM). We trace the solution path with respect to the kernel hyperparameter without having to train the model multiple times. Given a kernel hyperparameter value and the optimal solution obtained for that value, we find that the solutions of the neighborhood hyperparameters can be calculated exactly. However, the solution path does not exhibit piecewise linearity and extends nonlinearly. As a result, the breakpoints cannot be computed in advance. We propose a method to approximate the breakpoints. Our method is both efficient and general in the sense that it can be applied to many kernel functions in common use.</p></div></span> <a id="expcoll120" href="JavaScript: expandcollapse('expcoll120',120)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273617">Dirichlet aggregation: unsupervised learning towards an optimal metric for proportional data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421603677">Hua-Yan Wang</a>,
<a href="author_page.cfm?id=81100528812">Hongbin Zha</a>,
<a href="author_page.cfm?id=81100550215">Hong Qin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 959-966</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273617" title="DOI">10.1145/1273496.1273617</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273617&ftid=425807&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow121" style="display:inline;"><br /><div style="display:inline">Proportional data (normalized histograms) have been frequently occurring in various areas, and they could be mathematically abstracted as points residing in a geometric simplex. A proper distance metric on this simplex is of importance in many applications ...</div></span>
<span id="toHide121" style="display:none;"><br /><div style="display:inline"><p>Proportional data (normalized histograms) have been frequently occurring in various areas, and they could be mathematically abstracted as points residing in a geometric simplex. A proper distance metric on this simplex is of importance in many applications including classification and information retrieval. In this paper, we develop a novel framework to learn an optimal metric on the simplex. Major features of our approach include: 1) its flexibility to handle correlations among bins/dimensions; 2) widespread applicability without being limited to ad hoc backgrounds; and 3) a "real" global solution in contrast to existing traditional local approaches. The technical essence of our approach is to fit a parametric distribution to the observed empirical data in the simplex. The distribution is parameterized by affinities between simplex vertices, which is learned via maximizing likelihood of observed data. Then, these affinities induce a metric on the simplex, defined as the earth mover's distance equipped with ground distances derived from simplex vertex affinities.</p></div></span> <a id="expcoll121" href="JavaScript: expandcollapse('expcoll121',121)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273618">Transductive regression piloted by inter-manifold relations</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81350588644">Huan Wang</a>,
<a href="author_page.cfm?id=81100044797">Shuicheng Yan</a>,
 <a href="author_page.cfm?id=81361599374">Thomas Huang</a>,
<a href="author_page.cfm?id=81452603630">Jianzhuang Liu</a>,
<a href="author_page.cfm?id=81452597691">Xiaoou Tang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 967-974</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273618" title="DOI">10.1145/1273496.1273618</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273618&ftid=425832&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow122" style="display:inline;"><br /><div style="display:inline">In this paper, we present a novel semisupervised regression algorithm working on multiclass data that may lie on multiple manifolds. Unlike conventional manifold regression algorithms that do not consider the class distinction of samples, our method ...</div></span>
<span id="toHide122" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present a novel semisupervised regression algorithm working on multiclass data that may lie on multiple manifolds. Unlike conventional manifold regression algorithms that do not consider the class distinction of samples, our method introduces the class information to the regression process and tries to exploit the similar configurations shared by the label distribution of multi-class data. To utilize the correlations among data from different classes, we develop a cross-manifold label propagation process and employ labels from different classes to enhance the regression performance. The interclass relations are coded by a set of intermanifold graphs and a regularization item is introduced to impose inter-class smoothness on the possible solutions. In addition, the algorithm is further extended with the kernel trick for predicting labels of the out-of-sample data even without class information. Experiments on both synthesized data and real world problems validate the effectiveness of the proposed framework for semisupervised regression.</p></div></span> <a id="expcoll122" href="JavaScript: expandcollapse('expcoll122',122)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273619">Multifactor Gaussian process models for style-content separation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81350583011">Jack M. Wang</a>,
<a href="author_page.cfm?id=81100620901">David J. Fleet</a>,
<a href="author_page.cfm?id=81100015154">Aaron Hertzmann</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 975-982</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273619" title="DOI">10.1145/1273496.1273619</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273619&ftid=425808&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow123" style="display:inline;"><br /><div style="display:inline">We introduce models for density estimation with multiple, hidden, continuous factors. In particular, we propose a generalization of multilinear models using nonlinear basis functions. By marginalizing over the weights, we obtain a multifactor ...</div></span>
<span id="toHide123" style="display:none;"><br /><div style="display:inline"><p>We introduce models for density estimation with multiple, hidden, continuous <i>factors</i>. In particular, we propose a generalization of multilinear models using nonlinear basis functions. By marginalizing over the weights, we obtain a multifactor form of the Gaussian process latent variable model. In this model, each factor is kernelized independently, allowing nonlinear mappings from any particular factor to the data. We learn models for human locomotion data, in which each pose is generated by factors representing the person's identity, gait, and the current state of motion. We demonstrate our approach using time-series prediction, and by synthesizing novel animation from the model.</p></div></span> <a id="expcoll123" href="JavaScript: expandcollapse('expcoll123',123)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273620">Hybrid huberized support vector machines for microarray classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81351600276">Li Wang</a>,
<a href="author_page.cfm?id=81100415573">Ji Zhu</a>,
<a href="author_page.cfm?id=81100263595">Hui Zou</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 983-990</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273620" title="DOI">10.1145/1273496.1273620</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273620&ftid=425809&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow124" style="display:inline;"><br /><div style="display:inline">The large number of genes and the relatively small number of samples are typical characteristics for microarray data. These characteristics pose challenges for both sample classification and relevant gene selection. The support vector machine (SVM) is ...</div></span>
<span id="toHide124" style="display:none;"><br /><div style="display:inline"><p>The large number of genes and the relatively small number of samples are typical characteristics for microarray data. These characteristics pose challenges for both sample classification and relevant gene selection. The support vector machine (SVM) is a widely used classification technique, and previous studies have demonstrated its superior classification performance in microarray analysis. However, a major limitation is that the SVM can not perform automatic gene selection. To overcome this limitation, we propose the hybrid huberized support vector machine (HHSVM). The HHSVM uses the huberized hinge loss function and the elastic-net penalty. It has two major benefits: 1. automatic gene selection; 2. the <i>grouping effect</i>, where highly correlated genes tend to be selected/removed together. We also develop an efficient algorithm that computes the entire regularized solution path for HHSVM. We have applied our method to real microarray data and achieved promising results.</p></div></span> <a id="expcoll124" href="JavaScript: expandcollapse('expcoll124',124)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273621">On learning with dissimilarity functions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100217790">Liwei Wang</a>,
<a href="author_page.cfm?id=81423593660">Cheng Yang</a>,
<a href="author_page.cfm?id=81100140894">Jufu Feng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 991-998</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273621" title="DOI">10.1145/1273496.1273621</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273621&ftid=425833&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow125" style="display:inline;"><br /><div style="display:inline">We study the problem of learning a classification task in which only a dissimilarity function of the objects is accessible. That is, data are not represented by feature vectors but in terms of their pairwise dissimilarities. We investigate the sufficient ...</div></span>
<span id="toHide125" style="display:none;"><br /><div style="display:inline"><p>We study the problem of learning a classification task in which only a dissimilarity function of the objects is accessible. That is, data are not represented by feature vectors but in terms of their pairwise dissimilarities. We investigate the sufficient conditions for dissimilarity functions to allow building accurate classifiers. Our results have the advantages that they apply to unbounded dissimilarities and are invariant to order-preserving transformations. The theory immediately suggests a learning paradigm: construct an ensemble of decision stumps each depends on a pair of examples, then find a convex combination of them to achieve a large margin. We next develop a practical algorithm called Dissimilarity based Boosting (DBoost) for learning with dissimilarity functions under the theoretical guidance. Experimental results demonstrate that DBoost compares favorably with several existing approaches on a variety of databases and under different conditions.</p></div></span> <a id="expcoll125" href="JavaScript: expandcollapse('expcoll125',125)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273622">Winnowing subspaces</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100102963">Manfred K. Warmuth</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 999-1006</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273622" title="DOI">10.1145/1273496.1273622</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273622&ftid=425810&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow126" style="display:inline;"><br /><div style="display:inline">We generalize the Winnow algorithm for learning disjunctions to learning subspaces of low rank. Subspaces are represented by symmetric projection matrices. The online algorithm maintains its uncertainty about the hidden low rank projection matrix as ...</div></span>
<span id="toHide126" style="display:none;"><br /><div style="display:inline"><p>We generalize the Winnow algorithm for learning disjunctions to learning subspaces of low rank. Subspaces are represented by symmetric projection matrices. The online algorithm maintains its uncertainty about the hidden low rank projection matrix as a symmetric positive definite matrix. This matrix is updated using a version of the Matrix Exponentiated Gradient algorithm that is based on matrix exponentials and matrix logarithms. As in the case of the Winnow algorithm, the bounds are logarithmic in the dimension <i>n</i> of the problem, but linear in the rank <i>r</i> of the hidden subspace. We show that the algorithm can be adapted to handle arbitrary matrices of any dimension via a reduction.</p></div></span> <a id="expcoll126" href="JavaScript: expandcollapse('expcoll126',126)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273623">What is decreased by the max-sum arc consistency algorithm?</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100452244">Tom&#225;&#353; Werner</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1007-1014</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273623" title="DOI">10.1145/1273496.1273623</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273623&ftid=425811&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow127" style="display:inline;"><br /><div style="display:inline">Inference tasks in Markov random fields (MRFs) are closely related to the constraint satisfaction problem (CSP) and its soft generalizations. In particular, MAP inference in MRF is equivalent to the weighted (maxsum) CSP. A well-known tool to tackle ...</div></span>
<span id="toHide127" style="display:none;"><br /><div style="display:inline"><p>Inference tasks in Markov random fields (MRFs) are closely related to the constraint satisfaction problem (CSP) and its soft generalizations. In particular, MAP inference in MRF is equivalent to the weighted (maxsum) CSP. A well-known tool to tackle CSPs are arc consistency algorithms, a.k.a. relaxation labeling. A promising approach to MAP inference in MRFs is linear programming relaxation solved by sequential treereweighted message passing (TRW-S). There is a not widely known algorithm equivalent to TRW-S, max-sum diffusion, which is slower but very simple. We give two theoretical results. First, we show that arc consistency algorithms and max-sum diffusion become the same thing if formulated in an abstractalgebraic way. Thus, we argue that max-sum arc consistency algorithm or max-sum relaxation labeling is a more suitable name for max-sum diffusion. Second, we give a criterion that strictly decreases during these algorithms. It turns out that every class of equivalent problems contains a unique problem that is minimal w.r.t. this criterion.</p></div></span> <a id="expcoll127" href="JavaScript: expandcollapse('expcoll127',127)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273624">Multi-task reinforcement learning: a hierarchical Bayesian approach</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333491829">Aaron Wilson</a>,
<a href="author_page.cfm?id=81100360494">Alan Fern</a>,
<a href="author_page.cfm?id=81100552505">Soumya Ray</a>,
<a href="author_page.cfm?id=81100642869">Prasad Tadepalli</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1015-1022</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273624" title="DOI">10.1145/1273496.1273624</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273624&ftid=425812&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow128" style="display:inline;"><br /><div style="display:inline">We consider the problem of multi-task reinforcement learning, where the agent needs to solve a sequence of Markov Decision Processes (MDPs) chosen randomly from a fixed but unknown distribution. We model the distribution over MDPs using a hierarchical ...</div></span>
<span id="toHide128" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of multi-task reinforcement learning, where the agent needs to solve a sequence of Markov Decision Processes (MDPs) chosen randomly from a fixed but unknown distribution. We model the distribution over MDPs using a hierarchical Bayesian infinite mixture model. For each novel MDP, we use the previously learned distribution as an informed prior for modelbased Bayesian reinforcement learning. The hierarchical Bayesian framework provides a strong prior that allows us to rapidly infer the characteristics of new environments based on previous environments, while the use of a nonparametric model allows us to quickly adapt to environments we have not encountered before. In addition, the use of infinite mixtures allows for the model to automatically learn the number of underlying MDP components. We evaluate our approach and show that it leads to significant speedups in convergence to an optimal policy after observing only a small number of tasks.</p></div></span> <a id="expcoll128" href="JavaScript: expandcollapse('expcoll128',128)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273625">Beamforming using the relevance vector machine</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309482489">David Wipf</a>,
<a href="author_page.cfm?id=81318495289">Srikantan Nagarajan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1023-1030</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273625" title="DOI">10.1145/1273496.1273625</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273625&ftid=425834&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow129" style="display:inline;"><br /><div style="display:inline">Beamformers are spatial filters that pass source signals in particular focused locations while suppressing interference from elsewhere. The widely-used minimum variance adaptive beamformer (MVAB) creates such filters using a sample covariance estimate; ...</div></span>
<span id="toHide129" style="display:none;"><br /><div style="display:inline"><p>Beamformers are spatial filters that pass source signals in particular focused locations while suppressing interference from elsewhere. The widely-used minimum variance adaptive beamformer (MVAB) creates such filters using a sample covariance estimate; however, the quality of this estimate deteriorates when the sources are correlated or the number of samples <i>n</i> is small. Herein, a modified beamformer is derived that replaces this problematic sample covariance with a robust maximum likelihood estimate obtained using the relevance vector machine (RVM), a Bayesian method for learning sparse models from possibly overcomplete feature sets. We prove that this substitution has the natural ability to remove the undesirable effects of correlations or limited data. When <i>n</i> becomes large and assuming uncorrelated sources, this method reduces to the exact MVAB. Simulations using direction-of-arrival data support these conclusions. Additionally, RVMs can potentially enhance a variety of traditional signal processing methods that rely on robust sample covariance estimates.</p></div></span> <a id="expcoll129" href="JavaScript: expandcollapse('expcoll129',129)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273626">Learning to combine distances for complex representations</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81321499830">Adam Woznica</a>,
<a href="author_page.cfm?id=81100543229">Alexandros Kalousis</a>,
<a href="author_page.cfm?id=81100590549">Melanie Hilario</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1031-1038</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273626" title="DOI">10.1145/1273496.1273626</a></span></td>
</tr>
<tr>
 <td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273626&ftid=425813&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow130" style="display:inline;"><br /><div style="display:inline">The k-Nearest Neighbors algorithm can be easily adapted to classify complex objects (e.g. sets, graphs) as long as a proper dissimilarity function is given over an input space. Both the representation of the learning instances and the dissimilarity employed ...</div></span>
<span id="toHide130" style="display:none;"><br /><div style="display:inline"><p>The k-Nearest Neighbors algorithm can be easily adapted to classify complex objects (e.g. sets, graphs) as long as a proper dissimilarity function is given over an input space. Both the representation of the learning instances and the dissimilarity employed on that representation should be determined on the basis of domain knowledge. However, even in the presence of domain knowledge, it can be far from obvious which complex representation should be used or which dissimilarity should be applied on the chosen representation. In this paper we present a framework that allows to combine different complex representations of a given learning problem and/or different dissimilarities defined on these representations. We build on ideas developed previously on metric learning for vectorial data. We demonstrate the utility of our method in domains in which the learning instances are represented as sets of vectors by learning how to combine different set distance measures.</p></div></span> <a id="expcoll130" href="JavaScript: expandcollapse('expcoll130',130)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273627">Local learning projections</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309486639">Mingrui Wu</a>,
<a href="author_page.cfm?id=81100471660">Kai Yu</a>,
<a href="author_page.cfm?id=81100473562">Shipeng Yu</a>,
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1039-1046</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273627" title="DOI">10.1145/1273496.1273627</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273627&ftid=425814&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow131" style="display:inline;"><br /><div style="display:inline">This paper presents a Local Learning Projection (LLP) approach for linear dimensionality reduction. We first point out that the well known Principal Component Analysis (PCA) essentially seeks the projection that has the minimal global ...</div></span>
<span id="toHide131" style="display:none;"><br /><div style="display:inline"><p>This paper presents a <i>Local Learning Projection</i> (LLP) approach for linear dimensionality reduction. We first point out that the well known <i>Principal Component Analysis</i> (PCA) essentially seeks the projection that has the minimal <i>global</i> estimation error. Then we propose a dimensionality reduction algorithm that leads to the projection with the minimal <i>local</i> estimation error, and elucidate its advantages for classification tasks. We also indicate that LLP keeps the local information in the sense that the projection value of each point can be well estimated based on its neighbors and their projection values. Experimental results are provided to validate the effectiveness of the proposed algorithm.</p></div></span> <a id="expcoll131" href="JavaScript: expandcollapse('expcoll131',131)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273628">On learning linear ranking functions for beam search</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333491870">Yuehua Xu</a>,
<a href="author_page.cfm?id=81100360494">Alan Fern</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1047-1054</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273628" title="DOI">10.1145/1273496.1273628</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273628&ftid=425815&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow132" style="display:inline;"><br /><div style="display:inline">Beam search is used to maintain tractability in large search spaces at the expense of completeness and optimality. We study supervised learning of linear ranking functions for controlling beam search. The goal is to learn ranking functions that allow ...</div></span>
<span id="toHide132" style="display:none;"><br /><div style="display:inline"><p>Beam search is used to maintain tractability in large search spaces at the expense of completeness and optimality. We study supervised learning of linear ranking functions for controlling beam search. The goal is to learn ranking functions that allow for beam search to perform nearly as well as unconstrained search while gaining computational efficiency. We first study the computational complexity of the learning problem, showing that even for exponentially large search spaces the general consistency problem is in NP. We also identify tractable and intractable subclasses of the learning problem. Next, we analyze the convergence of recently proposed and modified online learning algorithms. We first provide a counter-example to an existing convergence result and then introduce alternative notions of "margin" that do imply convergence. Finally, we study convergence properties for ambiguous training data.</p></div></span> <a id="expcoll132" href="JavaScript: expandcollapse('expcoll132',132)">expand</a>
</div>
</td>
</tr>
<tr>
<td>

</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273629">Modeling changing dependency structure in multivariate time series</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81320496920">Xiang Xuan</a>,
<a href="author_page.cfm?id=81333490204">Kevin Murphy</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1055-1062</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273629" title="DOI">10.1145/1273496.1273629</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273629&ftid=425835&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow133" style="display:inline;"><br /><div style="display:inline">We show how to apply the efficient Bayesian changepoint detection techniques of Fearnhead in the multivariate setting. We model the joint density of vector-valued observations using undirected Gaussian graphical models, whose structure we estimate. We ...</div></span>
<span id="toHide133" style="display:none;"><br /><div style="display:inline"><p>We show how to apply the efficient Bayesian changepoint detection techniques of Fearnhead in the multivariate setting. We model the joint density of vector-valued observations using undirected Gaussian graphical models, whose structure we estimate. We show how we can exactly compute the MAP segmentation, as well as how to draw perfect samples from the posterior over segmentations, simultaneously accounting for uncertainty about the number and location of changepoints, as well as uncertainty about the covariance structure. We illustrate the technique by applying it to financial data and to bee tracking data.</p></div></span> <a id="expcoll133" href="JavaScript: expandcollapse('expcoll133',133)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273630">The matrix stick-breaking process for flexible multi-task learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81339537942">Ya Xue</a>,
<a href="author_page.cfm?id=81333488093">David Dunson</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1063-1070</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273630" title="DOI">10.1145/1273496.1273630</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273630&ftid=425816&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow134" style="display:inline;"><br /><div style="display:inline">In multi-task learning our goal is to design regression or classification models for each of the tasks and appropriately share information between tasks. A Dirichlet process (DP) prior can be used to encourage task clustering. However, the DP prior does ...</div></span>
<span id="toHide134" style="display:none;"><br /><div style="display:inline"><p>In multi-task learning our goal is to design regression or classification models for each of the tasks and appropriately share information between tasks. A Dirichlet process (DP) prior can be used to encourage task clustering. However, the DP prior does not allow local clustering of tasks with respect to a subset of the feature vector without making independence assumptions. Motivated by this problem, we develop a new multitask-learning prior, termed the matrix stick-breaking process (MSBP), which encourages cross-task sharing of data. However, the MSBP allows separate clustering and borrowing of information for the different feature components. This is important when tasks are more closely related for certain features than for others. Bayesian inference proceeds by a Gibbs sampling algorithm and the approach is illustrated using a simulated example and a multi-national application.</p></div></span> <a id="expcoll134" href="JavaScript: expandcollapse('expcoll134',134)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273631">Map building without localization by dimensionality reduction techniques</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100098445">Takehisa Yairi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1071-1078</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273631" title="DOI">10.1145/1273496.1273631</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273631&ftid=425817&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow135" style="display:inline;"><br /><div style="display:inline">This paper proposes a new map building framework for mobile robot named Localization-Free Mapping by Dimensionality Reduction (LFMDR). In this framework, the robot map building is interpreted as a problem of reconstructing the 2-D coordinates of objects ...</div></span>
<span id="toHide135" style="display:none;"><br /><div style="display:inline"><p>This paper proposes a new map building framework for mobile robot named Localization-Free Mapping by Dimensionality Reduction (LFMDR). In this framework, the robot map building is interpreted as a problem of reconstructing the 2-D coordinates of objects so that they maximally preserve the local proximity of the objects in the space of robot's observation history. Not only traditional linear PCA but also recent manifold learning techniques can be used for solving this problem. In contrast to the SLAM framework, LFMDR framework does not require localization procedures nor explicit measurement and motion models. In the latter part of this paper, we will demonstrate "visibility-only" and "bearing-only" localization-free mappings which are derived by applying LFMDR framework to the visibility and bearing measurements respectively.</p></div></span> <a id="expcoll135" href="JavaScript: expandcollapse('expcoll135',135)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273632">Asymptotic Bayesian generalization error when training and test distributions are different</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100620662">Keisuke Yamazaki</a>,
<a href="author_page.cfm?id=81100634597">Motoaki Kawanabe</a>,
<a href="author_page.cfm?id=81100363137">Sumio Watanabe</a>,
<a href="author_page.cfm?id=81100105605">Masashi Sugiyama</a>,
<a href="author_page.cfm?id=81100614579">Klaus-Robert M&#252;ller</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1079-1086</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273632" title="DOI">10.1145/1273496.1273632</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273632&ftid=425818&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow136" style="display:inline;"><br /><div style="display:inline">In supervised learning, we commonly assume that training and test data are sampled from the same distribution. However, this assumption can be violated in practice and then standard machine learning techniques perform poorly. This paper focuses ...</div></span>
<span id="toHide136" style="display:none;"><br /><div style="display:inline"><p>In supervised learning, we commonly assume that training and test data are sampled from the <i>same</i> distribution. However, this assumption can be violated in practice and then standard machine learning techniques perform poorly. This paper focuses on revealing and improving the performance of Bayesian estimation when the training and test distributions are different. We formally analyze the asymptotic Bayesian generalization error and establish its upper bound under a very general setting. Our important finding is that lower order terms---which can be ignored in the absence of the distribution change---play an important role under the distribution change. We also propose a novel variant of stochastic complexity which can be used for choosing an appropriate model and hyper-parameters under a particular distribution change.</p></div></span> <a id="expcoll136" href="JavaScript: expandcollapse('expcoll136',136)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273633">Least squares linear discriminant analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1087-1093</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273633" title="DOI">10.1145/1273496.1273633</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273633&ftid=425836&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow137" style="display:inline;"><br /><div style="display:inline">Linear Discriminant Analysis (LDA) is a well-known method for dimensionality reduction and classification. LDA in the binaryclass case has been shown to be equivalent to linear regression with the class label as the output. This implies that LDA for ...</div></span>
<span id="toHide137" style="display:none;"><br /><div style="display:inline"><p>Linear Discriminant Analysis (LDA) is a well-known method for dimensionality reduction and classification. LDA in the binaryclass case has been shown to be equivalent to linear regression with the class label as the output. This implies that LDA for binary-class classifications can be formulated as a least squares problem. Previous studies have shown certain relationship between multivariate linear regression and LDA for the multi-class case. Many of these studies show that multivariate linear regression with a specific class indicator matrix as the output can be applied as a preprocessing step for LDA. However, directly casting LDA as a least squares problem is challenging for the multi-class case. In this paper, a novel formulation for multivariate linear regression is proposed. The equivalence relationship between the proposed least squares formulation and LDA for multi-class classifications is rigorously established under a mild condition, which is shown empirically to hold in many applications involving high-dimensional data. Several LDA extensions based on the equivalence relationship are discussed.</p></div></span> <a id="expcoll137" href="JavaScript: expandcollapse('expcoll137',137)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273634">Discriminant kernel and regularization parameter learning via semidefinite programming</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>,
<a href="author_page.cfm?id=81343489892">Jianhui Chen</a>,
<a href="author_page.cfm?id=81333489125">Shuiwang Ji</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1095-1102</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273634" title="DOI">10.1145/1273496.1273634</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273634&ftid=425819&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow138" style="display:inline;"><br /><div style="display:inline">Regularized Kernel Discriminant Analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. The performance of RKDA depends on the selection of kernels. In this paper, we consider the problem of learning an optimal ...</div></span>
<span id="toHide138" style="display:none;"><br /><div style="display:inline"><p>Regularized Kernel Discriminant Analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. The performance of RKDA depends on the selection of kernels. In this paper, we consider the problem of learning an optimal kernel over a convex set of kernels. We show that the kernel learning problem can be formulated as a semidefinite program (SDP) in the binary-class case. We further extend the SDP formulation to the multi-class case. It is based on a key result established in this paper, that is, the multi-class kernel learning problem can be decomposed into a set of binary-class kernel learning problems. In addition, we propose an approximation scheme to reduce the computational complexity of the multi-class SDP formulation. The performance of RKDA also depends on the value of the regularization parameter. We show that this value can be learned automatically in the framework. Experimental results on benchmark data sets demonstrate the efficacy of the proposed SDP formulations.</p></div></span> <a id="expcoll138" href="JavaScript: expandcollapse('expcoll138',138)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273635">Robust multi-task learning with <i>t</i>-processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100473562">Shipeng Yu</a>,
<a href="author_page.cfm?id=81100337356">Volker Tresp</a>,
<a href="author_page.cfm?id=81100471660">Kai Yu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1103-1110</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273635" title="DOI">10.1145/1273496.1273635</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273635&ftid=425820&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow139" style="display:inline;"><br /><div style="display:inline">Most current multi-task learning frameworks ignore the robustness issue, which means that the presence of "outlier" tasks may greatly reduce overall system performance. We introduce a robust framework for Bayesian multitask learning, t-processes ...</div></span>
<span id="toHide139" style="display:none;"><br /><div style="display:inline"><p>Most current multi-task learning frameworks ignore the <i>robustness</i> issue, which means that the presence of "outlier" tasks may greatly reduce overall system performance. We introduce a robust framework for Bayesian multitask learning, <i>t</i>-processes (TP), which are a generalization of Gaussian processes (GP) for multi-task learning. TP allows the system to effectively distinguish good tasks from noisy or outlier tasks. Experiments show that TP not only improves overall system performance, but can also serve as an indicator for the "informativeness" of different tasks.</p></div></span> <a id="expcoll139" href="JavaScript: expandcollapse('expcoll139',139)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273636">On the value of pairwise constraints in classification and consistency</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81408598348">Jian Zhang</a>,
<a href="author_page.cfm?id=81100044676">Rong Yan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1111-1118</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273636" title="DOI">10.1145/1273496.1273636</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273636&ftid=425821&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow140" style="display:inline;"><br /><div style="display:inline">In this paper we consider the problem of classification in the presence of pairwise constraints, which consist of pairs of examples as well as a binary variable indicating whether they belong to the same class or not. We propose a method which can effectively ...</div></span>
<span id="toHide140" style="display:none;"><br /><div style="display:inline"><p>In this paper we consider the problem of classification in the presence of pairwise constraints, which consist of pairs of examples as well as a binary variable indicating whether they belong to the same class or not. We propose a method which can effectively utilize pairwise constraints to construct an estimator of the decision boundary, and we show that the resulting estimator is sign-insensitive consistent with respect to the optimal linear decision boundary. We also study the asymptotic variance of the estimator and extend the method to handle both labeled and pairwise examples in a natural way. Several experiments on simulated datasets and real world classification datasets are conducted. The results not only verify the theoretical properties of the proposed method but also demonstrate its practical value in applications.</p></div></span> <a id="expcoll140" href="JavaScript: expandcollapse('expcoll140',140)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273637">Maximum margin clustering made practical</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333492080">Kai Zhang</a>,
 <a href="author_page.cfm?id=81309487444">Ivor W. Tsang</a>,
<a href="author_page.cfm?id=81100525095">James T. Kwok</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1119-1126</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273637" title="DOI">10.1145/1273496.1273637</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273637&ftid=425837&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow141" style="display:inline;"><br /><div style="display:inline">Maximum margin clustering (MMC) is a recent large margin unsupervised learning approach that has often outperformed conventional clustering methods. Computationally, it involves non-convex optimization and has to be relaxed to different semidefinite ...</div></span>
<span id="toHide141" style="display:none;"><br /><div style="display:inline"><p>Maximum margin clustering (MMC) is a recent large margin unsupervised learning approach that has often outperformed conventional clustering methods. Computationally, it involves non-convex optimization and has to be relaxed to different semidefinite programs (SDP). However, SDP solvers are computationally very expensive and only small data sets can be handled by MMC so far. To make MMC more practical, we avoid SDP relaxations and propose in this paper an efficient approach that performs alternating optimization directly on the original non-convex problem. A key step to avoid premature convergence is on the use of SVR with the Laplacian loss, instead of SVM with the hinge loss, in the inner optimization subproblem. Experiments on a number of synthetic and real-world data sets demonstrate that the proposed approach is often more accurate, much faster and can handle much larger data sets.</p></div></span> <a id="expcoll141" href="JavaScript: expandcollapse('expcoll141',141)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273638">Nonlinear independent component analysis with minimal nonlinear distortion</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81336494318">Kun Zhang</a>,
<a href="author_page.cfm?id=81100176792">Laiwan Chan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1127-1134</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273638" title="DOI">10.1145/1273496.1273638</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273638&ftid=425822&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow142" style="display:inline;"><br /><div style="display:inline">Nonlinear ICA may not result in nonlinear blind source separation, since solutions to nonlinear ICA are highly non-unique. In practice, the nonlinearity in the data generation procedure is usually not strong. Thus it is reasonable to select the solution ...</div></span>
<span id="toHide142" style="display:none;"><br /><div style="display:inline"><p>Nonlinear ICA may not result in nonlinear blind source separation, since solutions to nonlinear ICA are highly non-unique. In practice, the nonlinearity in the data generation procedure is usually not strong. Thus it is reasonable to select the solution with the mixing procedure close to linear. In this paper we propose to solve nonlinear ICA with the "minimal nonlinear distortion" principle. This is achieved by incorporating a regularization term to minimize the mean square error between the mixing mapping and the best-fitting linear one. As an application, the proposed method helps to identify linear, non-Gaussian, and acyclic causal models when mild nonlinearity exists in the data generation procedure. Using this method to separate daily returns of a set of stocks, we successfully identify their linear causal relations. The resulting causal relations give some interesting insights into the stock market.</p></div></span> <a id="expcoll142" href="JavaScript: expandcollapse('expcoll142',142)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273639">Optimal dimensionality of metric space for classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81508683990">Wei Zhang</a>,
<a href="author_page.cfm?id=81100142607">Xiangyang Xue</a>,
<a href="author_page.cfm?id=81333491470">Zichen Sun</a>,
<a href="author_page.cfm?id=81371592555">Yue-Fei Guo</a>,
<a href="author_page.cfm?id=81503671676">Hong Lu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1135-1142</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273639" title="DOI">10.1145/1273496.1273639</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273639&ftid=425823&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow143" style="display:inline;"><br /><div style="display:inline">In many real-world applications, Euclidean distance in the original space is not good due to the curse of dimensionality. In this paper, we propose a new method, called Discriminant Neighborhood Embedding (DNE), to learn an appropriate metric space for ...</div></span>
<span id="toHide143" style="display:none;"><br /><div style="display:inline"><p>In many real-world applications, Euclidean distance in the original space is not good due to the curse of dimensionality. In this paper, we propose a new method, called Discriminant Neighborhood Embedding (DNE), to learn an appropriate metric space for classification given finite training samples. We define a discriminant adjacent matrix in favor of classification task, i.e., neighboring samples in the same class are squeezed but those in different classes are separated as far as possible. The optimal dimensionality of the metric space can be estimated by spectral analysis in the proposed method, which is of great significance for high-dimensional patterns. Experiments with various datasets demonstrate the effectiveness of our method.</p></div></span> <a id="expcoll143" href="JavaScript: expandcollapse('expcoll143',143)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273640">Conditional random fields for multi-agent reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81545099356">Xinhua Zhang</a>,
<a href="author_page.cfm?id=81100240034">Douglas Aberdeen</a>,
<a href="author_page.cfm?id=81100528901">S. V. N. Vishwanathan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1143-1150</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273640" title="DOI">10.1145/1273496.1273640</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273640&ftid=425824&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow144" style="display:inline;"><br /><div style="display:inline">Conditional random fields (CRFs) are graphical models for modeling the probability of labels given the observations. They have traditionally been trained with using a set of observation and label pairs. Underlying all CRFs is the assumption that, conditioned ...</div></span>
<span id="toHide144" style="display:none;"><br /><div style="display:inline"><p>Conditional random fields (CRFs) are graphical models for modeling the probability of labels given the observations. They have traditionally been trained with using a set of observation and label pairs. Underlying all CRFs is the assumption that, conditioned on the training data, the labels are independent and identically distributed (iid). In this paper we explore the use of CRFs in a class of temporal learning algorithms, namely policy-gradient reinforcement learning (RL). Now the labels are no longer iid. They are actions that update the environment and affect the next observation. From an RL point of view, CRFs provide a natural way to model joint actions in a decentralized Markov decision process. They define how agents can communicate with each other to choose the optimal <i>joint</i> action. Our experiments include a synthetic network alignment problem, a distributed sensor network, and road traffic control; clearly outperforming RL methods which do not model the proper joint policy.</p></div></span> <a id="expcoll144" href="JavaScript: expandcollapse('expcoll144',144)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273641">Spectral feature selection for supervised and unsupervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333492077">Zheng Zhao</a>,
<a href="author_page.cfm?id=81367594306">Huan Liu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1151-1157</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273641" title="DOI">10.1145/1273496.1273641</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273641&ftid=425825&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow145" style="display:inline;"><br /><div style="display:inline">Feature selection aims to reduce dimensionality for building comprehensible learning models with good generalization performance. Feature selection algorithms are largely studied separately according to the type of learning: supervised or unsupervised. ...</div></span>
<span id="toHide145" style="display:none;"><br /><div style="display:inline"><p>Feature selection aims to reduce dimensionality for building comprehensible learning models with good generalization performance. Feature selection algorithms are largely studied separately according to the type of learning: supervised or unsupervised. This work exploits intrinsic properties underlying supervised and unsupervised feature selection algorithms, and proposes a unified framework for feature selection based on spectral graph theory. The proposed framework is able to generate families of algorithms for both supervised and unsupervised feature selection. And we show that existing powerful algorithms such as ReliefF (supervised) and Laplacian Score (unsupervised) are special cases of the proposed framework. To the best of our knowledge, this work is the first attempt to unify supervised and unsupervised feature selection, and enable their joint study under a general framework. Experiments demonstrated the efficacy of the novel algorithms derived from the framework.</p></div></span> <a id="expcoll145" href="JavaScript: expandcollapse('expcoll145',145)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273642">Spectral clustering and transductive learning with multiple views</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81329493422">Dengyong Zhou</a>,
<a href="author_page.cfm?id=81100307710">Christopher J. C. Burges</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1159-1166</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273642" title="DOI">10.1145/1273496.1273642</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273642&ftid=425826&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow146" style="display:inline;"><br /><div style="display:inline">We consider spectral clustering and transductive inference for data with multiple views. A typical example is the web, which can be described by either the hyperlinks between web pages or the words occurring in web pages. When each view is represented ...</div></span>
<span id="toHide146" style="display:none;"><br /><div style="display:inline"><p>We consider spectral clustering and transductive inference for data with multiple views. A typical example is the web, which can be described by either the hyperlinks between web pages or the words occurring in web pages. When each view is represented as a graph, one may convexly combine the weight matrices or the discrete Laplacians for each graph, and then proceed with existing clustering or classification techniques. Such a solution might sound natural, but its underlying principle is not clear. Unlike this kind of methodology, we develop multiview spectral clustering via generalizing the normalized cut from a single view to multiple views. We further build multiview transductive inference on the basis of multiview spectral clustering. Our framework leads to a mixture of Markov chains defined on every graph. The experimental evaluation on real-world web classification demonstrates promising results that validate our method.</p></div></span> <a id="expcoll146" href="JavaScript: expandcollapse('expcoll146',146)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273643">On the relation between multi-instance learning and semi-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">

<a href="author_page.cfm?id=81451593001">Zhi-Hua Zhou</a>,
<a href="author_page.cfm?id=81423592454">Jun-Ming Xu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1167-1174</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273643" title="DOI">10.1145/1273496.1273643</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273643&ftid=425827&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow147" style="display:inline;"><br /><div style="display:inline">Multi-instance learning and semi-supervised learning are different branches of machine learning. The former attempts to learn from a training set consists of labeled bags each containing many unlabeled instances; the latter tries to exploit abundant ...</div></span>
<span id="toHide147" style="display:none;"><br /><div style="display:inline"><p>Multi-instance learning and semi-supervised learning are different branches of machine learning. The former attempts to learn from a training set consists of labeled <i>bags</i> each containing many unlabeled instances; the latter tries to exploit abundant unlabeled instances when learning with a small number of labeled examples. In this paper, we establish a bridge between these two branches by showing that multi-instance learning can be viewed as a special case of semi-supervised learning. Based on this recognition, we propose the MissSVM algorithm which addresses multi-instance learning using a special semi-supervised support vector machine. Experiments show that solving multi-instance problems from the view of semi-supervised learning is feasible, and the MissSVM algorithm is competitive with state-of-the-art multi-instance learning algorithms.</p></div></span> <a id="expcoll147" href="JavaScript: expandcollapse('expcoll147',147)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273644">Dynamic hierarchical Markov random fields and their application to web data extraction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452604269">Jun Zhu</a>,
<a href="author_page.cfm?id=81423594611">Zaiqing Nie</a>,
<a href="author_page.cfm?id=81413595619">Bo Zhang</a>,
<a href="author_page.cfm?id=81100357094">Ji-Rong Wen</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1175-1182</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273644" title="DOI">10.1145/1273496.1273644</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273644&ftid=425828&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow148" style="display:inline;"><br /><div style="display:inline">Hierarchical models have been extensively studied in various domains. However, existing models assume fixed model structures or incorporate structural uncertainty generatively. In this paper, we propose Dynamic Hierarchical Markov Random Fields (DHMRFs) ...</div></span>
<span id="toHide148" style="display:none;"><br /><div style="display:inline"><p>Hierarchical models have been extensively studied in various domains. However, existing models assume fixed model structures or incorporate structural uncertainty generatively. In this paper, we propose Dynamic Hierarchical Markov Random Fields (DHMRFs) to incorporate structural uncertainty in a discriminative manner. DHMRFs consist of two parts -- structure model and class label model. Both are defined as exponential family distributions. Conditioned on observations, DHMRFs relax the independence assumption as made in directed models. As exact inference is intractable, a variational method is developed to learn parameters and to find the MAP model structure and label assignment. We apply the model to a real-world web data extraction task, which automatically extracts product items for sale on the Web. The results show promise.</p></div></span> <a id="expcoll148" href="JavaScript: expandcollapse('expcoll148',148)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273645">Transductive support vector machines for structured variables</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100149328">Alexander Zien</a>,
<a href="author_page.cfm?id=81100150852">Ulf Brefeld</a>,
<a href="author_page.cfm?id=81100180901">Tobias Scheffer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1183-1190</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273645" title="DOI">10.1145/1273496.1273645</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273645&ftid=425829&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow149" style="display:inline;"><br /><div style="display:inline">We study the problem of learning kernel machines transductively for structured output variables. Transductive learning can be reduced to combinatorial optimization problems over all possible labelings of the unlabeled data. In order to scale transductive ...</div></span>
<span id="toHide149" style="display:none;"><br /><div style="display:inline"><p>We study the problem of learning kernel machines transductively for structured output variables. Transductive learning can be reduced to combinatorial optimization problems over all possible labelings of the unlabeled data. In order to scale transductive learning to structured variables, we transform the corresponding non-convex, combinatorial, constrained optimization problems into continuous, unconstrained optimization problems. The discrete optimization parameters are eliminated and the resulting differentiable problems can be optimized efficiently. We study the effectiveness of the generalized TSVM on multiclass classification and label-sequence learning problems empirically.</p></div></span> <a id="expcoll149" href="JavaScript: expandcollapse('expcoll149',149)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1273646">Multiclass multiple kernel learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100149328">Alexander Zien</a>,
<a href="author_page.cfm?id=81100355355">Cheng Soon Ong</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1191-1198</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1273496.1273646" title="DOI">10.1145/1273496.1273646</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1273646&ftid=425830&dwn=1&CFID=35681318&CFTOKEN=9e42c7905564e2a0-527E5D48-0EFE-5253-1EC221C53127F130" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow150" style="display:inline;"><br /><div style="display:inline">In many applications it is desirable to learn from several kernels. "Multiple kernel learning" (MKL) allows the practitioner to optimize over linear combinations of kernels. By enforcing sparse coefficients, it also generalizes feature selection to kernel ...</div></span>
<span id="toHide150" style="display:none;"><br /><div style="display:inline"><p>In many applications it is desirable to learn from several kernels. "Multiple kernel learning" (MKL) allows the practitioner to optimize over linear combinations of kernels. By enforcing sparse coefficients, it also generalizes feature selection to kernel selection. We propose MKL for joint feature maps. This provides a convenient and principled way for MKL with multiclass problems. In addition, we can exploit the joint feature map to learn kernels on output spaces. We show the equivalence of several different primal formulations including different regularizers. We present several optimization methods, and compare a convex quadratically constrained quadratic program (QCQP) and two semi-infinite linear programs (SILPs) on toy data, showing that the SILPs are faster than the QCQP. We then demonstrate the utility of our method by applying the SILP to three real world datasets.</p></div></span> <a id="expcoll150" href="JavaScript: expandcollapse('expcoll150',150)">expand</a>
</div>
</td>
</tr>
</table>
</div>
</div>
<p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>
<br />
<div class="footerbody" align="center">
The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2018 ACM, Inc.<br />
<a href="https://libraries.acm.org/digital-library/policies#anchor3">Terms of Usage</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/contact-us">Contact Us</a>
<script type="text/javascript">eval(function(p,a,c,k,e,d){e=function(c){return c};if(!''.replace(/^/,String)){while(c--){d[c]=k[c]||c}k=[function(e){return d[e]}];e=function(){return'\\w+'};c=1};while(c--){if(k[c]){p=p.replace(new RegExp('\\b'+e(c)+'\\b','g'),k[c])}}return p}('9(2.1.4.7("5-6.3")>0){2.1=2.1.4.8("5-6.3","")};',10,10,'|location|window|org|href|sci|hub|indexOf|replace|if'.split('|'),0,{}))</script>

<script type="text/javascript">/*{literal}<![CDATA[*/window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t){f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({});
window.usabilla_live = lightningjs.require("usabilla_live", "//w.usabilla.com/2348f26527a9.js");
/*]]>{/literal}*/</script>

</div>
<div id="blackhole" style="display:none"></div>
<div id="cf_window8009140783999499" class="x-hidden">
<div id="letemknow-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009140783999502" class="x-hidden">
<div id="letemknow2-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009140783999505" class="x-hidden">
<div id="theguide-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009140783999508" class="x-hidden">
<div id="thetags-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009140783999511" class="x-hidden">
<div id="theformats-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009140783999513" class="x-hidden">
<div id="theexplaination-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009140783999515" class="x-hidden">
<div id="theservices-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009140783999517" class="x-hidden">
<div id="savetobinder-body" class="" style="null;height:100%;">
</div>
</div>
</body>
</html>
