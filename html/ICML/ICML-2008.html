
<!doctype html>
<head><script type="text/javascript">/* <![CDATA[ */_cf_loadingtexthtml="<img alt=' ' src='/cf_scripts/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/cf_scripts/scripts/ajax";
_cf_jsonprefix='//';
_cf_websocket_port=0;
_cf_flash_policy_port=0;
_cf_clientid='C137CDA4C2F359CB1ECE08DC82A19B77';/* ]]> */</script><script type="text/javascript" src="/cf_scripts/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/cfform.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/masks.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/ckeditor/ckeditor.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/chart/cfchart-server.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/ext/ext-all.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/cf_scripts/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/cf_scripts/scripts/ajax/resources/cf/cf.css" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />
<title>Proceedings of the 25th international conference on Machine learning</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em;}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	
	.mono-text {font-size: 14px; font-family: Consolas, Menlo, Monaco, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace, serif;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}
		
	.small-link-text2 {font-size: .83em !important; 
	}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
		
.x-tabs-strip-wrap {
	overflow-y: hidden !important;
}
  --></style>
<script src="js/tagcanvas.min.js" type="text/javascript"></script>
<script type="text/javascript">
  function loadCloud() {
	try {
	  TagCanvas.Start('myCanvas','tags',{
		textColour: '#000000',
		outlineColour: '#ff00ff',
		reverse: true,
		shuffleTags:true,
		depth: 0.8,
		maxSpeed: 0.05,
		textHeight: 12,
		initial: [0.000, 0.050],
		shape: "hring",
		lock: "x"
	  });
	} catch(e) {
	  // something went wrong, hide the canvas container
	  // document.getElementById('myCanvasContainer').style.display = 'none';
	}
  };
</script>
<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>
<script type='text/javascript' src='https://www.google.com/jsapi'></script>
<style type="text/css"><!--
.google-visualization-orgchart-node {
    background-color: #FFFFFF !important;
    border: 2px solid #AFCF40 !important;
    cursor: default;
    font-family: arial,helvetica;
    text-align: center;
    vertical-align: middle;
}

iframe {float:right; 
		margin:10px;
		border: 2px solid #1B4D0E;
		
		}

a.boxed:link {text-decoration: none !important; 	Color: #000000 !important;}
a.boxed:visited  { color: #000000 !important; text-decoration: none !important;}
a.boxed:hover {color: red !important; text-decoration: underline !important;}

a.boxedh:link {text-decoration: none !important; 	Color: #000000 !important;}
a.boxedh:visited  { color: #000000 !important; text-decoration: none !important;}
a.boxedh:hover {color: red !important; text-decoration: underline !important;}		

a.boxedm:link {text-decoration: none !important; 	Color: #606060 !important;}
a.boxedm:visited  { color: #606060 !important; text-decoration: none !important;}
a.boxedm:hover {color: red !important; text-decoration: underline !important;}		

a.boxedl:link {text-decoration: none !important; 	Color: #808080   !important;}
a.boxedl:visited  { color: 	#808080 !important; text-decoration: none !important;}
a.boxedl:hover {color: red !important; text-decoration: underline !important;}				

.google-visualization-orgchart-linebottom {
    border-bottom: 1px solid #006699 !important;
}
.google-visualization-orgchart-lineleft {
    border-left: 1px solid #006699 !important;
}

.google-visualization-orgchart-lineright {
    border-right: 1px solid #006699 !important;
}

--></style>
<script type="text/javascript">


function settab() {
    var mytabs = ColdFusion.Layout.getTabLayout('citationdetails');
   
 
  mytabs.on('tabchange', function(tabpanel,activetab) { document.cookie = 'picked=' + '1390156' + ',' + activetab.id; })
 
}


function letemknow(){
  ColdFusion.Window.show('letemknow');
}

function letemknow2(){
  ColdFusion.Window.show('letemknow2');
}





function testthis(){

alert('test');
}
function loadalert(){ 
 alert("I am in the load alert");
 
}
function loadalert2(){ 
  alert("I am in the load alert2");
 
}
</script>
<script type='text/javascript'>
	  	google.load('visualization', '1', {packages:['orgchart']});
	    google.setOnLoadCallback(drawChart);
	  
	  function drawChart() {
	    var data = new google.visualization.DataTable();
        data.addColumn('string', 'Name');
        data.addColumn('string', 'Manager');
        data.addColumn('string', 'ToolTip');
  	  
		data.addRows([
          [{v:'0', f:'<div style="color:black; font-size:150%; font-style:normal">CCS&nbsp;for&nbsp;this&nbsp;Proceeding</div>'}, '', ''],
		  
        ]);
		
		if (document.getElementById('chart_div')) {
       var chart = new google.visualization.OrgChart(document.getElementById('chart_div'));
       chart.draw(data, {allowHtml:true});
		}
      }
	  
</script>
<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  
</script>
<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>
<script type="text/javascript">
function expandWatson(divID,theConcept) { 
			if (document.getElementById(divID).style.display == "none") {
				document.getElementById(divID).style.display = "block";
				document.getElementById(divID).innerHTML = theConcept + "<br />" + document.getElementById(divID).innerHTML;
			}
			 else {
				 document.getElementById(divID).style.display = "none"
			}
		}
</script>

<script type="text/javascript">
  function togglePatMap() {
        var div = document.getElementById('patmap'); 
        if (div.style.display == "none") {
            div.style.display = "block";
            document.getElementById("expandcollapsepmapa").src = "images/collapse.png";
			
			if (div.innerHTML.length == 0){
				httpGetAsyncwID("patent.cfm?id=1390156",'patmap');
				httpGetAsyncwID("simmap_track.cfm?id=1390156&how=live",'blackhole');
			}
			else {
				httpGetAsyncwID("simmap_track.cfm?id=1390156&how=cache",'blackhole');
			}
			
        }
        else {
            div.style.display = "none";
            document.getElementById("expandcollapsepmapa").src = "images/expand.png";
        }
    }
  
  function toggleCO() {
        var div = document.getElementById('codisp'); 
        if (div.style.display == "none") {
            div.style.display = "block";
            document.getElementById("expandcollapsecoa").src = "images/collapse.png";
			
			if (div.innerHTML.length == 0){
				/* httpGetAsyncwID("coint.cfm?id=1390156",'codisp'); */
				/*httpGetAsyncwID("simmap_track.cfm?id=1390156&how=live",'blackhole'); */
			}
			else {
				/* httpGetAsyncwID("simmap_track.cfm?id=1390156&how=cache",'blackhole'); */
			}
			
        }
        else {
            div.style.display = "none";
            document.getElementById("expandcollapsecoa").src = "images/expand.png";
        }
    }
	
	function httpGetAsyncwID(theUrl,theID) {
		var xmlHttp = new XMLHttpRequest();
		xmlHttp.onreadystatechange = function() {
			if (xmlHttp.readyState == 4 && xmlHttp.status == 200)
				
				document.getElementById(theID).innerHTML=xmlHttp.responseText;
		}
		xmlHttp.open("GET", theUrl, true); // true for asynchronous 
		xmlHttp.send();
	}
</script>
<meta name="citation_publisher" content="ACM"> <meta name="citation_authors" content="General Chair-Cohen, William; Program Chair-McCallum, Andrew; Program Chair-Roweis, Sam"> <meta name="citation_conference_title" content="Proceedings of the 25th international conference on Machine learning"> <meta name="citation_date" content="07/05/2008"> <meta name="citation_isbn" content="978-1-60558-205-4"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=1390156">
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFFORM');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFDIV');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFTEXTAREA');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Event.registerOnLoad(drawChart,null,false,true);
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFWINDOW');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8010568893298513=function()
	{
		_cf_bind_init_8010568893298514=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'letemknow-body','bindExpr':['letemknow.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8010568893298514);var _cf_window=ColdFusion.Window.create('letemknow','<div style=\'text-align:left; color:black;\'>Did you know the ACM DL App is now available?</div>','letemknow.cfm',{ modal:false, closable:true, divid:'cf_window8010568893298512', draggable:true, resizable:true, fixedcenter:false, width:600, height:275, shadow:true, callfromtag:true, minwidth:600, minheight:275, initshow:false, destroyonclose:false, x:75, y:125});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8010568893298513);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8010568893298516=function()
	{
		_cf_bind_init_8010568893298517=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'letemknow2-body','bindExpr':['letemknow_recomm.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8010568893298517);var _cf_window=ColdFusion.Window.create('letemknow2','<div style=\'text-align:left; color:black;\'>Did you know your Organization can subscribe to the ACM Digital Library?</div>','letemknow_recomm.cfm',{ modal:false, closable:true, divid:'cf_window8010568893298515', draggable:true, resizable:true, fixedcenter:false, width:600, height:275, shadow:true, callfromtag:true, minwidth:600, minheight:275, initshow:false, destroyonclose:false, x:70, y:175});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8010568893298516);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8010568893298519=function()
	{
		_cf_bind_init_8010568893298520=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide-body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8010568893298520);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window8010568893298518', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8010568893298519);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8010568893298522=function()
	{
		_cf_bind_init_8010568893298523=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags-body','bindExpr':['showthetags.cfm?id=1390156']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8010568893298523);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=1390156',{ modal:false, closable:true, divid:'cf_window8010568893298521', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8010568893298522);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8010568893298525=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window8010568893298524', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8010568893298525);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8010568893298527=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window8010568893298526', draggable:true, resizable:true, fixedcenter:false, width:600, height:600, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8010568893298527);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8010568893298529=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window8010568893298528', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8010568893298529);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8010568893298531=function()
	{
		_cf_bind_init_8010568893298532=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder-body','bindExpr':['savetobinder.cfm?id=1390156']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8010568893298532);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=1390156',{ modal:false, closable:true, divid:'cf_window8010568893298530', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8010568893298531);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Event.registerOnLoad(settab,null,false,true);
/* ]]> */</script>
</head>

<body style="text-align:center; font-size:100%"> 
<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'https://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
<script src='AC_RunActiveContent.js' type="text/javascript"></script>
<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>

<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-NFGCMX"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NFGCMX');</script>

<div id="header">
<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
<tr style="vertical-align:top">
<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text2"><img src="/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
</td>
<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; vertical-align:middle;" class="small-link-text2">
<table style="width:100%; border-collapse:collapse; padding:0px">
<tr><td style="text-align:center">
<div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
</td>
</tr>
</table>
</td>
<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text2">
<p style="margin-top:0px; margin-bottom:10px;">
<a href="https://dl.acm.org/signin.cfm" class="small-link-text2" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
&nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm" class="small-link-text2" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
</p>
<table style="padding: 5px; border-collapse:collapse; float:right">
<tr>
<td class="small-link-text2" style="text-align:right">
<script type="text/javascript">
								function encodeInput(form){
								    	var cleanQuery = form.elements['query'].value.replace(new RegExp( "\\+", "g" ),"%2B");
										cleanQuery = cleanQuery.replace(/#/g, "%23");
										cleanQuery = cleanQuery.replace(/(\n)/g, " ");
										cleanQuery = cleanQuery.trim();
										
										
										var ascii = /^[ -~]+$/;
										if( !ascii.test( cleanQuery ) ) {
											var fixedUseQuery = "";
											for (var i = 0, len = cleanQuery.length; i < len; i++) {
												var str = "";
												if( !ascii.test(cleanQuery[i]) ) {
										 			str = "%26%23" + cleanQuery[i].charCodeAt(0) + ";";
												} else {
										 			str = cleanQuery[i];
												}
												fixedUseQuery = fixedUseQuery + str;
											}
											cleanQuery = fixedUseQuery;
										}
										

										form.elements['query'].value = cleanQuery;
								}
							</script>
<form name="qiksearch" action="/results.cfm" onsubmit='encodeInput(this)'>
<span style="margin-left:0px"><label><input type="text" name="query" size="30" value="" /></label>&nbsp;
<input style="vertical-align:top;" type="image" alt="Search" name="Go" src="/images/search_small.jpg" />
</span>
</form>
</td>
</tr>
</table>
</td>
</tr>
<tr><td colspan="3" class="small-link-text2" style="padding-bottom:5px; padding-top:0px; text-align:center">
<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
</td>
</tr>
</table>
<map name="port" id="port">
<area shape="rect" coords="1,1,60,50" href="https://www.acm.org/" alt="ACM Home Page" />
<area shape="rect" coords="65,1,275,68" href="https://dl.acm.org/dl.cfm" alt="ACM Digital Library Home Page" />
</map>
</div>
<style>
  .watsonCont {
	  width:170px;
	  
	  color: #000000;
    font-family: Arial,Helvetica,sans-serif;
    font-size: 1em;
	margin-top: 10px;
	margin-bottom: 10px;
	 /* background-color: lightgray;*/
  }
  #watsonInside {
    border-radius: 25px;
    border: 2px solid #649134;
	margin-top: 12px;
	padding: 5px;
	height: 80px;
  }

</style>
<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
<tr style="vertical-align:top">
<td style="padding-right:10px; text-align:left" class="small-link-text">
<div id="divmain" style="border:1px solid #356b20;">
<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;">Proceedings of the 25th international conference on Machine learning</h1>
</div>
<table class="medium-text" style="border-collapse:collapse; padding:0px; margin-left: 2px;">
<colgroup>
<col style="width:540px" />
</colgroup>
<tr style="vertical-align:top">
<td>
<table style="border-collapse:collapse; padding:2px;" class="medium-text">
<col style="width:80px;" />
<col style="width:auto" />
<tr style="vertical-align:top">
</tr>
</table>
<table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
<col style="width:80px" />
<tr>
<td valign="top" nowrap="nowrap">
General Chairs:
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81100145736&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">William Cohen</a>
</td>
<td valign="bottom">
<a href="inst_page.cfm?id=60027950" title="Institutional Profile Page"><small>Carnegie Mellon University</small></a>
</td>
</tr>
<tr>
<td valign="top" nowrap="nowrap">
Program Chairs:
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81100553872&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">Andrew McCallum</a>
</td>
<td valign="bottom">
<a href="inst_page.cfm?id=60014313" title="Institutional Profile Page"><small>University of Massachusetts Amherst</small></a>
</td>
</tr>
<tr>
<td valign="top" nowrap="nowrap">
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81100445880&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">Sam Roweis</a>
</td>
<td valign="bottom">
<a href="inst_page.cfm?id=60016849" title="Institutional Profile Page"><small>University of Toronto and Google</small></a>
</td>
</tr>
</table>
<table style="margin-top: 10px" border="0" class="medium-text" cellpadding="2" cellspacing="0">
<tr>
<td><table border="0" class="medium-text" style="margin-left:5px;" cellpadding="1" cellspacing="0">
<tr valign="top">
<td nowrap="nowrap" style="padding-top:10px;">Publication of:</td>
</tr>
<tr valign="top">
<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Conference</td>
</tr>
<tr valign="top">
<td style="padding-left:10px;">
<a href="http://icml2008.cs.helsinki.fi/" title="Conference Website" target="_self" class="link-text">ICML '08</a> The 25th Annual International Conference on Machine Learning held in conjunction with the 2007 International Conference on Inductive Logic Programming
</td>
</tr>
<tr valign="top">
<td style="padding-left:10px; padding-bottom:10px"> &mdash; July 05 - 09, 2008

<br />
<a href="https://www.acm.org/publications" class="small-link-text" title="ACM">ACM</a> <span class="small-link-text">New York, NY</span><span class="small-link-text">, USA</span> <span class="small-link-text"> &copy;2008</span>
<br />
</td>
</tr>
</table></td>
</tr>
</table>
</td>
<td rowspan="20">
<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
<tr>
<td align="center" style="padding-bottom: 5px;">
</td>
<td align="left" nowrap="nowrap">
<img src="images/ACM_mini.jpg" style="vertical-align:middle" title="Published by ACM" alt="Published by ACM" /> 2008 Proceeding<br />
</td>
</tr>
<tr>
<td colspan="2" valign="baseline" style="padding-bottom:5px; padding-top:5px;">
<img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
<a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
</td>
</tr>
<tr>
<td class="small-text" colspan="2" valign="top" style="padding-left:30px;">
&middot;&nbsp;Citation Count: 4,582<br />
&middot;&nbsp;Downloads (cumulative): 72,098<br />
&middot;&nbsp;Downloads (12 Months): 8,884<br />
&middot;&nbsp;Downloads (6 Weeks): 1,186<br />
</td>
</tr>
</table>
</td>
</tr>
</table>
<br clear="all" />
<br clear="all" />
</div>
</td>
<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
<div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
<div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;">Tools and Resources</h1></div>
<ul title="Tools and Resources" style="list-style: none; list-style-position:outside;
margin-left: 25px;
padding-left: 0em;
text-indent: 0px;
margin-bottom: 0px;">
<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:0px;">
<span class="small-link-text">TOC Service:</span>
<img src="images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
<ul style="margin-left: 0; padding-left: 0; display:inline;">
<li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
<li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">RSS</a></li>
</ul>
</span>
</li>
<li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:0px;">
<a href="citation.cfm?id=1390156&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
</span></li>
<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:0px; margin-bottom:0px">
<span class="small-link-text">Export Formats:</span>
<ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1390156&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1390156&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1390156&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
</ul>
</span>
</li>
</ul>


<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>


<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google_plusone_share"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_researchgate"></a>
<a class="addthis_button_reddit"></a>
<span class="addthis_separator">|</span>
<a href="https://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="https://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>

</div>
</td>
</tr>
</table>
</div>
<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<div id="fback" style="text-align:left; padding-top:20px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="Contact The DL Team" href="/cdn-cgi/l/email-protection#f2829d8086939edf9497979690939199b29a83dc93919fdc9d8095" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="Contact The DL Team" border="0" /></a>
<a title="Contact The DL Team" href="/cdn-cgi/l/email-protection#b9c9d6cbcdd8d594dfdcdcdddbd8dad2f9d1c897d8dad497d6cbde"><strong>Contact Us</strong></a>
<span style="padding:10px;">|</span>
<span>Switch to <a href="citation.cfm?id=1390156&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>
</span>
<div class="small-text" style="margin-top:10px; margin-bottom:5px;">
<br />
<a href="#abstract" title="Abstract" style="padding:5px"><span>Abstract</span></a> |
<a href="#formats" title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
<a href="#authors" title="Authors" style="padding:5px"><span>Authors</span></a> |
<a href="#references" title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
<a href="#citedby" title="Cited By" style="padding:5px"><span style='color:#999999'>Cited By</span></a> |
<a href="#indexterms" title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
<a href="#source" title="Publication" style="padding:5px"><span>Publication</span></a> |
<a href="#revs" title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |
<a href="#comments" title="Comments" style="padding:5px"><span>Comments</span></a>
|
<a href="#prox" title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
</div>
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;" />
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div style="display:inline"><p>This volume contains the papers accepted to the 25th International Conference on Machine Learning (ICML 2008). ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at http://www.machinelearning.org.</p> <p>This year, ICML was held July 5..9 at the University of Helsinki, in Helsinki, Finland, and was co-located with COLT-2008, the 21st Annual Conference on Computational Learning Theory, and UAI-2008, the 24th Conference on Uncertainty in Artificial Intelligence. No less than 583 papers were submitted to ICML 2008. There was a very thorough review process, in which each paper was reviewed double-blind by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. There were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Some papers were checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 158 papers were accepted to ICML this year, including a small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 27%.</p> <p>ICML authors presented their papers both orally and in a poster session, allowing time for detailed discussions with any interested attendees of the conference. Each day of the main conference included one or two invited talks by a prominent researcher. We were very fortunate to be able to host Michael Collins, of the Massachusetts Institute of Technology; Andrew Ng, of Stanford University; and Luc De Raedt, of the Katholieke Universiteit Leuven, and John Winn of Microsoft Research Cambridge. In addition to the technical talks, ICML- 2008 also included nine tutorials held before the main conference, presented by Alex Smola, Arthur Gretton, and Kenji Fukumizu; Bert Kappen and Marc Toussaint; Neil Lawrence; MartinWainwright; Ralf Herbrich and Thore Graepel; Andreas Krause and Carlos Guestrin; Shai Shalev-Shwartz and Yoram Singer; Rob Fergus; and Matthias Seeger. This year our workshops were organized jointly with COLT and UAI as part of a special "overlap day," consisting of eleven workshops selected and arranged collaboratively by the respective workshop chairs of the three conferences. This day provided a rich opportunity for interaction among the attendees of the conferences.</p> <p>This year, ICML enlarged its award offerings to match several other well-established conferences. We hope these will help build our community, celebrate our advances, and encourage applications and long-term thinking. In addition to our previously traditional "Best Paper" and "Best Student Paper" awards, we also gave awards for "Best Application Paper" and "10-year Best Paper" (for the best paper of ICML 1998, optionally given in conjunction with a co-located conference). We thank the Machine Learning Journal for sponsoring some of our paper awards.</p></div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div class="abstract">
<SPAN style="font-weight:bold">FRONT MATTER</span>
</div>
<div style="margin-left:10px; line-height:180%;">
<A NAME="FullText" HREF="https://portalparts.acm.org/1400000/1390156/fm/frontmatter.pdf?ip=173.16.22.104" title="PDF" target="_blank">
<img src="imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
&nbsp;Front matter (Title page, TOC, Preface, Organization, Invited talks, Workshop and tutorial summaries)
</div>
<div style="margin-top: 10px;" class="abstract">
<SPAN style="font-weight:bold">BACK MATTER</span>
</div>
<div style="margin-left:10px; line-height:180%;">
<A NAME="FullText" HREF="https://portalparts.acm.org/1400000/1390156/bm/backmatter.pdf?ip=173.16.22.104" title="PDF" target="_blank">
<img src="imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
&nbsp;Back matter (Author index)
</div>
<div style="margin-top: 10px; height: auto; padding: 5px; ">
<div style="margin-top:20px;" class="abstract">
<SPAN style="font-weight:bold">APPEARS IN</span>
</div>
<div>
<a href="/icps.cfm" title="ICPS"><img src="images/ACM_ICPS.jpg" alt="ICPS" style="padding-right:10px; vertical-align:middle" border="0" /></a> ICPS: <a href="/icps.cfm" title="ICPS" target="_blank">ACM International Conference Proceeding Series</a>
</div>
</div>
<br clear="all" />
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<dl title="Authors" style="margin-top:0px">
<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
<strong>
General Chairs
</strong>
</dt>
<dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of William Cohen" href="author_page.cfm?id=81100145736">William Cohen</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1987-2017</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">159</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">3,291</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">62</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">315</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">2,435</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">30,558</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">492.87</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">20.70</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of William Cohen" href="author_page.cfm?id=81100145736&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of William Cohen
</td>
</tr>
</table>
</span>
</dd>
<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
<strong>
Program Chairs
</strong>
</dt>
<dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of Andrew McCallum" href="author_page.cfm?id=81100553872">Andrew McCallum</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1990-2017</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">143</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">8,928</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">78</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">1,349</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">8,145</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">86,358</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">1,107.15</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">62.43</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of Andrew McCallum" href="author_page.cfm?id=81100553872&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of Andrew McCallum
</td>
</tr>
</table>
</span>
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of Sam Roweis" href="author_page.cfm?id=81100445880">Sam Roweis</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1997-2011</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">39</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">1,222</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">7</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">72</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">434</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">9,442</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">1,348.86</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">31.33</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of Sam Roweis" href="author_page.cfm?id=81100445880&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of Sam Roweis
</td>
</tr>
</table>
</span>
</dd>
</dl>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
References are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
Citings are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 0px;" class="flatbody">
Index Terms are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<table border="0" class="medium-text" cellpadding="5" cellspacing="5">
<tr valign="top">
<td style="padding: 10px;">Title</td>
<td style="padding: 10px;">
<a href="http://icml2008.cs.helsinki.fi/" title="Conference Website" target="_self" class="link-text">ICML '08</a> The 25th Annual International Conference on Machine Learning held in conjunction with the 2007 International Conference on Inductive Logic Programming
</td>
</tr>
<tr><td style="padding: 10px;"></td><td style="padding: 10px;"> July 05 - 09, 2008</td></tr> <tr><td style="padding: 10px;">Pages</td><td style="padding: 10px;">1310</td></tr>
<tr>
<td style="padding: 10px;">Sponsors</td>
<td style="padding: 10px;">
<a name="sponsor"> Federation of Finnish Learned Societies</a>
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> Google</a>
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> Helsinki Institute for Information Technology</a>
</td>
</tr>
<tr>

<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> IBM</a> IBM
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> Intel</a>
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> Machine Learning Journal/Springer</a>
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> Microsoft Research</a> Microsoft Research
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> NSF</a>
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> Pascal</a>
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> University of Helsinki</a>
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> Xerox</a>
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> Yahoo&#33;</a>
</td>
</tr>
<tr><td style="padding: 10px;">Publisher</td><td style="padding: 10px;"><a href="https://www.acm.org/publications">ACM</a> New York, NY, USA</td>
</tr>
<tr><td style="padding: 10px;">ISBN</td><td style="padding: 10px;"> 978-1-60558-205-4</td></tr>
<tr valign="top">
<td style="padding-left: 10px;">Conference</td>
<td valign="top" align="left" style="padding-bottom: 25px; padding-left:10px;">
<strong style="padding-right:10px">ICML</strong><a href="event.cfm?id=RE548" title="International Conference on Machine Learning">International Conference on Machine Learning</a>
</td>
</tr>
<tr><td colspan="2">Paper Acceptance Rate 158 of 583 submissions, 27%</td></tr> <tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 448 of 1,653 submissions, 27%</td></tr>
<tr valign="top">
<td colspan="2" style="padding-left:25px;">
<table>
<tr><td style="padding: 10px;">
<img border="0" class="chart" id="2566850490120838-img" src="/CFFileServlet/_cf_chart/2566850490120838.jpg" usemap="#2566850490120838-map" />
<div id="2566850490120838-tooltip" style="position:fixed;display:none;"></div>
<map id="2566850490120838-map" name="2566850490120838-map">
<area style="cursor:auto" shape="rect" id="2566850490120838-graph-id0-plotset-plot-0-node-0" coords="37,24,57,214" />
<area style="cursor:auto" shape="rect" id="2566850490120838-graph-id0-plotset-plot-0-node-1" coords="95,33,115,214" />
<area style="cursor:auto" shape="rect" id="2566850490120838-graph-id0-plotset-plot-0-node-2" coords="152,12,173,214" />
<area style="cursor:auto" shape="rect" id="2566850490120838-graph-id0-plotset-plot-1-node-0" coords="64,167,85,214" />
<area style="cursor:auto" shape="rect" id="2566850490120838-graph-id0-plotset-plot-1-node-1" coords="122,164,142,214" />
<area style="cursor:auto" shape="rect" id="2566850490120838-graph-id0-plotset-plot-1-node-2" coords="180,161,200,214" />
</map>
<script data-cfasync="false" src="/cdn-cgi/scripts/f2bf09f8/cloudflare-static/email-decode.min.js"></script><script>
if (!CFCHART) {var CFCHART={};};if (!CFCHART.nodes) {CFCHART.nodes={};}
CFCHART.nodes["2566850490120838"]={};
CFCHART.nodes["2566850490120838"]["2566850490120838-graph-id0-plotset-plot-0-node-0"]={text:"548",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["2566850490120838"]["2566850490120838-graph-id0-plotset-plot-0-node-1"]={text:"522",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["2566850490120838"]["2566850490120838-graph-id0-plotset-plot-0-node-2"]={text:"583",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["2566850490120838"]["2566850490120838-graph-id0-plotset-plot-1-node-0"]={text:"140",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["2566850490120838"]["2566850490120838-graph-id0-plotset-plot-1-node-1"]={text:"150",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["2566850490120838"]["2566850490120838-graph-id0-plotset-plot-1-node-2"]={text:"158",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
</script>
</td>
<td style="padding-left:20px;">
<table style="border-width: 1px; border-style: solid; width:100%;  border-spacing: 6px;" class="text12">
<tr bgcolor="#ffffff">
<th style="width:50%">Year</th>
<th align="right" style="width:15%">Submitted</th>
<th align="right" style="width:15%">Accepted</th>
<th align="center">Rate</th>
</tr>
<tr bgcolor="#f0f0f0">
<td style="padding: 10px;">ICML '06</td>
<td align="right">548</td>
<td align="right">140</td>
<td align="center">26%</td>
</tr>
<tr bgcolor="#ffffff">
<td style="padding: 10px;">ICML '07</td>
<td align="right">522</td>
<td align="right">150</td>
<td align="center">29%</td>
</tr>
<tr bgcolor="#f0f0f0">
<td style="padding: 10px;">ICML '08</td>
<td align="right">583</td>
<td align="right">158</td>
<td align="center">27%</td>
</tr>
<tr bgcolor="#ffffff">
<td style="padding: 10px;"><strong>Overall</strong></td>
<td align="right">1,653</td>
<td align="right">448</td>
<td align="center">27%</td>
</tr>
</table>
</td>
</tr>
</table>
</td>
</tr>
</table>
</table>
<br />
<div class="abstract" style="margin-bottom:10px;">
<SPAN><strong>APPEARS IN</strong></span>
</div>
<div>
<a href="/icps.cfm" title="ICPS"><img src="images/ACM_ICPS.jpg" alt="ICPS" style="padding-right:10px; vertical-align:middle" border="0" /></a> ICPS: <a href="/icps.cfm" title="ICPS" target="_blank">ACM International Conference Proceeding Series</a>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<br />Reviews are not available for this item
<div align="left" style="margin-top:30px">
<a title="Computing Reviews" href="ocr_review_main.cfm">
<img src="images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
<ul style="list-style:disc; display:inline-block">
<li>Access <a href="ocr_review_main.cfm" target="_blank">critical reviews</a> of computing literature.</li>
<li><a href="http://www.computingreviews.com/Reviewer/" target="_blank">Become a reviewer</a> for Computing Reviews</li>
</ul>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div>
<div>
<p style="margin-left:5px;">
<strong>Be the first to comment</strong>
To Post a comment please <a href="signin.cfm">sign in or create</a> a free Web account</a>
</p>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;">
<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the 25th international conference on Machine learning</h5>
<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>
<div style="clear:both">
<div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1273496&picked=prox" title="previous: ICML '07"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><a href="citation.cfm?id=1553374&picked=prox" title="Next: ICML '09">next proceeding <img align="absmiddle" hspace="5" border="0" src="img/next.gif" width="19" height="11" alt="next" /></a></div>
</div>
<table class="text12" border="0">
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390157">Gaussian process product models for nonparametric nonstationarity</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435594431">Ryan Prescott Adams</a>,
<a href="author_page.cfm?id=81421593340">Oliver Stegle</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1-8</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390157" title="DOI">10.1145/1390156.1390157</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390157&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow1" style="display:inline;"><br /><div style="display:inline">Stationarity is often an unrealistic prior assumption for Gaussian process regression. One solution is to predefine an explicit nonstationary covariance function, but such covariance functions can be difficult to specify and require detailed prior knowledge ...</div></span>
<span id="toHide1" style="display:none;"><br /><div style="display:inline"><p>Stationarity is often an unrealistic prior assumption for Gaussian process regression. One solution is to predefine an explicit nonstationary covariance function, but such covariance functions can be difficult to specify and require detailed prior knowledge of the nonstationarity. We propose the Gaussian process product model (GPPM) which models data as the pointwise product of two latent Gaussian processes to nonparametrically infer nonstationary variations of amplitude. This approach differs from other nonparametric approaches to covariance function inference in that it operates on the outputs rather than the inputs, resulting in a significant reduction in computational cost and required data for inference. We present an approximate inference scheme using Expectation Propagation. This variational approximation yields convenient GP hyperparameter selection and compact approximate predictive distributions.</p></div></span> <a id="expcoll1" href="JavaScript: expandcollapse('expcoll1',1)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390158">Sequence kernels for predicting protein essentiality</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100355470">Cyril Allauzen</a>,
<a href="author_page.cfm?id=81100197439">Mehryar Mohri</a>,
<a href="author_page.cfm?id=81421595327">Ameet Talwalkar</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 9-16</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390158" title="DOI">10.1145/1390156.1390158</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390158&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow2" style="display:inline;"><br /><div style="display:inline">The problem of identifying the minimal gene set required to sustain life is of crucial importance in understanding cellular mechanisms and designing therapeutic drugs. This work describes several kernel-based solutions for predicting essential genes ...</div></span>
<span id="toHide2" style="display:none;"><br /><div style="display:inline"><p>The problem of identifying the minimal gene set required to sustain life is of crucial importance in understanding cellular mechanisms and designing therapeutic drugs. This work describes several kernel-based solutions for predicting essential genes that outperform existing models while using less training data. Our first solution is based on a semi-manually designed kernel derived from the Pfam database, which includes several Pfam domains. We then present novel and general <i>domain-based</i> sequence kernels that capture sequence similarity with respect to several domains made of large sets of protein sequences. We show how to deal with the large size of the problem -- several thousands of domains with individual domains sometimes containing thousands of sequences -- by representing and efficiently computing these kernels using automata. We report results of extensive experiments demonstrating that they compare favorably with the Pfam kernel in predicting protein essentiality, while requiring no manual tuning.</p></div></span> <a id="expcoll2" href="JavaScript: expandcollapse('expcoll2',2)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390159">Hierarchical kernel stick-breaking process for multi-task image analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421601443">Qi An</a>,
<a href="author_page.cfm?id=81421597564">Chunping Wang</a>,
<a href="author_page.cfm?id=81421601753">Ivo Shterev</a>,
<a href="author_page.cfm?id=81545308756">Eric Wang</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>,
<a href="author_page.cfm?id=81333488093">David B. Dunson</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 17-24</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390159" title="DOI">10.1145/1390156.1390159</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390159&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow3" style="display:inline;"><br /><div style="display:inline">The kernel stick-breaking process (KSBP) is employed to segment general imagery, imposing the condition that patches (small blocks of pixels) that are spatially proximate are more likely to be associated with the same cluster (segment). The number of ...</div></span>
<span id="toHide3" style="display:none;"><br /><div style="display:inline"><p>The kernel stick-breaking process (KSBP) is employed to segment general imagery, imposing the condition that patches (small blocks of pixels) that are spatially proximate are more likely to be associated with the same cluster (segment). The number of clusters is not set <i>a priori</i> and is inferred from the hierarchical Bayesian model. Further, KSBP is integrated with a shared Dirichlet process prior to simultaneously model multiple images, inferring their inter-relationships. This latter application may be useful for sorting and learning relationships between multiple images. The Bayesian inference algorithm is based on a hybrid of variational Bayesian analysis and local sampling. In addition to providing details on the model and associated inference framework, example results are presented for several image-analysis problems.</p></div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390160">Graph kernels between point clouds</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100328355">Francis R. Bach</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 25-32</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390160" title="DOI">10.1145/1390156.1390160</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390160&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow4" style="display:inline;"><br /><div style="display:inline">Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and ...</div></span>
<span id="toHide4" style="display:none;"><br /><div style="display:inline"><p>Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow one to use kernel methods for such objects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on probabilistic graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.</p></div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390161">Bolasso: model consistent Lasso estimation through the bootstrap</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100328355">Francis R. Bach</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 33-40</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390161" title="DOI">10.1145/1390156.1390161</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390161&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow5" style="display:inline;"><br /><div style="display:inline">We consider the least-square linear regression problem with regularization by the l1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various ...</div></span>
<span id="toHide5" style="display:none;"><br /><div style="display:inline"><p>We consider the least-square linear regression problem with regularization by the <i>l</i><sub>1</sub>-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository.</p></div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390162">Learning all optimal policies with multiple criteria</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309511366">Leon Barrett</a>,
<a href="author_page.cfm?id=81421598150">Srini Narayanan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 41-47</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390162" title="DOI">10.1145/1390156.1390162</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390162&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow6" style="display:inline;"><br /><div style="display:inline">We describe an algorithm for learning in the presence of multiple criteria. Our technique generalizes previous approaches in that it can learn optimal policies for all linear preference assignments over the multiple reward criteria at once. The algorithm ...</div></span>
<span id="toHide6" style="display:none;"><br /><div style="display:inline"><p>We describe an algorithm for learning in the presence of multiple criteria. Our technique generalizes previous approaches in that it can learn optimal policies for all linear preference assignments over the multiple reward criteria at once. The algorithm can be viewed as an extension to standard reinforcement learning for MDPs where instead of repeatedly backing up maximal expected rewards, we back up the set of expected rewards that are maximal for some set of linear preferences (given by a weight vector, <i>w</i>). We present the algorithm along with a proof of correctness showing that our solution gives the optimal policy for any linear preference function. The solution reduces to the standard value iteration algorithm for a specific weight vector, <i>w</i>.</p></div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390163">Multiple instance ranking</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597982">Charles Bergeron</a>,
<a href="author_page.cfm?id=81421595556">Jed Zaretzki</a>,
<a href="author_page.cfm?id=81100172625">Curt Breneman</a>,
<a href="author_page.cfm?id=81100291848">Kristin P. Bennett</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 48-55</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390163" title="DOI">10.1145/1390156.1390163</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390163&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow7" style="display:inline;"><br /><div style="display:inline">This paper introduces a novel machine learning model called multiple instance ranking (MIRank) that enables ranking to be performed in a multiple instance learning setting. The motivation for MIRank stems from the hydrogen abstraction problem in computational ...</div></span>
<span id="toHide7" style="display:none;"><br /><div style="display:inline"><p>This paper introduces a novel machine learning model called multiple instance ranking (MIRank) that enables ranking to be performed in a multiple instance learning setting. The motivation for MIRank stems from the hydrogen abstraction problem in computational chemistry, that of predicting the group of hydrogen atoms from which a hydrogen is abstracted (removed) during metabolism. The model predicts the preferred hydrogen group within a molecule by ranking the groups, with the ambiguity of not knowing which hydrogen atom within the preferred group is actually abstracted. This paper formulates MIRank in its general context and proposes an algorithm for solving MIRank problems using successive linear programming. The method outperforms multiple instance classification models on several real and synthetic datasets.</p></div></span> <a id="expcoll7" href="JavaScript: expandcollapse('expcoll7',7)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390164">Multi-task learning for HIV therapy screening</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333487432">Steffen Bickel</a>,
<a href="author_page.cfm?id=81421598375">Jasmina Bogojeska</a>,
 <a href="author_page.cfm?id=81100084952">Thomas Lengauer</a>,
<a href="author_page.cfm?id=81100180901">Tobias Scheffer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 56-63</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390164" title="DOI">10.1145/1390156.1390164</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390164&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow8" style="display:inline;"><br /><div style="display:inline">We address the problem of learning classifiers for a large number of tasks. We derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task. Our work is motivated by the problem ...</div></span>
<span id="toHide8" style="display:none;"><br /><div style="display:inline"><p>We address the problem of learning classifiers for a large number of tasks. We derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task. Our work is motivated by the problem of predicting the outcome of a therapy attempt for a patient who carries an HIV virus with a set of observed genetic properties. Such predictions need to be made for hundreds of possible combinations of drugs, some of which use similar biochemical mechanisms. Multi-task learning enables us to make predictions even for drug combinations with few or no training examples and substantially improves the overall prediction accuracy.</p></div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390165">Nonnegative matrix factorization via rank-one downdate</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421601765">Michael Biggs</a>,
<a href="author_page.cfm?id=81545241356">Ali Ghodsi</a>,
<a href="author_page.cfm?id=81100638928">Stephen Vavasis</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 64-71</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390165" title="DOI">10.1145/1390156.1390165</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390165&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow9" style="display:inline;"><br /><div style="display:inline">Nonnegative matrix factorization (NMF) was popularized as a tool for data mining by Lee and Seung in 1999. NMF attempts to approximate a matrix with nonnegative entries by a product of two low-rank matrices, also with nonnegative entries. We propose ...</div></span>
<span id="toHide9" style="display:none;"><br /><div style="display:inline"><p>Nonnegative matrix factorization (NMF) was popularized as a tool for data mining by Lee and Seung in 1999. NMF attempts to approximate a matrix with nonnegative entries by a product of two low-rank matrices, also with nonnegative entries. We propose an algorithm called rank-one downdate (R1D) for computing an NMF that is partly motivated by the singular value decomposition. This algorithm computes the dominant singular values and vectors of adaptively determined sub-matrices of a matrix. On each iteration, R1D extracts a rank-one submatrix from the original matrix according to an objective function. We establish a theoretical result that maximizing this objective function corresponds to correctly classifying articles in a nearly separable corpus. We also provide computational experiments showing the success of this method in identifying features in realistic datasets. The method is also much faster than other NMF routines.</p></div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390166">Strategy evaluation in extensive games with importance sampling</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100395285">Michael Bowling</a>,
<a href="author_page.cfm?id=81421595600">Michael Johanson</a>,
<a href="author_page.cfm?id=81421598079">Neil Burch</a>,
<a href="author_page.cfm?id=81100405412">Duane Szafron</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 72-79</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390166" title="DOI">10.1145/1390156.1390166</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390166&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow10" style="display:inline;"><br /><div style="display:inline">Typically agent evaluation is done through Monte Carlo estimation. However, stochastic agent decisions and stochastic outcomes can make this approach inefficient, requiring many samples for an accurate estimate. We present a new technique that can be ...</div></span>
<span id="toHide10" style="display:none;"><br /><div style="display:inline"><p>Typically agent evaluation is done through Monte Carlo estimation. However, stochastic agent decisions and stochastic outcomes can make this approach inefficient, requiring many samples for an accurate estimate. We present a new technique that can be used to simultaneously evaluate many strategies while playing a single strategy in the context of an extensive game. This technique is based on importance sampling, but utilizes two new mechanisms for significantly reducing variance in the estimates. We demonstrate its effectiveness in the domain of poker, where stochasticity makes traditional evaluation problematic.</p></div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390167">Actively learning level-sets of composite functions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365592321">Brent Bryan</a>,
<a href="author_page.cfm?id=81100155456">Jeff Schneider</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 80-87</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390167" title="DOI">10.1145/1390156.1390167</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390167&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow11" style="display:inline;"><br /><div style="display:inline">Scientists frequently have multiple types of experiments and data sets on which they can test the validity of their parameterized models and locate plausible regions for the model parameters. By examining multiple data sets, scientists can obtain inferences ...</div></span>
<span id="toHide11" style="display:none;"><br /><div style="display:inline"><p>Scientists frequently have multiple types of experiments and data sets on which they can test the validity of their parameterized models and locate plausible regions for the model parameters. By examining multiple data sets, scientists can obtain inferences which typically are much more informative than the deductions derived from each of the data sources independently. Several standard data combination techniques result in target functions which are a weighted sum of the observed data sources. Thus, computing constraints on the plausible regions of the model parameter space can be formulated as finding a level set of a target function which is the sum of observable functions. We propose an active learning algorithm for this problem which selects both a sample (from the parameter space) and an observable function upon which to compute the next sample. Empirical tests on synthetic functions and on real data for an eight parameter cosmological model show that our algorithm significantly reduces the number of samples required to identify the desired level-set.</p></div></span> <a id="expcoll11" href="JavaScript: expandcollapse('expcoll11',11)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390168">Sparse Bayesian nonparametric regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81325487472">Fran&#231;ois Caron</a>,
<a href="author_page.cfm?id=81100562973">Arnaud Doucet</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 88-95</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390168" title="DOI">10.1145/1390156.1390168</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390168&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow12" style="display:inline;"><br /><div style="display:inline">One of the most common problems in machine learning and statistics consists of estimating the mean response X&beta; from a vector of observations y assuming y = X&beta; + &epsilon; where X is known, &beta; is ...</div></span>
<span id="toHide12" style="display:none;"><br /><div style="display:inline"><p>One of the most common problems in machine learning and statistics consists of estimating the mean response <i>X&beta;</i> from a vector of observations <i>y</i> assuming <i>y</i> = <i>X&beta;</i> + <i>&epsilon;</i> where <i>X</i> is known, &beta; is a vector of parameters of interest and <i>&epsilon;</i> a vector of stochastic errors. We are particularly interested here in the case where the dimension <i>K</i> of &beta; is much higher than the dimension of <i>y</i>. We propose some flexible Bayesian models which can yield sparse estimates of &beta;. We show that as <i>K</i> &rarr; &infin; these models are closely related to a class of L&eacute;vy processes. Simulations demonstrate that our models outperform significantly a range of popular alternatives.</p></div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390169">An empirical evaluation of supervised learning in high dimensions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100100877">Rich Caruana</a>,
<a href="author_page.cfm?id=81421600112">Nikos Karampatziakis</a>,
<a href="author_page.cfm?id=81421593811">Ainur Yessenalina</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 96-103</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390169" title="DOI">10.1145/1390156.1390169</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390169&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow13" style="display:inline;"><br /><div style="display:inline">In this paper we perform an empirical evaluation of supervised learning on high-dimensional data. We evaluate performance on three metrics: accuracy, AUC, and squared loss and study the effect of increasing dimensionality on the performance of the learning ...</div></span>
<span id="toHide13" style="display:none;"><br /><div style="display:inline"><p>In this paper we perform an empirical evaluation of supervised learning on high-dimensional data. We evaluate performance on three metrics: accuracy, AUC, and squared loss and study the effect of increasing dimensionality on the performance of the learning algorithms. Our findings are consistent with previous studies for problems of relatively low dimension, but suggest that as dimensionality increases the relative performance of the learning algorithms changes. To our surprise, the method that performs consistently well across all dimensions is random forests, followed by neural nets, boosted trees, and SVMs.</p></div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390170">Fast support vector machine training and classification on graphics processors</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100178232">Bryan Catanzaro</a>,
<a href="author_page.cfm?id=81421594130">Narayanan Sundaram</a>,
<a href="author_page.cfm?id=81100164705">Kurt Keutzer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 104-111</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390170" title="DOI">10.1145/1390156.1390170</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390170&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow14" style="display:inline;"><br /><div style="display:inline">Recent developments in programmable, highly parallel Graphics Processing Units (GPUs) have enabled high performance implementations of machine learning algorithms. We describe a solver for Support Vector Machine training running on a GPU, using the Sequential ...</div></span>
<span id="toHide14" style="display:none;"><br /><div style="display:inline"><p>Recent developments in programmable, highly parallel Graphics Processing Units (GPUs) have enabled high performance implementations of machine learning algorithms. We describe a solver for Support Vector Machine training running on a GPU, using the Sequential Minimal Optimization algorithm and an adaptive first and second order working set selection heuristic, which achieves speedups of 9-35x over LIBSVM running on a traditional processor. We also present a GPU-based system for SVM classification which achieves speedups of 81-138x over LIBSVM (5-24x over our own CPU based SVM classifier).</p></div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390171">Fast nearest neighbor retrieval for bregman divergences</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
 <a href="author_page.cfm?id=81421595307">Lawrence Cayton</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 112-119</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390171" title="DOI">10.1145/1390156.1390171</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390171&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow15" style="display:inline;"><br /><div style="display:inline">We present a data structure enabling efficient nearest neighbor (NN) retrieval for bregman divergences. The family of bregman divergences includes many popular dissimilarity measures including KL-divergence (relative entropy), Mahalanobis distance, and ...</div></span>
<span id="toHide15" style="display:none;"><br /><div style="display:inline"><p>We present a data structure enabling efficient nearest neighbor (NN) retrieval for bregman divergences. The family of bregman divergences includes many popular dissimilarity measures including KL-divergence (relative entropy), Mahalanobis distance, and Itakura-Saito divergence. These divergences present a challenge for efficient NN retrieval because they are not, in general, metrics, for which most NN data structures are designed. The data structure introduced in this work shares the same basic structure as the popular metric ball tree, but employs convexity properties of bregman divergences in place of the triangle inequality. Experiments demonstrate speedups over brute-force search of up to several orders of magnitude.</p></div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390172">Nearest hyperdisk methods for high-dimensional classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100057031">Hakan Cevikalp</a>,
<a href="author_page.cfm?id=81100103974">Bill Triggs</a>,
<a href="author_page.cfm?id=81339522307">Robi Polikar</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 120-127</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390172" title="DOI">10.1145/1390156.1390172</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390172&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow16" style="display:inline;"><br /><div style="display:inline">In high-dimensional classification problems it is infeasible to include enough training samples to cover the class regions densely. Irregularities in the resulting sparse sample distributions cause local classifiers such as Nearest Neighbors (NN) and ...</div></span>
<span id="toHide16" style="display:none;"><br /><div style="display:inline"><p>In high-dimensional classification problems it is infeasible to include enough training samples to cover the class regions densely. Irregularities in the resulting sparse sample distributions cause local classifiers such as Nearest Neighbors (NN) and kernel methods to have irregular decision boundaries. One solution is to "fill in the holes" by building a convex model of the region spanned by the training samples of each class and classifying examples based on their distances to these approximate models. Methods of this kind based on affine and convex hulls and bounding hyperspheres have already been studied. Here we propose a method based on the <i>bounding hyperdisk</i> of each class - the intersection of the affine hull and the smallest bounding hypersphere of its training samples. We argue that in many cases hyperdisks are preferable to affine and convex hulls and hyperspheres: they bound the classes more tightly than affine hulls or hyperspheres while avoiding much of the sample overfitting and computational complexity that is inherent in high-dimensional convex hulls. We show that the hyperdisk method can be kernelized to provide nonlinear classifiers based on non-Euclidean distance metrics. Experiments on several classification problems show promising results.</p></div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390173">Learning to sportscast: a test of grounded language acquisition</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365590952">David L. Chen</a>,
<a href="author_page.cfm?id=81100539345">Raymond J. Mooney</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 128-135</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390173" title="DOI">10.1145/1390156.1390173</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390173&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow17" style="display:inline;"><br /><div style="display:inline">We present a novel commentator system that learns language from sportscasts of simulated soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous ...</div></span>
<span id="toHide17" style="display:none;"><br /><div style="display:inline"><p>We present a novel commentator system that learns language from sportscasts of simulated soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. We also present a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on. Human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries.</p></div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390174">Training SVM with indefinite kernels</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81343489892">Jianhui Chen</a>,
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 136-143</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390174" title="DOI">10.1145/1390156.1390174</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390174&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow18" style="display:inline;"><br /><div style="display:inline">Similarity matrices generated from many applications may not be positive semidefinite, and hence can't fit into the kernel machine framework. In this paper, we study the problem of training support vector machines with an indefinite kernel. We consider ...</div></span>
<span id="toHide18" style="display:none;"><br /><div style="display:inline"><p>Similarity matrices generated from many applications may not be positive semidefinite, and hence can't fit into the kernel machine framework. In this paper, we study the problem of training support vector machines with an indefinite kernel. We consider a regularized SVM formulation, in which the indefinite kernel matrix is treated as a noisy observation of some unknown positive semidefinite one (proxy kernel) and the support vectors and the proxy kernel can be computed simultaneously. We propose a semi-infinite quadratically constrained linear program formulation for the optimization, which can be solved iteratively to find a global optimum solution. We further propose to employ an additional pruning strategy, which significantly improves the efficiency of the algorithm, while retaining the convergence property of the algorithm. In addition, we show the close relationship between the proposed formulation and multiple kernel learning. Experiments on a collection of benchmark data sets demonstrate the efficiency and effectiveness of the proposed algorithm.</p></div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390175">Learning for control from multiple demonstrations</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
 <a href="author_page.cfm?id=81421601840">Adam Coates</a>,
<a href="author_page.cfm?id=81309498920">Pieter Abbeel</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 144-151</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390175" title="DOI">10.1145/1390156.1390175</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390175&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow19" style="display:inline;"><br /><div style="display:inline">We consider the problem of learning to follow a desired trajectory when given a small number of demonstrations from a sub-optimal expert. We present an algorithm that (i) extracts the---initially unknown---desired trajectory from the sub-optimal expert's ...</div></span>
<span id="toHide19" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of learning to follow a desired trajectory when given a small number of demonstrations from a sub-optimal expert. We present an algorithm that (i) extracts the---initially unknown---desired trajectory from the sub-optimal expert's demonstrations and (ii) learns a local model suitable for control along the learned trajectory. We apply our algorithm to the problem of autonomous helicopter flight. In all cases, the autonomous helicopter's performance exceeds that of our expert helicopter pilot's demonstrations. Even stronger, our results significantly extend the state-of-the-art in autonomous helicopter aerobatics. In particular, our results include the first autonomous tic-tocs, loops and hurricane, vastly superior performance on previously performed aerobatic maneuvers (such as in-place flips and rolls), and a complete airshow, which requires autonomous transitions between these and various other maneuvers.</p></div></span> <a id="expcoll19" href="JavaScript: expandcollapse('expcoll19',19)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390176">Spectral clustering with inconsistent advice</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384613543">Tom Coleman</a>,
<a href="author_page.cfm?id=81384619573">James Saunderson</a>,
<a href="author_page.cfm?id=81100261305">Anthony Wirth</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 152-159</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390176" title="DOI">10.1145/1390156.1390176</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390176&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow20" style="display:inline;"><br /><div style="display:inline">Clustering with advice (often known as constrained clustering) has been a recent focus of the data mining community. Success has been achieved incorporating advice into the k-means and spectral clustering frameworks. Although the theory community ...</div></span>
<span id="toHide20" style="display:none;"><br /><div style="display:inline"><p>Clustering with advice (often known as constrained clustering) has been a recent focus of the data mining community. Success has been achieved incorporating advice into the <i>k</i>-means and spectral clustering frameworks. Although the theory community has explored inconsistent advice, it has not yet been incorporated into spectral clustering. Extending work of De Bie and Cristianini, we set out a framework for finding minimum normalised cuts, subject to inconsistent advice.</p></div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390177">A unified architecture for natural language processing: deep neural networks with multitask learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100001072">Ronan Collobert</a>,
<a href="author_page.cfm?id=81100015405">Jason Weston</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 160-167</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390177" title="DOI">10.1145/1390156.1390177</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390177&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow21" style="display:inline;"><br /><div style="display:inline">We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that ...</div></span>
<span id="toHide21" style="display:none;"><br /><div style="display:inline"><p>We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained <i>jointly</i> on all these tasks using weight-sharing, an instance of <i>multitask learning</i>. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of <i>semi-supervised learning</i> for the shared tasks. We show how both <i>multitask learning</i> and <i>semi-supervised learning</i> improve the generalization of the shared tasks, resulting in state-of-the-art-performance.</p></div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390178">Autonomous geometric precision error estimation in low-level computer vision tasks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365596170">Andr&#233;s Corrada-Emmanuel</a>,
<a href="author_page.cfm?id=81100622836">Howard Schultz</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 168-175</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390178" title="DOI">10.1145/1390156.1390178</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390178&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow22" style="display:inline;"><br /><div style="display:inline">Errors in map-making tasks using computer vision are sparse. We demonstrate this by considering the construction of digital elevation models that employ stereo matching algorithms to triangulate real-world points. This sparsity, coupled with a geometric ...</div></span>
<span id="toHide22" style="display:none;"><br /><div style="display:inline"><p>Errors in map-making tasks using computer vision are sparse. We demonstrate this by considering the construction of digital elevation models that employ stereo matching algorithms to triangulate real-world points. This sparsity, coupled with a geometric theory of errors recently developed by the authors, allows for autonomous agents to calculate their own precision independently of ground truth. We connect these developments with recent advances in the mathematics of sparse signal reconstruction or compressed sensing. The theory presented here extends the autonomy of 3-D model reconstructions discovered in the 1990s to their errors.</p></div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390179">Stability of transductive regression algorithms</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81332494270">Corinna Cortes</a>,
<a href="author_page.cfm?id=81100197439">Mehryar Mohri</a>,
<a href="author_page.cfm?id=81351608270">Dmitry Pechyony</a>,
<a href="author_page.cfm?id=81333490588">Ashish Rastogi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 176-183</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390179" title="DOI">10.1145/1390156.1390179</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390179&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow23" style="display:inline;"><br /><div style="display:inline">This paper uses the notion of algorithmic stability to derive novel generalization bounds for several families of transductive regression algorithms, both by using convexity and closed-form solutions. Our analysis helps compare the stability of these ...</div></span>
<span id="toHide23" style="display:none;"><br /><div style="display:inline"><p>This paper uses the notion of algorithmic stability to derive novel generalization bounds for several families of transductive regression algorithms, both by using convexity and closed-form solutions. Our analysis helps compare the stability of these algorithms. It suggests that several existing algorithms might not be stable but prescribes a technique to make them stable. It also reports the results of experiments with local transductive regression demonstrating the benefit of our stability bounds for model selection, in particular for determining the radius of the local neighborhood used by the algorithm.</p></div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390180">A rate-distortion one-class model and its applications to clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100138543">Koby Crammer</a>,
<a href="author_page.cfm?id=81381599699">Partha Pratim Talukdar</a>,
<a href="author_page.cfm?id=81100147733">Fernando Pereira</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 184-191</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390180" title="DOI">10.1145/1390156.1390180</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390180&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow24" style="display:inline;"><br /><div style="display:inline">In one-class classification we seek a rule to find a coherent subset of instances similar to a few positive examples in a large pool of instances. The problem can be formulated and analyzed naturally in a rate-distortion framework, leading to an efficient ...</div></span>
<span id="toHide24" style="display:none;"><br /><div style="display:inline"><p>In one-class classification we seek a rule to find a coherent subset of instances similar to a few positive examples in a large pool of instances. The problem can be formulated and analyzed naturally in a rate-distortion framework, leading to an efficient algorithm that compares well with two previous one-class methods. The model can be also be extended to remove background clutter in clustering to improve cluster purity.</p></div></span> <a id="expcoll24" href="JavaScript: expandcollapse('expcoll24',24)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390181">Fast Gaussian process methods for point process intensity estimation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421598250">John P. Cunningham</a>,
<a href="author_page.cfm?id=81421599768">Krishna V. Shenoy</a>,
<a href="author_page.cfm?id=81100574937">Maneesh Sahani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 192-199</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390181" title="DOI">10.1145/1390156.1390181</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390181&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow25" style="display:inline;"><br /><div style="display:inline">Point processes are difficult to analyze because they provide only a sparse and noisy observation of the intensity function driving the process. Gaussian Processes offer an attractive framework within which to infer underlying intensity functions. The ...</div></span>
<span id="toHide25" style="display:none;"><br /><div style="display:inline"><p>Point processes are difficult to analyze because they provide only a sparse and noisy observation of the intensity function driving the process. Gaussian Processes offer an attractive framework within which to infer underlying intensity functions. The result of this inference is a continuous function defined across time that is typically more amenable to analytical efforts. However, a naive implementation will become computationally infeasible in any problem of reasonable size, both in memory and run time requirements. We demonstrate problem specific methods for a class of renewal processes that eliminate the memory burden and reduce the solve time by orders of magnitude.</p></div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390182">Self-taught clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333488189">Wenyuan Dai</a>,
<a href="author_page.cfm?id=81372591186">Qiang Yang</a>,
<a href="author_page.cfm?id=81100142932">Gui-Rong Xue</a>,
<a href="author_page.cfm?id=81363594294">Yong Yu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 200-207</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390182" title="DOI">10.1145/1390156.1390182</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390182&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow26" style="display:inline;"><br /><div style="display:inline">This paper focuses on a new clustering task, called self-taught clustering. Self-taught clustering is an instance of unsupervised transfer learning, which aims at clustering a small collection of target unlabeled data with the help of a ...</div></span>
<span id="toHide26" style="display:none;"><br /><div style="display:inline"><p>This paper focuses on a new clustering task, called <i>self-taught clustering</i>. Self-taught clustering is an instance of <i>unsupervised transfer learning</i>, which aims at clustering a small collection of target unlabeled data with the help of a large amount of <i>auxiliary</i> unlabeled data. The target and auxiliary data can be different in topic distribution. We show that even when the target data are not sufficient to allow effective learning of a high quality feature representation, it is possible to learn the useful features with the help of the auxiliary data on which the target data can be clustered effectively. We propose a co-clustering based self-taught clustering algorithm to tackle this problem, by clustering the target and auxiliary data simultaneously to allow the feature representation from the auxiliary data to influence the target data through a common set of features. Under the new data representation, clustering on the target data can be improved. Our experiments on image clustering show that our algorithm can greatly outperform several state-of-the-art clustering methods when utilizing irrelevant unlabeled auxiliary data.</p></div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390183">Hierarchical sampling for active learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100357774">Sanjoy Dasgupta</a>,
<a href="author_page.cfm?id=81542413256">Daniel Hsu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 208-215</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390183" title="DOI">10.1145/1390156.1390183</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390183&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow27" style="display:inline;"><br /><div style="display:inline">We present an active learning scheme that exploits cluster structure in data.</div></span>
<span id="toHide27" style="display:none;"><br /><div style="display:inline"><p>We present an active learning scheme that exploits cluster structure in data.</p></div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390184">Learning to classify with missing and corrupted features</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100230258">Ofer Dekel</a>,
<a href="author_page.cfm?id=81365597666">Ohad Shamir</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 216-223</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390184" title="DOI">10.1145/1390156.1390184</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390184&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow28" style="display:inline;"><br /><div style="display:inline">After a classifier is trained using a machine learning algorithm and put to use in a real world system, it often faces noise which did not appear in the training data. Particularly, some subset of features may be missing or may become corrupted. We present ...</div></span>
<span id="toHide28" style="display:none;"><br /><div style="display:inline"><p>After a classifier is trained using a machine learning algorithm and put to use in a real world system, it often faces noise which did not appear in the training data. Particularly, some subset of features may be missing or may become corrupted. We present two novel machine learning techniques that are robust to this type of classification-time noise. First, we solve an approximation to the learning problem using linear programming. We analyze the tightness of our approximation and prove statistical risk bounds for this approach. Second, we define the online-learning variant of our problem, address this variant using a modified Perceptron, and obtain a statistical learning algorithm using an online-to-batch technique. We conclude with a set of experiments that demonstrate the effectiveness of our algorithms.</p></div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390185">Maximum likelihood rule ensembles</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81371594005">Krzysztof Dembczy&#324;ski</a>,
<a href="author_page.cfm?id=81365594567">Wojciech Kot&#322;owski</a>,
<a href="author_page.cfm?id=81100048648">Roman S&#322;owi&#324;ski</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 224-231</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390185" title="DOI">10.1145/1390156.1390185</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390185&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow29" style="display:inline;"><br /><div style="display:inline">We propose a new rule induction algorithm for solving classification problems via probability estimation. The main advantage of decision rules is their simplicity and good interpretability. While the early approaches to rule induction were based on sequential ...</div></span>
<span id="toHide29" style="display:none;"><br /><div style="display:inline"><p>We propose a new rule induction algorithm for solving classification problems via probability estimation. The main advantage of decision rules is their simplicity and good interpretability. While the early approaches to rule induction were based on sequential covering, we follow an approach in which a single decision rule is treated as a base classifier in an ensemble. The ensemble is built by greedily minimizing the negative loglikelihood which results in estimating the class conditional probability distribution. The introduced approach is compared with other decision rule induction algorithms such as SLIPPER, LRI and RuleFit.</p></div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390186">Learning from incomplete data with infinite imputations</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421600894">Uwe Dick</a>,
<a href="author_page.cfm?id=81310486073">Peter Haider</a>,
<a href="author_page.cfm?id=81100180901">Tobias Scheffer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 232-239</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390186" title="DOI">10.1145/1390156.1390186</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390186&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow30" style="display:inline;"><br /><div style="display:inline">We address the problem of learning decision functions from training data in which some attribute values are unobserved. This problem can arise, for instance, when training data is aggregated from multiple sources, and some sources record only a subset ...</div></span>
<span id="toHide30" style="display:none;"><br /><div style="display:inline"><p>We address the problem of learning decision functions from training data in which some attribute values are unobserved. This problem can arise, for instance, when training data is aggregated from multiple sources, and some sources record only a subset of attributes. We derive a generic joint optimization problem in which the distribution governing the missing values is a free parameter. We show that the optimal solution concentrates the density mass on finitely many imputations, and provide a corresponding algorithm for learning from incomplete data. We report on empirical results on benchmark data, and on the email spam application that motivates our work.</p></div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390187">An object-oriented representation for efficient reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81318488284">Carlos Diuk</a>,
<a href="author_page.cfm?id=81421598099">Andre Cohen</a>,
<a href="author_page.cfm?id=81406601119">Michael L. Littman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 240-247</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390187" title="DOI">10.1145/1390156.1390187</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390187&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow31" style="display:inline;"><br /><div style="display:inline">Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, ...</div></span>
<span id="toHide31" style="display:none;"><br /><div style="display:inline"><p>Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the well-known Taxi domain, plus a real-life videogame.</p></div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390188">Optimizing estimated loss reduction for active sampling in rank learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365597045">Pinar Donmez</a>,
<a href="author_page.cfm?id=81452611253">Jaime G. Carbonell</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 248-255</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390188" title="DOI">10.1145/1390156.1390188</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390188&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow32" style="display:inline;"><br /><div style="display:inline">Learning to rank is becoming an increasingly popular research area in machine learning. The ranking problem aims to induce an ordering or preference relations among a set of instances in the input space. However, collecting labeled data is growing into ...</div></span>
<span id="toHide32" style="display:none;"><br /><div style="display:inline"><p>Learning to rank is becoming an increasingly popular research area in machine learning. The ranking problem aims to induce an ordering or preference relations among a set of instances in the input space. However, collecting labeled data is growing into a burden in many rank applications since labeling requires eliciting the relative ordering over the set of alternatives. In this paper, we propose a novel active learning framework for SVM-based and boosting-based rank learning. Our approach suggests sampling based on maximizing the estimated loss differential over unlabeled data. Experimental results on two benchmark corpora show that the proposed model substantially reduces the labeling effort, and achieves superior performance rapidly with as much as 30% relative improvement over the margin-based sampling baseline.</p></div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390189">Reinforcement learning with limited reinforcement: using Bayes risk for active learning in POMDPs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81325487917">Finale Doshi</a>,
<a href="author_page.cfm?id=81100040835">Joelle Pineau</a>,
<a href="author_page.cfm?id=81339525411">Nicholas Roy</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 256-263</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390189" title="DOI">10.1145/1390156.1390189</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390189&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow33" style="display:inline;"><br /><div style="display:inline">Partially Observable Markov Decision Processes (POMDPs) have succeeded in planning domains that require balancing actions that increase an agent's knowledge and actions that increase an agent's reward. Unfortunately, most POMDPs are defined with a large ...</div></span>
<span id="toHide33" style="display:none;"><br /><div style="display:inline"><p>Partially Observable Markov Decision Processes (POMDPs) have succeeded in planning domains that require balancing actions that increase an agent's knowledge and actions that increase an agent's reward. Unfortunately, most POMDPs are defined with a large number of parameters which are difficult to specify only from domain knowledge. In this paper, we present an approximation approach that allows us to treat the POMDP model parameters as additional hidden state in a "model-uncertainty" POMDP. Coupled with model-directed queries, our planner actively learns good policies. We demonstrate our approach on several POMDP problems.</p></div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390190">Confidence-weighted linear classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100534399">Mark Dredze</a>,
<a href="author_page.cfm?id=81100138543">Koby Crammer</a>,
<a href="author_page.cfm?id=81100147733">Fernando Pereira</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 264-271</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390190" title="DOI">10.1145/1390156.1390190</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390190&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow34" style="display:inline;"><br /><div style="display:inline">We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms ...</div></span>
<span id="toHide34" style="display:none;"><br /><div style="display:inline"><p>We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and covariance of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training.</p></div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390191">Efficient projections onto the <i>l</i><sub>1</sub>-ball for learning in high dimensions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597874">John Duchi</a>,
<a href="author_page.cfm?id=81100489319">Shai Shalev-Shwartz</a>,
<a href="author_page.cfm?id=81100308085">Yoram Singer</a>,
<a href="author_page.cfm?id=81541826456">Tushar Chandra</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 272-279</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390191" title="DOI">10.1145/1390156.1390191</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390191&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow35" style="display:inline;"><br /><div style="display:inline">We describe efficient algorithms for projecting a vector onto the l1-ball. We present two methods for projection. The first performs exact projection in O(n) expected time, where n is the dimension of the space. The second ...</div></span>
<span id="toHide35" style="display:none;"><br /><div style="display:inline"><p>We describe efficient algorithms for projecting a vector onto the <i>l</i><sub>1</sub>-ball. We present two methods for projection. The first performs exact projection in <i>O(n)</i> expected time, where <i>n</i> is the dimension of the space. The second works on vectors <i>k</i> of whose elements are perturbed outside the <i>l</i><sub>1</sub>-ball, projecting in <i>O(k</i> log(<i>n</i>)) time. This setting is especially useful for online learning in sparse feature spaces such as text categorization applications. We demonstrate the merits and effectiveness of our algorithms in numerous batch and online learning tasks. We show that variants of stochastic gradient projection methods augmented with our efficient projection procedures outperform interior point methods, which are considered state-of-the-art optimization techniques. We also show that in online settings gradient updates with <i>l</i><sub>1</sub> projections outperform the exponentiated gradient algorithm while obtaining models with high degrees of sparsity.</p></div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390192">Pointwise exact bootstrap distributions of cost curves</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597104">Charles Dugas</a>,
<a href="author_page.cfm?id=81421595801">David Gadoury</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 280-287</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390192" title="DOI">10.1145/1390156.1390192</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390192&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow36" style="display:inline;"><br /><div style="display:inline">Cost curves have recently been introduced as an alternative or complement to ROC curves in order to visualize binary classifiers performance. Of importance to both cost and ROC curves is the computation of confidence intervals along with the curves themselves ...</div></span>
<span id="toHide36" style="display:none;"><br /><div style="display:inline"><p>Cost curves have recently been introduced as an alternative or complement to ROC curves in order to visualize binary classifiers performance. Of importance to both cost and ROC curves is the computation of confidence intervals along with the curves themselves so that the reliability of a classifier's performance can be assessed. Computing confidence intervals for the difference in performance between two classifiers allows the determination of whether one classifier performs <i>significantly</i> better than another. A simple procedure to obtain confidence intervals for costs or the difference between two costs, under various <i>operating conditions</i>, is to perform bootstrap resampling of the test set. In this paper, we derive <i>exact</i> bootstrap distributions for these values and use these dstributions to obtain confidence intervals, under various operating conditions. Performances of these confidence intervals are measured in terms of coverage accuracies. Simulations show excellent results.</p></div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390193">Polyhedral classifier for target detection: a case study: colorectal cancer</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421593120">M. Murat Dundar</a>,
<a href="author_page.cfm?id=81421592285">Matthias Wolf</a>,
<a href="author_page.cfm?id=81100123065">Sarang Lakare</a>,
<a href="author_page.cfm?id=81326492345">Marcos Salganicoff</a>,
<a href="author_page.cfm?id=81100214434">Vikas C. Raykar</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 288-295</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390193" title="DOI">10.1145/1390156.1390193</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390193&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow37" style="display:inline;"><br /><div style="display:inline">In this study we introduce a novel algorithm for learning a polyhedron to describe the target class. The proposed approach takes advantage of the limited subclass information made available for the negative samples and jointly optimizes multiple hyperplane ...</div></span>
<span id="toHide37" style="display:none;"><br /><div style="display:inline"><p>In this study we introduce a novel algorithm for learning a polyhedron to describe the target class. The proposed approach takes advantage of the limited subclass information made available for the negative samples and jointly optimizes multiple hyperplane classifiers each of which is designed to classify positive samples from a subclass of the negative samples. The flat faces of the polyhedron provides robustness whereas multiple faces contributes to the flexibility required to deal with complex datasets. Apart from improving the prediction accuracy of the system, the proposed polyhedral classifier also provides run-time speedups as a by-product when executed in a cascaded framework in real-time. We evaluate the performance of the proposed technique on a real-world Colon dataset both in terms of prediction accuracy and online execution speed.</p></div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390194">Active reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81361603566">Arkady Epshteyn</a>,
<a href="author_page.cfm?id=81421601169">Adam Vogel</a>,
<a href="author_page.cfm?id=81100619998">Gerald DeJong</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 296-303</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390194" title="DOI">10.1145/1390156.1390194</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390194&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow38" style="display:inline;"><br /><div style="display:inline">When the transition probabilities and rewards of a Markov Decision Process (MDP) are known, an agent can obtain the optimal policy without any interaction with the environment. However, exact transition probabilities are difficult for experts to specify. ...</div></span>
<span id="toHide38" style="display:none;"><br /><div style="display:inline"><p>When the transition probabilities and rewards of a Markov Decision Process (MDP) are known, an agent can obtain the optimal policy without any interaction with the environment. However, exact transition probabilities are difficult for experts to specify. One option left to an agent is a long and potentially costly exploration of the environment. In this paper, we propose another alternative: given initial (possibly inaccurate) specification of the MDP, the agent determines the sensitivity of the optimal policy to changes in transitions and rewards. It then focuses its exploration on the regions of space to which the optimal policy is most sensitive. We show that the proposed exploration strategy performs well on several control and planning problems.</p></div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390195">Training structural SVMs when exact inference is intractable</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100568786">Thomas Finley</a>,
<a href="author_page.cfm?id=81100184551">Thorsten Joachims</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 304-311</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390195" title="DOI">10.1145/1390156.1390195</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390195&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow39" style="display:inline;"><br /><div style="display:inline">While discriminative training (e.g., CRF, structural SVM) holds much promise for machine translation, image segmentation, and clustering, the complex inference these applications require make exact training intractable. This leads to a need for approximate ...</div></span>
<span id="toHide39" style="display:none;"><br /><div style="display:inline"><p>While discriminative training (e.g., CRF, structural SVM) holds much promise for machine translation, image segmentation, and clustering, the complex inference these applications require make exact training intractable. This leads to a need for approximate training methods. Unfortunately, knowledge about how to perform efficient and effective approximate training is limited. Focusing on structural SVMs, we provide and explore algorithms for two different classes of approximate training algorithms, which we call undergenerating (e.g., greedy) and overgenerating (e.g., relaxations) algorithms. We provide a theoretical and empirical analysis of both types of approximate trained structural SVMs, focusing on fully connected pairwise Markov random fields. We find that models trained with overgenerating methods have theoretic advantages over undergenerating methods, are empirically robust relative to their undergenerating brethren, and relaxed trained models favor non-fractional predictions from relaxed predictors.</p></div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390196">An HDP-HMM for systems with state persistence</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421601337">Emily B. Fox</a>,
<a href="author_page.cfm?id=81100340809">Erik B. Sudderth</a>,
<a href="author_page.cfm?id=81339507945">Michael I. Jordan</a>,
<a href="author_page.cfm?id=81100007609">Alan S. Willsky</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 312-319</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390196" title="DOI">10.1145/1390156.1390196</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390196&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow40" style="display:inline;"><br /><div style="display:inline">The hierarchical Dirichlet process hidden Markov model (HDP-HMM) is a flexible, nonparametric model which allows state spaces of unknown size to be learned from data. We demonstrate some limitations of the original HDP-HMM formulation (Teh et al., 2006), ...</div></span>
<span id="toHide40" style="display:none;"><br /><div style="display:inline"><p>The hierarchical Dirichlet process hidden Markov model (HDP-HMM) is a flexible, nonparametric model which allows state spaces of unknown size to be learned from data. We demonstrate some limitations of the original HDP-HMM formulation (Teh et al., 2006), and propose a <i>sticky</i> extension which allows more robust learning of smoothly varying dynamics. Using DP mixtures, this formulation also allows learning of more complex, multimodal emission distributions. We further develop a sampling algorithm that employs a truncated approximation of the DP to jointly resample the full state sequence, greatly improving mixing rates. Via extensive experiments with synthetic data and the NIST speaker diarization database, we demonstrate the advantages of our sticky extension, and the utility of the HDP-HMM in real-world applications.</p></div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390197">Optimized cutting plane algorithm for support vector machines</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100008473">Vojt&#283;ch Franc</a>,
<a href="author_page.cfm?id=81330498803">Soeren Sonnenburg</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 320-327</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390197" title="DOI">10.1145/1390156.1390197</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390197&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow41" style="display:inline;"><br /><div style="display:inline">We have developed a new Linear Support Vector Machine (SVM) training algorithm called OCAS. Its computational effort scales linearly with the sample size. In an extensive empirical evaluation OCAS significantly outperforms current state of the art SVM ...</div></span>
<span id="toHide41" style="display:none;"><br /><div style="display:inline"><p>We have developed a new Linear Support Vector Machine (SVM) training algorithm called OCAS. Its computational effort scales linearly with the sample size. In an extensive empirical evaluation OCAS significantly outperforms current state of the art SVM solvers, like SVM<sup>light</sup>, SVM<sup>perf</sup> and BMRM, achieving speedups of over 1,000 on some datasets over SVM<sup>light</sup> and 20 over SVM<sup>perf</sup>, while obtaining the same precise Support Vector solution. OCAS even in the early optimization steps shows often faster convergence than the so far in this domain prevailing approximative methods SGD and Pegasos. Effectively parallelizing OCAS we were able to train on a dataset of size 15 million examples (itself about 32GB in size) in just 671 seconds --- a competing string kernel SVM required 97,484 seconds to train on 10 million examples sub-sampled from this dataset.</p></div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390198">Stopping conditions for exact computation of leave-one-out error in support vector machines</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100008473">Vojt&#283;ch Franc</a>,
<a href="author_page.cfm?id=81100345404">Pavel Laskov</a>,
<a href="author_page.cfm?id=81100614579">Klaus-Robert M&#252;ller</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 328-335</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390198" title="DOI">10.1145/1390156.1390198</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390198&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow42" style="display:inline;"><br /><div style="display:inline">We propose a new stopping condition for a Support Vector Machine (SVM) solver which precisely reflects the objective of the Leave-One-Out error computation. The stopping condition guarantees that the output on an intermediate SVM solution is identical ...</div></span>
<span id="toHide42" style="display:none;"><br /><div style="display:inline"><p>We propose a new stopping condition for a Support Vector Machine (SVM) solver which precisely reflects the objective of the Leave-One-Out error computation. The stopping condition guarantees that the output on an intermediate SVM solution is identical to the output of the optimal SVM solution with one data point excluded from the training set. A simple augmentation of a general SVM training algorithm allows one to use a stopping criterion equivalent to the proposed sufficient condition. A comprehensive experimental evaluation of our method shows consistent speedup of the exact LOO computation by our method, up to the factor of 13 for the linear kernel. The new algorithm can be seen as an example of constructive guidance of an optimization algorithm towards achieving the best attainable expected risk at optimal computational cost.</p></div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390199">Reinforcement learning in the presence of rare events</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435595924">Jordan Frank</a>,
<a href="author_page.cfm?id=81100515533">Shie Mannor</a>,
<a href="author_page.cfm?id=81100275806">Doina Precup</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 336-343</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390199" title="DOI">10.1145/1390156.1390199</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390199&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow43" style="display:inline;"><br /><div style="display:inline">We consider the task of reinforcement learning in an environment in which rare significant events occur independently of the actions selected by the controlling agent. If these events are sampled according to their natural probability of occurring, convergence ...</div></span>
<span id="toHide43" style="display:none;"><br /><div style="display:inline"><p>We consider the task of reinforcement learning in an environment in which rare significant events occur independently of the actions selected by the controlling agent. If these events are sampled according to their natural probability of occurring, convergence of conventional reinforcement learning algorithms is likely to be slow, and the learning algorithms may exhibit high variance. In this work, we assume that we have access to a simulator, in which the rare event probabilities can be artificially altered. Then, importance sampling can be used to learn with this simulation data. We introduce algorithms for policy evaluation, using both tabular and function approximation representations of the value function. We prove that in both cases, the reinforcement learning algorithms converge. In the tabular case, we also analyze the bias and variance of our approach compared to TD-learning. We evaluate empirically the performance of the algorithm on random Markov Decision Processes, as well as on a large network planning task.</p></div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390200">Memory bounded inference in topic models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421593469">Ryan Gomes</a>,
<a href="author_page.cfm?id=81100461506">Max Welling</a>,
<a href="author_page.cfm?id=81336492228">Pietro Perona</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 344-351</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390200" title="DOI">10.1145/1390156.1390200</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390200&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow44" style="display:inline;"><br /><div style="display:inline">What type of algorithms and statistical techniques support learning from very large datasets over long stretches of time? We address this question through a memory bounded version of a variational EM algorithm that approximates inference in a topic model. ...</div></span>
<span id="toHide44" style="display:none;"><br /><div style="display:inline"><p>What type of algorithms and statistical techniques support learning from very large datasets over long stretches of time? We address this question through a memory bounded version of a variational EM algorithm that approximates inference in a topic model. The algorithm alternates two phases: "model building" and "model compression" in order to always satisfy a given memory constraint. The model building phase expands its internal representation (the number of topics) as more data arrives through Bayesian model selection. Compression is achieved by merging data-items in clumps and only caching their sufficient statistics. Empirically, the resulting algorithm is able to handle datasets that are orders of magnitude larger than the standard batch version.</p></div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390201">Localized multiple kernel learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421599627">Mehmet G&#246;nen</a>,
<a href="author_page.cfm?id=81100187315">Ethem Alpaydin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 352-359</span></td>
</tr>

<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390201" title="DOI">10.1145/1390156.1390201</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390201&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow45" style="display:inline;"><br /><div style="display:inline">Recently, instead of selecting a single kernel, multiple kernel learning (MKL) has been proposed which uses a convex combination of kernels, where the weight of each kernel is optimized during training. However, MKL assigns the same weight to a kernel ...</div></span>
<span id="toHide45" style="display:none;"><br /><div style="display:inline"><p>Recently, instead of selecting a single kernel, multiple kernel learning (MKL) has been proposed which uses a convex combination of kernels, where the weight of each kernel is optimized during training. However, MKL assigns the same weight to a kernel over the whole input space. In this paper, we develop a localized multiple kernel learning (LMKL) algorithm using a gating model for selecting the appropriate kernel function locally. The localizing gating model and the kernel-based classifier are coupled and their optimization is done in a joint manner. Empirical results on ten benchmark and two bioinformatics data sets validate the applicability of our approach. LMKL achieves statistically similar accuracy results compared with MKL by storing fewer support vectors. LMKL can also combine multiple copies of the same kernel function localized in different parts. For example, LMKL with multiple linear kernels gives better accuracy results than using a single linear kernel on bioinformatics data sets.</p></div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390202">No-regret learning in convex games</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100037343">Geoffrey J. Gordon</a>,
<a href="author_page.cfm?id=81100237596">Amy Greenwald</a>,
<a href="author_page.cfm?id=81421598734">Casey Marks</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 360-367</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390202" title="DOI">10.1145/1390156.1390202</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390202&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow46" style="display:inline;"><br /><div style="display:inline">Quite a bit is known about minimizing different kinds of regret in experts problems, and how these regret types relate to types of equilibria in the multiagent setting of repeated matrix games. Much less is known about the possible kinds of regret in ...</div></span>
<span id="toHide46" style="display:none;"><br /><div style="display:inline"><p>Quite a bit is known about minimizing different kinds of regret in experts problems, and how these regret types relate to types of equilibria in the multiagent setting of repeated matrix games. Much less is known about the possible kinds of regret in online convex programming problems (OCPs), or about equilibria in the analogous multiagent setting of repeated convex games. This gap is unfortunate, since convex games are much more expressive than matrix games, and since many important machine learning problems can be expressed as OCPs. In this paper, we work to close this gap: we analyze a spectrum of regret types which lie between <i>external</i> and <i>swap</i> regret, along with their corresponding equilibria, which lie between <i>coarse correlated</i> and <i>correlated</i> equilibrium. We also analyze algorithms for minimizing these regret types. As examples of our framework, we derive algorithms for learning correlated equilibria in polyhedral convex games and extensive-form correlated equilibria in extensive-form games. The former is exponentially more efficient than previous algorithms, and the latter is the first of its type.</p></div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390203">Boosting with incomplete information</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100078777">Gholamreza Haffari</a>,
<a href="author_page.cfm?id=81100232541">Yang Wang</a>,
<a href="author_page.cfm?id=81451597400">Shaojun Wang</a>,
<a href="author_page.cfm?id=81100500947">Greg Mori</a>,
<a href="author_page.cfm?id=81541339156">Feng Jiao</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 368-375</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390203" title="DOI">10.1145/1390156.1390203</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390203&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow47" style="display:inline;"><br /><div style="display:inline">In real-world machine learning problems, it is very common that part of the input feature vector is incomplete: either not available, missing, or corrupted. In this paper, we present a boosting approach that integrates features with incomplete information ...</div></span>
<span id="toHide47" style="display:none;"><br /><div style="display:inline"><p>In real-world machine learning problems, it is very common that part of the input feature vector is incomplete: either not available, missing, or corrupted. In this paper, we present a boosting approach that integrates features with incomplete information and those with complete information to form a strong classifier. By introducing hidden variables to model missing information, we form loss functions that combine fully labeled data with partially labeled data to effectively learn normalized and unnormalized models. The primal problems of the proposed optimization problems with these loss functions are provided to show their close relationship and the motivations behind them. We use auxiliary functions to bound the change of the loss functions and derive explicit parameter update rules for the learning algorithms. We demonstrate encouraging results on two real-world problems --- visual object recognition in computer vision and named entity recognition in natural language processing --- to show the effectiveness of the proposed boosting approach.</p></div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390204">Grassmann discriminant analysis: a unifying view on subspace-based learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81436593865">Jihun Hamm</a>,
<a href="author_page.cfm?id=81100397923">Daniel D. Lee</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 376-383</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390204" title="DOI">10.1145/1390156.1390204</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390204&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow48" style="display:inline;"><br /><div style="display:inline">In this paper we propose a discriminant learning framework for problems in which data consist of linear subspaces instead of vectors. By treating subspaces as basic elements, we can make learning algorithms adapt naturally to the problems with linear ...</div></span>
<span id="toHide48" style="display:none;"><br /><div style="display:inline"><p>In this paper we propose a discriminant learning framework for problems in which data consist of linear subspaces instead of vectors. By treating subspaces as basic elements, we can make learning algorithms adapt naturally to the problems with linear invariant structures. We propose a unifying view on the subspace-based learning method by formulating the problems on the Grassmann manifold, which is the set of fixed-dimensional linear subspaces of a Euclidean space. Previous methods on the problem typically adopt an inconsistent strategy: feature extraction is performed in the <i>Euclidean</i> space while <i>non-Euclidean</i> distances are used. In our approach, we treat each sub-space as a point in the Grassmann space, and perform feature extraction and classification in the same space. We show feasibility of the approach by using the Grassmann kernel functions such as the Projection kernel and the Binet-Cauchy kernel. Experiments with real image databases show that the proposed method performs well compared with state-of-the-art algorithms.</p></div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
 </div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390205">Modified MMI/MPE: a direct evaluation of the margin in speech recognition</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421596209">Georg Heigold</a>,
<a href="author_page.cfm?id=81100637333">Thomas Deselaers</a>,
<a href="author_page.cfm?id=81100622650">Ralf Schl&#252;ter</a>,
<a href="author_page.cfm?id=81100558834">Hermann Ney</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 384-391</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390205" title="DOI">10.1145/1390156.1390205</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390205&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow49" style="display:inline;"><br /><div style="display:inline">In this paper we show how common speech recognition training criteria such as the Minimum Phone Error criterion or the Maximum Mutual Information criterion can be extended to incorporate a margin term. Different margin-based training algorithms have ...</div></span>
<span id="toHide49" style="display:none;"><br /><div style="display:inline"><p>In this paper we show how common speech recognition training criteria such as the Minimum Phone Error criterion or the Maximum Mutual Information criterion can be extended to incorporate a margin term. Different margin-based training algorithms have been proposed to refine existing training algorithms for general machine learning problems. However, for speech recognition, some special problems have to be addressed and all approaches proposed either lack practical applicability or the inclusion of a margin term enforces significant changes to the underlying model, e.g. the optimization algorithm, the loss function, or the parameterization of the model. In our approach, the conventional training criteria are modified to incorporate a margin term. This allows us to do large-margin training in speech recognition using the same efficient algorithms for accumulation and optimization and to use the same software as for conventional discriminative training. We show that the proposed criteria are equivalent to Support Vector Machines with suitable smooth loss functions, approximating the non-smooth hinge loss function or the hard error (e.g. phone error). Experimental results are given for two different tasks: the rather simple digit string recognition task Sietill which severely suffers from overfitting and the large vocabulary European Parliament Plenary Sessions English task which is supposed to be dominated by the risk and the generalization does not seem to be such an issue.</p></div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390206">Statistical models for partial membership</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309509693">Katherine A. Heller</a>,
<a href="author_page.cfm?id=81421595375">Sinead Williamson</a>,
<a href="author_page.cfm?id=81100572858">Zoubin Ghahramani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 392-399</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390206" title="DOI">10.1145/1390156.1390206</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390206&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow50" style="display:inline;"><br /><div style="display:inline">We present a principled Bayesian framework for modeling partial memberships of data points to clusters. Unlike a standard mixture model which assumes that each data point belongs to one and only one mixture component, or cluster, a partial membership ...</div></span>
<span id="toHide50" style="display:none;"><br /><div style="display:inline"><p>We present a principled Bayesian framework for modeling <i>partial memberships</i> of data points to clusters. Unlike a standard mixture model which assumes that each data point belongs to one and only one mixture component, or cluster, a partial membership model allows data points to have fractional membership in multiple clusters. Algorithms which assign data points partial memberships to clusters can be useful for tasks such as clustering genes based on microarray data (Gasch & Eisen, 2002). Our Bayesian Partial Membership Model (BPM) uses exponential family distributions to model each cluster, and a product of these distibtutions, with weighted parameters, to model each datapoint. Here the weights correspond to the degree to which the datapoint belongs to each cluster. All parameters in the BPM are continuous, so we can use Hybrid Monte Carlo to perform inference and learning. We discuss relationships between the BPM and Latent Dirichlet Allocation, Mixed Membership models, Exponential Family PCA, and fuzzy clustering. Lastly, we show some experimental results and discuss nonparametric extensions to our model.</p></div></span> <a id="expcoll50" href="JavaScript: expandcollapse('expcoll50',50)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390207">Active kernel learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100337419">Steven C. H. Hoi</a>,
<a href="author_page.cfm?id=81100054575">Rong Jin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 400-407</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390207" title="DOI">10.1145/1390156.1390207</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390207&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow51" style="display:inline;"><br /><div style="display:inline">Identifying the appropriate kernel function/matrix for a given dataset is essential to all kernel-based learning techniques. A number of kernel learning algorithms have been proposed to learn kernel functions or matrices from side information (e.g., ...</div></span>
<span id="toHide51" style="display:none;"><br /><div style="display:inline"><p>Identifying the appropriate kernel function/matrix for a given dataset is essential to all kernel-based learning techniques. A number of kernel learning algorithms have been proposed to learn kernel functions or matrices from side information (e.g., either labeled examples or pairwise constraints). However, most previous studies are limited to "passive" kernel learning in which side information is provided beforehand. In this paper we present a framework of <i>Active Kernel Learning</i> (<b>AKL</b>) that actively identifies the most informative pairwise constraints for kernel learning. The key challenge of active kernel learning is how to measure the informativeness of an example pair given its class label is unknown. To this end, we propose a <b>min-max</b> approach for active kernel learning that selects the example pair that results in a large classification margin regardless of its assigned class label. We furthermore approximate the related optimization problem into a convex programming problem. We evaluate the effectiveness of the proposed algorithm by comparing it to two other implementations of active kernel learning. Empirical study with nine datasets on semi-supervised data clustering shows that the proposed algorithm is more effective than its competitors.</p></div></span> <a id="expcoll51" href="JavaScript: expandcollapse('expcoll51',51)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390208">A dual coordinate descent method for large-scale linear SVM</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365592567">Cho-Jui Hsieh</a>,
<a href="author_page.cfm?id=81365593715">Kai-Wei Chang</a>,
<a href="author_page.cfm?id=81384612017">Chih-Jen Lin</a>,
<a href="author_page.cfm?id=81100176197">S. Sathiya Keerthi</a>,
<a href="author_page.cfm?id=81310499952">S. Sundararajan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 408-415</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390208" title="DOI">10.1145/1390156.1390208</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390208&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
 <div style="padding-left:0">
<span id="toShow52" style="display:inline;"><br /><div style="display:inline">In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines (SVM) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent ...</div></span>
<span id="toHide52" style="display:none;"><br /><div style="display:inline"><p>In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines (SVM) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent method for linear SVM with L1-and L2-loss functions. The proposed method is simple and reaches an <i>&epsilon;</i>-accurate solution in <i>O</i>(log(1/<i>&epsilon;</i>)) iterations. Experiments indicate that our method is much faster than state of the art solvers such as Pegasos, TRON, SVM<sup>perf</sup>, and a recent primal coordinate descent implementation.</p></div></span> <a id="expcoll52" href="JavaScript: expandcollapse('expcoll52',52)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390209">Discriminative structure and parameter learning for Markov logic networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421593842">Tuyen N. Huynh</a>,
<a href="author_page.cfm?id=81100539345">Raymond J. Mooney</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 416-423</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390209" title="DOI">10.1145/1390156.1390209</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390209&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow53" style="display:inline;"><br /><div style="display:inline">Markov logic networks (MLNs) are an expressive representation for statistical relational learning that generalizes both first-order logic and graphical models. Existing methods for learning the logical structure of an MLN are not discriminative; however, ...</div></span>
<span id="toHide53" style="display:none;"><br /><div style="display:inline"><p>Markov logic networks (MLNs) are an expressive representation for statistical relational learning that generalizes both first-order logic and graphical models. Existing methods for learning the logical structure of an MLN are not discriminative; however, many relational learning problems involve specific target predicates that must be inferred from given background information. We found that existing MLN methods perform very poorly on several such ILP benchmark problems, and we present improved discriminative methods for learning MLN clauses and weights that outperform existing MLN and traditional ILP methods.</p></div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390210">Causal modelling combining instantaneous and lagged effects: an identifiable model based on non-Gaussianity</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100100305">Aapo Hyv&#228;rinen</a>,
<a href="author_page.cfm?id=81365595417">Shohei Shimizu</a>,
<a href="author_page.cfm?id=81100304686">Patrik O. Hoyer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 424-431</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390210" title="DOI">10.1145/1390156.1390210</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390210&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow54" style="display:inline;"><br /><div style="display:inline">Causal analysis of continuous-valued variables typically uses either autoregressive models or linear Gaussian Bayesian networks with instantaneous effects. Estimation of Gaussian Bayesian networks poses serious identifiability problems, which is why ...</div></span>
<span id="toHide54" style="display:none;"><br /><div style="display:inline"><p>Causal analysis of continuous-valued variables typically uses either autoregressive models or linear Gaussian Bayesian networks with instantaneous effects. Estimation of Gaussian Bayesian networks poses serious identifiability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. We show that such a non-Gaussian model is identifiable without prior knowledge of network structure, and we propose an estimation method shown to be consistent. This approach also points out how neglecting instantaneous effects can lead to completely wrong estimates of the autoregressive coefficients.</p></div></span> <a id="expcoll54" href="JavaScript: expandcollapse('expcoll54',54)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390211">Hierarchical model-based reinforcement learning: <scp>R-max</scp> + MAXQ</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81336490214">Nicholas K. Jong</a>,
<a href="author_page.cfm?id=81100388010">Peter Stone</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 432-439</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390211" title="DOI">10.1145/1390156.1390211</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390211&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow55" style="display:inline;"><br /><div style="display:inline">Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their underlying structure. Model-based algorithms, which provided the first finite-time convergence guarantees for reinforcement ...</div></span>
<span id="toHide55" style="display:none;"><br /><div style="display:inline"><p>Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their underlying structure. Model-based algorithms, which provided the first finite-time convergence guarantees for reinforcement learning, may also play an important role in coping with the relative scarcity of data in large environments. In this paper, we introduce an algorithm that fully integrates modern hierarchical and model-learning methods in the standard reinforcement learning setting. Our algorithm, <scp>R-maxq</scp>, inherits the efficient model-based exploration of the <scp>R-max</scp> algorithm and the opportunities for abstraction provided by the MAXQ framework. We analyze the sample complexity of our algorithm, and our experiments in a standard simulation environment illustrate the advantages of combining hierarchies and models.</p></div></span> <a id="expcoll55" href="JavaScript: expandcollapse('expcoll55',55)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390212">Efficient bandit algorithms for online multiclass prediction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100155305">Sham M. Kakade</a>,
<a href="author_page.cfm?id=81100489319">Shai Shalev-Shwartz</a>,
<a href="author_page.cfm?id=81330499128">Ambuj Tewari</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 440-447</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390212" title="DOI">10.1145/1390156.1390212</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390212&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow56" style="display:inline;"><br /><div style="display:inline">This paper introduces the Banditron, a variant of the Perceptron [Rosenblatt, 1958], for the multiclass bandit setting. The multiclass bandit setting models a wide range of practical supervised learning applications where the learner only receives partial ...</div></span>
<span id="toHide56" style="display:none;"><br /><div style="display:inline"><p>This paper introduces the Banditron, a variant of the Perceptron [Rosenblatt, 1958], for the multiclass bandit setting. The multiclass bandit setting models a wide range of practical supervised learning applications where the learner only receives partial feedback (referred to as "bandit" feedback, in the spirit of multi-armed bandit models) with respect to the true label (e.g. in many web applications users often only provide positive "click" feedback which does not necessarily fully disclose a true label). The Banditron has the ability to learn in a multiclass classification setting with the "bandit" feedback which only reveals whether or not the prediction made by the algorithm was correct or not (but does not necessarily reveal the true label). We provide (relative) mistake bounds which show how the Banditron enjoys favorable performance, and our experiments demonstrate the practicality of the algorithm. Furthermore, this paper pays close attention to the important special case when the data is linearly separable --- a problem which has been exhaustively studied in the full information setting yet is novel in the bandit setting.</p></div></span> <a id="expcoll56" href="JavaScript: expandcollapse('expcoll56',56)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390213">Large scale manifold transduction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421593627">Michael Karlen</a>,
<a href="author_page.cfm?id=81100015405">Jason Weston</a>,
<a href="author_page.cfm?id=81365595999">Ayse Erkan</a>,
<a href="author_page.cfm?id=81100001072">Ronan Collobert</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 448-455</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390213" title="DOI">10.1145/1390156.1390213</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390213&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow57" style="display:inline;"><br /><div style="display:inline">We show how the regularizer of Transductive Support Vector Machines (TSVM) can be trained by stochastic gradient descent for linear models and multi-layer architectures. The resulting methods can be trained online, have vastly superior training ...</div></span>
<span id="toHide57" style="display:none;"><br /><div style="display:inline"><p>We show how the regularizer of Transductive Support Vector Machines (TSVM) can be trained by stochastic gradient descent for linear models and multi-layer architectures. The resulting methods can be trained <i>online</i>, have vastly superior training and testing speed to existing TSVM algorithms, can encode prior knowledge in the network architecture, and obtain competitive error rates. We then go on to propose a natural generalization of the TSVM loss function that takes into account neighborhood and manifold information directly, unifying the two-stage Low Density Separation method into a single criterion, and leading to state-of-the-art results.</p></div></span> <a id="expcoll57" href="JavaScript: expandcollapse('expcoll57',57)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390214">Non-parametric policy gradients: a unified treatment of propositional and relational domains</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81337490510">Kristian Kersting</a>,
<a href="author_page.cfm?id=81100650504">Kurt Driessens</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 456-463</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390214" title="DOI">10.1145/1390156.1390214</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390214&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow58" style="display:inline;"><br /><div style="display:inline">Policy gradient approaches are a powerful instrument for learning how to interact with the environment. Existing approaches have focused on propositional and continuous domains only. Without extensive feature engineering, it is difficult - if not impossible ...</div></span>
<span id="toHide58" style="display:none;"><br /><div style="display:inline"><p>Policy gradient approaches are a powerful instrument for learning how to interact with the environment. Existing approaches have focused on propositional and continuous domains only. Without extensive feature engineering, it is difficult - if not impossible - to apply them within structured domains, in which e.g. there is a varying number of objects and relations among them. In this paper, we describe a non-parametric policy gradient approach - called NPPG - that overcomes this limitation. The key idea is to apply Friedmann's gradient boosting: policies are represented as a weighted sum of regression models grown in an stage-wise optimization. Employing off-the-shelf regression learners, NPPG can deal with propositional, continuous, and relational domains in a unified way. Our experimental results show that it can even improve on established results.</p></div></span> <a id="expcoll58" href="JavaScript: expandcollapse('expcoll58',58)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390215">ICA and ISA using Schweizer-Wolff measure of dependence</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421593891">Sergey Kirshner</a>,
<a href="author_page.cfm?id=81309498613">Barnab&#225;s P&#243;czos</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 464-471</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390215" title="DOI">10.1145/1390156.1390215</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390215&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow59" style="display:inline;"><br /><div style="display:inline">We propose a new algorithm for independent component and independent subspace analysis problems. This algorithm uses a contrast based on the Schweizer-Wolff measure of pairwise dependence (Schweizer & Wolff, 1981), a non-parametric measure computed on ...</div></span>
<span id="toHide59" style="display:none;"><br /><div style="display:inline"><p>We propose a new algorithm for independent component and independent subspace analysis problems. This algorithm uses a contrast based on the Schweizer-Wolff measure of pairwise dependence (Schweizer & Wolff, 1981), a non-parametric measure computed on pairwise ranks of the variables. Our algorithm frequently outperforms state of the art ICA methods in the normal setting, is significantly more robust to outliers in the mixed signals, and performs well even in the presence of noise. Our method can also be used to solve independent subspace analysis (ISA) problems by grouping signals recovered by ICA methods. We provide an extensive empirical evaluation using simulated, sound, and image data.</p></div></span> <a id="expcoll59" href="JavaScript: expandcollapse('expcoll59',59)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390216">Unsupervised rank aggregation with distance-based models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323492365">Alexandre Klementiev</a>,
<a href="author_page.cfm?id=81100201453">Dan Roth</a>,
<a href="author_page.cfm?id=81100249076">Kevin Small</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 472-479</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390216" title="DOI">10.1145/1390156.1390216</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390216&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow60" style="display:inline;"><br /><div style="display:inline">The need to meaningfully combine sets of rankings often comes up when one deals with ranked data. Although a number of heuristic and supervised learning approaches to rank aggregation exist, they require domain knowledge or supervised ranked data, both ...</div></span>
<span id="toHide60" style="display:none;"><br /><div style="display:inline"><p>The need to meaningfully combine sets of rankings often comes up when one deals with ranked data. Although a number of heuristic and supervised learning approaches to rank aggregation exist, they require domain knowledge or supervised ranked data, both of which are expensive to acquire. In order to address these limitations, we propose a mathematical and algorithmic framework for learning to aggregate (partial) rankings without supervision. We instantiate the framework for the cases of combining permutations and combining top-<i>k</i> lists, and propose a novel metric for the latter. Experiments in both scenarios demonstrate the effectiveness of the proposed formalism.</p></div></span> <a id="expcoll60" href="JavaScript: expandcollapse('expcoll60',60)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390217">On partial optimality in multi-label MRFs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81339510339">Pushmeet Kohli</a>,
<a href="author_page.cfm?id=81421596695">Alexander Shekhovtsov</a>,
<a href="author_page.cfm?id=81100467323">Carsten Rother</a>,
<a href="author_page.cfm?id=81100432019">Vladimir Kolmogorov</a>,
<a href="author_page.cfm?id=81100100032">Philip Torr</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 480-487</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390217" title="DOI">10.1145/1390156.1390217</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390217&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow61" style="display:inline;"><br /><div style="display:inline">We consider the problem of optimizing multilabel MRFs, which is in general NP-hard and ubiquitous in low-level computer vision. One approach for its solution is to formulate it as an integer linear programming and relax the integrality constraints. The ...</div></span>
<span id="toHide61" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of optimizing multilabel MRFs, which is in general NP-hard and ubiquitous in low-level computer vision. One approach for its solution is to formulate it as an integer linear programming and relax the integrality constraints. The approach we consider in this paper is to first convert the multi-label MRF into an equivalent binary-label MRF and then to relax it. The resulting relaxation can be efficiently solved using a maximum flow algorithm. Its solution provides us with a partially optimal labelling of the binary variables. This partial labelling is then easily transferred to the multi-label problem. We study the theoretical properties of the new relaxation and compare it with the standard one. Specifically, we compare tightness, and characterize a subclass of problems where the two relaxations coincide. We propose several combined algorithms based on the technique and demonstrate their performance on challenging computer vision problems.</p></div></span> <a id="expcoll61" href="JavaScript: expandcollapse('expcoll61',61)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390218">Space-indexed dynamic programming: learning to follow trajectories</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100095014">J. Zico Kolter</a>,
<a href="author_page.cfm?id=81421601840">Adam Coates</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>,
<a href="author_page.cfm?id=81542701556">Yi Gu</a>,
<a href="author_page.cfm?id=81421598120">Charles DuHadway</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 488-495</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390218" title="DOI">10.1145/1390156.1390218</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390218&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow62" style="display:inline;"><br /><div style="display:inline">We consider the task of learning to accurately follow a trajectory in a vehicle such as a car or helicopter. A number of dynamic programming algorithms such as Differential Dynamic Programming (DDP) and Policy Search by Dynamic Programming (PSDP), can ...</div></span>
<span id="toHide62" style="display:none;"><br /><div style="display:inline"><p>We consider the task of learning to accurately follow a trajectory in a vehicle such as a car or helicopter. A number of dynamic programming algorithms such as Differential Dynamic Programming (DDP) and Policy Search by Dynamic Programming (PSDP), can efficiently compute non-stationary policies for these tasks --- such policies in general are well-suited to trajectory following since they can easily generate different control actions at different times in order to follow the trajectory. However, a weakness of these algorithms is that their policies are <i>time-indexed</i>, in that they apply different policies depending on the current time. This is problematic since 1) the current time may not correspond well to where we are along the trajectory and 2) the uncertainty over states can prevent these algorithms from finding any good policies at all. In this paper we propose a method for <i>space-indexed</i> dynamic programming that overcomes both these difficulties. We begin by showing how a dynamical system can be rewritten in terms of a spatial index variable (i.e., how far along the trajectory we are) rather than as a function of time. We then use these space-indexed dynamical systems to derive space-indexed version of the DDP and PSDP algorithms. Finally, we show that these algorithms perform well on a variety of control tasks, both in simulation and on real systems.</p></div></span> <a id="expcoll62" href="JavaScript: expandcollapse('expcoll62',62)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390219">The skew spectrum of graphs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100583105">Risi Kondor</a>,
<a href="author_page.cfm?id=81100155678">Karsten M. Borgwardt</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 496-503</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390219" title="DOI">10.1145/1390156.1390219</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390219&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow63" style="display:inline;"><br /><div style="display:inline">The central issue in representing graph-structured data instances in learning algorithms is designing features which are invariant to permuting the numbering of the vertices. We present a new system of invariant graph features which we call the skew ...</div></span>
<span id="toHide63" style="display:none;"><br /><div style="display:inline"><p>The central issue in representing graph-structured data instances in learning algorithms is designing features which are invariant to permuting the numbering of the vertices. We present a new system of invariant graph features which we call the skew spectrum of graphs. The skew spectrum is based on mapping the adjacency matrix of any (weigted, directed, unlabeled) graph to a function on the symmetric group and computing bispectral invariants. The reduced form of the skew spectrum is computable in <i>O(n<sup>3</sup>)</i> time, and experiments show that on several benchmark datasets it can outperform state of the art graph kernels.</p></div></span> <a id="expcoll63" href="JavaScript: expandcollapse('expcoll63',63)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390220">Fast estimation of first-order clause coverage through randomization and maximum likelihood</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421599092">Ond&#345;ej Ku&#382;elka</a>,
<a href="author_page.cfm?id=81100354766">Filip &#381;elezn&#253;</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 504-511</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390220" title="DOI">10.1145/1390156.1390220</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390220&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow64" style="display:inline;"><br /><div style="display:inline">In inductive logic programming, &theta;-subsumption is a widely used coverage test. Unfortunately, testing &theta;-subsumption is NP-complete, which represents a crucial efficiency bottleneck for many relational learners. In this paper, ...</div></span>
<span id="toHide64" style="display:none;"><br /><div style="display:inline"><p>In inductive logic programming, <i>&theta;</i>-subsumption is a widely used coverage test. Unfortunately, testing <i>&theta;</i>-subsumption is NP-complete, which represents a crucial efficiency bottleneck for many relational learners. In this paper, we present a probabilistic estimator of clause coverage, based on a randomized restarted search strategy. Under a distribution assumption, our algorithm can estimate clause coverage without having to decide subsumption for all examples. We implement this algorithm in program ReCovEr. On generated graph data and real-world datasets, we show that ReCovEr provides reasonably accurate estimates while achieving dramatic runtimes improvements compared to a state-of-the-art algorithm.</p></div></span> <a id="expcoll64" href="JavaScript: expandcollapse('expcoll64',64)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390221">Query-level stability and generalization in learning to rank</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435599880">Yanyan Lan</a>,
<a href="author_page.cfm?id=81350580267">Tie-Yan Liu</a>,
<a href="author_page.cfm?id=81100549718">Tao Qin</a>,
<a href="author_page.cfm?id=81100447066">Zhiming Ma</a>,
<a href="author_page.cfm?id=81350598903">Hang Li</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 512-519</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390221" title="DOI">10.1145/1390156.1390221</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390221&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow65" style="display:inline;"><br /><div style="display:inline">This paper is concerned with the generalization ability of learning to rank algorithms for information retrieval (IR). We point out that the key for addressing the learning problem is to look at it from the viewpoint of query. We define a number ...</div></span>
<span id="toHide65" style="display:none;"><br /><div style="display:inline"><p>This paper is concerned with the generalization ability of learning to rank algorithms for information retrieval (IR). We point out that the key for addressing the learning problem is to look at it from the viewpoint of <i>query</i>. We define a number of new concepts, including query-level loss, query-level risk, and query-level stability. We then analyze the generalization ability of learning to rank algorithms by giving query-level generalization bounds to them using query-level stability as a tool. Such an analysis is very helpful for us to derive more advanced algorithms for IR. We apply the proposed theory to the existing algorithms of Ranking SVM and IRSVM. Experimental results on the two algorithms verify the correctness of the theoretical analysis.</p></div></span> <a id="expcoll65" href="JavaScript: expandcollapse('expcoll65',65)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390222">Modeling interleaved hidden processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81330493648">Niels Landwehr</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 520-527</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390222" title="DOI">10.1145/1390156.1390222</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390222&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow66" style="display:inline;"><br /><div style="display:inline">Hidden Markov models assume that observations in time series data stem from some hidden process that can be compactly represented as a Markov chain. We generalize this model by assuming that the observed data stems from multiple hidden processes, ...</div></span>
<span id="toHide66" style="display:none;"><br /><div style="display:inline"><p>Hidden Markov models assume that observations in time series data stem from some hidden process that can be compactly represented as a Markov chain. We generalize this model by assuming that the observed data stems from <i>multiple</i> hidden processes, whose outputs interleave to form the sequence of observations. Exact inference in this model is NP-hard. However, a tractable and effective inference algorithm is obtained by extending structured approximate inference methods used in factorial hidden Markov models. The proposed model is evaluated in an activity recognition domain, where multiple activities interleave and together generate a stream of sensor observations. It is shown to be more accurate than a standard hidden Markov model in this domain.</p></div></span> <a id="expcoll66" href="JavaScript: expandcollapse('expcoll66',66)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390223">Exploration scavenging</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100453722">John Langford</a>,
<a href="author_page.cfm?id=81100216859">Alexander Strehl</a>,
<a href="author_page.cfm?id=81100379669">Jennifer Wortman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 528-535</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390223" title="DOI">10.1145/1390156.1390223</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390223&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow67" style="display:inline;"><br /><div style="display:inline">We examine the problem of evaluating a policy in the contextual bandit setting using only observations collected during the execution of another policy. We show that policy evaluation can be impossible if the exploration policy chooses actions based ...</div></span>
<span id="toHide67" style="display:none;"><br /><div style="display:inline"><p>We examine the problem of evaluating a policy in the contextual bandit setting using only observations collected during the execution of another policy. We show that policy evaluation can be impossible if the exploration policy chooses actions based on the side information provided at each time step. We then propose and prove the correctness of a principled method for policy evaluation which works when this is not the case, even when the exploration policy is deterministic, as long as each action is explored sufficiently often. We apply this general technique to the problem of offline evaluation of internet advertising policies. Although our theoretical results hold only when the exploration policy chooses ads independent of side information, an assumption that is typically violated by commercial systems, we show how clever uses of the theory provide non-trivial and realistic applications. We also provide an empirical demonstration of the effectiveness of our techniques on real ad placement data.</p></div></span> <a id="expcoll67" href="JavaScript: expandcollapse('expcoll67',67)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390224">Classification using discriminative restricted Boltzmann machines</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81319495662">Hugo Larochelle</a>,
<a href="author_page.cfm?id=81100287057">Yoshua Bengio</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 536-543</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390224" title="DOI">10.1145/1390156.1390224</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390224&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow68" style="display:inline;"><br /><div style="display:inline">Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization ...</div></span>
<span id="toHide68" style="display:none;"><br /><div style="display:inline"><p>Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.</p></div></span> <a id="expcoll68" href="JavaScript: expandcollapse('expcoll68',68)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390225">Transfer of samples in batch reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81318489471">Alessandro Lazaric</a>,
<a href="author_page.cfm?id=81100117476">Marcello Restelli</a>,
<a href="author_page.cfm?id=81100129459">Andrea Bonarini</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 544-551</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390225" title="DOI">10.1145/1390156.1390225</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390225&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow69" style="display:inline;"><br /><div style="display:inline">The main objective of transfer in reinforcement learning is to reduce the complexity of learning the solution of a target task by effectively reusing the knowledge retained from solving a set of source tasks. In this paper, we introduce a novel algorithm ...</div></span>
<span id="toHide69" style="display:none;"><br /><div style="display:inline"><p>The main objective of transfer in reinforcement learning is to reduce the complexity of learning the solution of a target task by effectively reusing the knowledge retained from solving a set of source tasks. In this paper, we introduce a novel algorithm that transfers samples (i.e., tuples &lang;<i>s, a, s', r</i>&rang;) from source to target tasks. Under the assumption that tasks have similar transition models and reward functions, we propose a method to select samples from the source tasks that are mostly similar to the target task, and, then, to use them as input for batch reinforcement-learning algorithms. As a result, the number of samples an agent needs to collect from the target task to learn its solution is reduced. We empirically show that, following the proposed approach, the transfer of samples is effective in reducing the learning complexity, even when some source tasks are significantly different from the target task.</p></div></span> <a id="expcoll69" href="JavaScript: expandcollapse('expcoll69',69)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390226">Local likelihood modeling of temporal text streams</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100415296">Guy Lebanon</a>,
<a href="author_page.cfm?id=81421601449">Yang Zhao</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 552-559</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390226" title="DOI">10.1145/1390156.1390226</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390226&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow70" style="display:inline;"><br /><div style="display:inline">Temporal text data is often generated by a time-changing process or distribution. Such a drift in the underlying distribution cannot be captured by stationary likelihood techniques. We consider the application of local likelihood methods to generative ...</div></span>
<span id="toHide70" style="display:none;"><br /><div style="display:inline"><p>Temporal text data is often generated by a time-changing process or distribution. Such a drift in the underlying distribution cannot be captured by stationary likelihood techniques. We consider the application of local likelihood methods to generative and conditional modeling of temporal document sequences. We examine the asymptotic bias and variance and present an experimental study using the RCV1 dataset containing a temporal sequence of Reuters news stories.</p></div></span> <a id="expcoll70" href="JavaScript: expandcollapse('expcoll70',70)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390227">A worst-case comparison between temporal difference and residual gradient with linear function approximation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331497976">Lihong Li</a>
</span>
</td>
</tr>

<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 560-567</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390227" title="DOI">10.1145/1390156.1390227</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390227&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow71" style="display:inline;"><br /><div style="display:inline">Residual gradient (RG) was proposed as an alternative to TD(0) for policy evaluation when function approximation is used, but there exists little formal analysis comparing them except in very limited cases. This paper employs techniques from online learning ...</div></span>
<span id="toHide71" style="display:none;"><br /><div style="display:inline"><p>Residual gradient (RG) was proposed as an alternative to TD(0) for policy evaluation when function approximation is used, but there exists little formal analysis comparing them except in very limited cases. This paper employs techniques from online learning of linear functions and provides a worst-case (non-probabilistic) analysis to compare these two types of algorithms when linear function approximation is used. No statistical assumptions are made on the sequence of observations, so the analysis applies to non-Markovian and even adversarial domains as well. In particular, our results suggest that RG may result in smaller temporal differences, while TD(0) is more likely to yield smaller prediction errors. These phenomena can be observed even in two simple Markov chain examples that are non-adversarial.</p></div></span> <a id="expcoll71" href="JavaScript: expandcollapse('expcoll71',71)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390228">Knows what it knows: a framework for self-aware learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331497976">Lihong Li</a>,
<a href="author_page.cfm?id=81406601119">Michael L. Littman</a>,
<a href="author_page.cfm?id=81100377963">Thomas J. Walsh</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 568-575</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390228" title="DOI">10.1145/1390156.1390228</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390228&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow72" style="display:inline;"><br /><div style="display:inline">We introduce a learning framework that combines elements of the well-known PAC and mistake-bound models. The KWIK (knows what it knows) framework was designed particularly for its utility in learning settings where active exploration can impact the training ...</div></span>
<span id="toHide72" style="display:none;"><br /><div style="display:inline"><p>We introduce a learning framework that combines elements of the well-known PAC and mistake-bound models. The KWIK (knows what it knows) framework was designed particularly for its utility in learning settings where active exploration can impact the training examples the learner is exposed to, as is true in reinforcement-learning and active-learning problems. We catalog several KWIK-learnable classes and open problems.</p></div></span> <a id="expcoll72" href="JavaScript: expandcollapse('expcoll72',72)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390229">Pairwise constraint propagation by semidefinite programming for semi-supervised classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421594456">Zhenguo Li</a>,
<a href="author_page.cfm?id=81452603630">Jianzhuang Liu</a>,
<a href="author_page.cfm?id=81452597691">Xiaoou Tang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 576-583</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390229" title="DOI">10.1145/1390156.1390229</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390229&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow73" style="display:inline;"><br /><div style="display:inline">We consider the general problem of learning from both pairwise constraints and unlabeled data. The pairwise constraints specify whether two objects belong to the same class or not, known as the must-link constraints and the cannot-link constraints. We ...</div></span>
<span id="toHide73" style="display:none;"><br /><div style="display:inline"><p>We consider the general problem of learning from both pairwise constraints and unlabeled data. The pairwise constraints specify whether two objects belong to the same class or not, known as the must-link constraints and the cannot-link constraints. We propose to learn a mapping that is smooth over the data graph and maps the data onto a unit hypersphere, where two must-link objects are mapped to the same point while two cannot-link objects are mapped to be orthogonal. We show that such a mapping can be achieved by formulating a semidefinite programming problem, which is convex and can be solved globally. Our approach can effectively propagate pairwise constraints to the whole data set. It can be directly applied to multi-class classification and can handle data labels, pairwise constraints, or a mixture of them in a unified framework. Promising experimental results are presented for classification tasks on a variety of synthetic and real data sets.</p></div></span> <a id="expcoll73" href="JavaScript: expandcollapse('expcoll73',73)">expand</a>
</div>
</td>
</tr>

<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390230">An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323492904">Percy Liang</a>,
<a href="author_page.cfm?id=81339507945">Michael I. Jordan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 584-591</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390230" title="DOI">10.1145/1390156.1390230</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390230&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow74" style="display:inline;"><br /><div style="display:inline">Statistical and computational concerns have motivated parameter estimators based on various forms of likelihood, e.g., joint, conditional, and pseudolikelihood. In this paper, we present a unified framework for studying these estimators, which allows ...</div></span>
<span id="toHide74" style="display:none;"><br /><div style="display:inline"><p>Statistical and computational concerns have motivated parameter estimators based on various forms of likelihood, e.g., joint, conditional, and pseudolikelihood. In this paper, we present a unified framework for studying these estimators, which allows us to compare their relative (statistical) efficiencies. Our asymptotic analysis suggests that modeling more of the data tends to reduce variance, but at the cost of being more sensitive to model misspecification. We present experiments validating our analysis.</p></div></span> <a id="expcoll74" href="JavaScript: expandcollapse('expcoll74',74)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390231">Structure compilation: trading structure for features</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323492904">Percy Liang</a>,
<a href="author_page.cfm?id=81100635457">Hal Daum&#233;, III</a>,
<a href="author_page.cfm?id=81100142014">Dan Klein</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 592-599</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390231" title="DOI">10.1145/1390156.1390231</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390231&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow75" style="display:inline;"><br /><div style="display:inline">Structured models often achieve excellent performance but can be slow at test time. We investigate structure compilation, where we replace structure with features, which are often computationally simpler but unfortunately statistically more complex. ...</div></span>
<span id="toHide75" style="display:none;"><br /><div style="display:inline"><p>Structured models often achieve excellent performance but can be slow at test time. We investigate <i>structure compilation</i>, where we replace structure with features, which are often computationally simpler but unfortunately statistically more complex. We analyze this tradeoff theoretically and empirically on three natural language processing tasks. We also introduce a simple method to transfer predictive power from structure to features via unlabeled data, while incurring a minimal statistical penalty.</p></div></span> <a id="expcoll75" href="JavaScript: expandcollapse('expcoll75',75)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390232">ManifoldBoost: stagewise function approximation for fully-, semi- and un-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81332513194">Nicolas Loeff</a>,
<a href="author_page.cfm?id=81100502370">David Forsyth</a>,
<a href="author_page.cfm?id=81548258756">Deepak Ramachandran</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 600-607</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390232" title="DOI">10.1145/1390156.1390232</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390232&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow76" style="display:inline;"><br /><div style="display:inline">We describe a manifold learning framewor that naturally accommodates supervised learning, partially supervised learning and unsupervised clustering as particular cases. Our method chooses a function by minimizing loss subject to a manifold regularization ...</div></span>
<span id="toHide76" style="display:none;"><br /><div style="display:inline"><p>We describe a manifold learning framewor that naturally accommodates supervised learning, partially supervised learning and unsupervised clustering as particular cases. Our method chooses a function by minimizing loss subject to a manifold regularization penalty. This augmented cost is minimized using a greedy, stagewise, functional minimization procedure, as in Gradientboost. Each stage of boosting is fast and efficient. We demonstrate our approach using both radial basis function approximations and trees. The performance of our method is at the state of the art on many standard semi-supervised learning benchmarks, and we produce results for large scale datasets.</p></div></span> <a id="expcoll76" href="JavaScript: expandcollapse('expcoll76',76)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390233">Random classification noise defeats all convex potential boosters</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100466924">Philip M. Long</a>,
<a href="author_page.cfm?id=81452616886">Rocco A. Servedio</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 608-615</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390233" title="DOI">10.1145/1390156.1390233</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390233&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow77" style="display:inline;"><br /><div style="display:inline">A broad class of boosting algorithms can be interpreted as performing coordinate-wise gradient descent to minimize some potential function of the margins of a data set. This class includes AdaBoost, LogitBoost, and other widely used and well-studied ...</div></span>
<span id="toHide77" style="display:none;"><br /><div style="display:inline"><p>A broad class of boosting algorithms can be interpreted as performing coordinate-wise gradient descent to minimize some potential function of the margins of a data set. This class includes AdaBoost, LogitBoost, and other widely used and well-studied boosters. In this paper we show that for a broad class of convex potential functions, any such boosting algorithm is highly susceptible to random classification noise. We do this by showing that for any such booster and any nonzero random classification noise rate &eta;, there is a simple data set of examples which is efficiently learnable by such a booster if there is no noise, but which cannot be learned to accuracy better than 1/2 if there is random classification noise at rate &eta;. This negative result is in contrast with known branching program based boosters which do not fall into the convex potential function framework and which can provably learn to high accuracy in the presence of random classification noise.</p></div></span> <a id="expcoll77" href="JavaScript: expandcollapse('expcoll77',77)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390234">Uncorrelated multilinear principal component analysis through successive variance maximization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81312482584">Haiping Lu</a>,
<a href="author_page.cfm?id=81351602650">Konstantinos N. Plataniotis</a>,
<a href="author_page.cfm?id=81100599109">Anastasios N. Venetsanopoulos</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 616-623</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390234" title="DOI">10.1145/1390156.1390234</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390234&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow78" style="display:inline;"><br /><div style="display:inline">Tensorial data are frequently encountered in various machine learning tasks today and dimensionality reduction is one of their most important applications. This paper extends the classical principal component analysis (PCA) to its multilinear version ...</div></span>
<span id="toHide78" style="display:none;"><br /><div style="display:inline"><p>Tensorial data are frequently encountered in various machine learning tasks today and dimensionality reduction is one of their most important applications. This paper extends the classical principal component analysis (PCA) to its multilinear version by proposing a novel unsupervised dimensionality reduction algorithm for tensorial data, named as uncorrelated multilinear PCA (UMPCA). UMPCA seeks a tensor-to-vector projection that captures most of the variation in the original tensorial input while producing uncorrelated features through successive variance maximization. We evaluate the UMPCA on a second-order tensorial problem, face recognition, and the experimental results show its superiority, especially in low-dimensional spaces, through the comparison with three other PCA-based algorithms.</p></div></span> <a id="expcoll78" href="JavaScript: expandcollapse('expcoll78',78)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390235">A reproducing kernel Hilbert space framework for pairwise time series distances</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81330494547">Zhengdong Lu</a>,
<a href="author_page.cfm?id=81100074007">Todd K. Leen</a>,
<a href="author_page.cfm?id=81539524956">Yonghong Huang</a>,
<a href="author_page.cfm?id=81100230882">Deniz Erdogmus</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 624-631</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390235" title="DOI">10.1145/1390156.1390235</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390235&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow79" style="display:inline;"><br /><div style="display:inline">A good distance measure for time series needs to properly incorporate the temporal structure, and should be applicable to sequences with unequal lengths. In this paper, we propose a distance measure as a principled solution to the two requirements. Unlike ...</div></span>
<span id="toHide79" style="display:none;"><br /><div style="display:inline"><p>A good distance measure for time series needs to properly incorporate the temporal structure, and should be applicable to sequences with unequal lengths. In this paper, we propose a distance measure as a principled solution to the two requirements. Unlike the conventional feature vector representation, our approach represents each time series with a summarizing smooth curve in a reproducing kernel Hilbert space (RKHS), and therefore translate the distance between time series into distances between curves. Moreover we propose to learn the kernel of this RKHS from a population of time series with discrete observations using Gaussian process-based non-parametric mixed-effect models. Experiments on two vastly different real-world problems show that the proposed distance measure leads to improved classification accuracy over the conventional distance measures.</p></div></span> <a id="expcoll79" href="JavaScript: expandcollapse('expcoll79',79)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390236">On-line discovery of temporal-difference networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=99659113684">Takaki Makino</a>,
<a href="author_page.cfm?id=81544791756">Toshihisa Takagi</a>
</span>
 </td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 632-639</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390236" title="DOI">10.1145/1390156.1390236</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390236&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow80" style="display:inline;"><br /><div style="display:inline">We present an algorithm for on-line, incremental discovery of temporal-difference (TD) networks. The key contribution is the establishment of three criteria to expand a node in TD network: a node is expanded when the node is well-known, independent, ...</div></span>
<span id="toHide80" style="display:none;"><br /><div style="display:inline"><p>We present an algorithm for on-line, incremental discovery of temporal-difference (TD) networks. The key contribution is the establishment of three criteria to expand a node in TD network: a node is expanded when the node is well-known, independent, and has a prediction error that requires further explanation. Since none of these criteria requires centralized calculation operations, they are easily computed in a parallel and distributed manner, and scalable for bigger problems compared to other discovery methods of predictive state representations. Through computer experiments, we demonstrate the empirical effectiveness of our algorithm.</p></div></span> <a id="expcoll80" href="JavaScript: expandcollapse('expcoll80',80)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390237">Nonextensive entropic kernels</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597553">Andr&#233; F. T. Martins</a>,
<a href="author_page.cfm?id=81365597000">M&#225;rio A. T. Figueiredo</a>,
<a href="author_page.cfm?id=81100090829">Pedro M. Q. Aguiar</a>,
<a href="author_page.cfm?id=81329492133">Noah A. Smith</a>,
<a href="author_page.cfm?id=81407592503">Eric P. Xing</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 640-647</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390237" title="DOI">10.1145/1390156.1390237</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390237&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow81" style="display:inline;"><br /><div style="display:inline">Positive definite kernels on probability measures have been recently applied in structured data classification problems. Some of these kernels are related to classic information theoretic quantities, such as mutual information and the Jensen-Shannon ...</div></span>
<span id="toHide81" style="display:none;"><br /><div style="display:inline"><p>Positive definite kernels on probability measures have been recently applied in structured data classification problems. Some of these kernels are related to classic information theoretic quantities, such as mutual information and the Jensen-Shannon divergence. Meanwhile, driven by recent advances in Tsallis statistics, nonextensive generalizations of Shannon's information theory have been proposed. This paper bridges these two trends. We introduce the <i>Jensen-Tsallis q-difference</i>, a generalization of the Jensen-Shannon divergence. We then define a new family of nonextensive mutual information kernels, which allow weights to be assigned to their arguments, and which includes the Boolean, Jensen-Shannon, and linear kernels as particular cases. We illustrate the performance of these kernels on text categorization tasks.</p></div></span> <a id="expcoll81" href="JavaScript: expandcollapse('expcoll81',81)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390238">Automatic discovery and transfer of MAXQ hierarchies</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81413601930">Neville Mehta</a>,
<a href="author_page.cfm?id=81100552505">Soumya Ray</a>,
<a href="author_page.cfm?id=81100642869">Prasad Tadepalli</a>,
<a href="author_page.cfm?id=81452609390">Thomas Dietterich</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 648-655</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390238" title="DOI">10.1145/1390156.1390238</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390238&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow82" style="display:inline;"><br /><div style="display:inline">We present an algorithm, HI-MAT (Hierarchy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by applying dynamic Bayesian network models to a successful trajectory from a source reinforcement learning task. HI-MAT discovers ...</div></span>
<span id="toHide82" style="display:none;"><br /><div style="display:inline"><p>We present an algorithm, HI-MAT (Hierarchy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by applying dynamic Bayesian network models to a successful trajectory from a source reinforcement learning task. HI-MAT discovers subtasks by analyzing the causal and temporal relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consistent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empirically that HI-MAT constructs compact hierarchies that are comparable to manually-engineered hierarchies and facilitate significant speedup in learning when transferred to a target task.</p></div></span> <a id="expcoll82" href="JavaScript: expandcollapse('expcoll82',82)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390239">Rank minimization via online learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421594508">Raghu Meka</a>,
<a href="author_page.cfm?id=81547334756">Prateek Jain</a>,
<a href="author_page.cfm?id=81314481305">Constantine Caramanis</a>,
<a href="author_page.cfm?id=81100098715">Inderjit S. Dhillon</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 656-663</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390239" title="DOI">10.1145/1390156.1390239</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390239&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow83" style="display:inline;"><br /><div style="display:inline">Minimum rank problems arise frequently in machine learning applications and are notoriously difficult to solve due to the non-convex nature of the rank objective. In this paper, we present the first online learning approach for the problem of rank minimization ...</div></span>
<span id="toHide83" style="display:none;"><br /><div style="display:inline"><p>Minimum rank problems arise frequently in machine learning applications and are notoriously difficult to solve due to the non-convex nature of the rank objective. In this paper, we present the first online learning approach for the problem of rank minimization of matrices over polyhedral sets. In particular, we present two online learning algorithms for rank minimization - our first algorithm is a multiplicative update method based on a generalized experts framework, while our second algorithm is a novel application of the online convex programming framework (Zinkevich, 2003). In the latter, we flip the role of the decision maker by making the decision maker search over the constraint space instead of feasible points, as is usually the case in online convex programming. A salient feature of our online learning approach is that it allows us to give provable approximation guarantees for the rank minimization problem over polyhedral sets. We demonstrate the effectiveness of our methods on synthetic examples, and on the real-life application of low-rank kernel learning.</p></div></span> <a id="expcoll83" href="JavaScript: expandcollapse('expcoll83',83)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390240">An analysis of reinforcement learning with function approximation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100456483">Francisco S. Melo</a>,
<a href="author_page.cfm?id=81100505560">Sean P. Meyn</a>,
<a href="author_page.cfm?id=81100429196">M. Isabel Ribeiro</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 664-671</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390240" title="DOI">10.1145/1390156.1390240</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390240&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow84" style="display:inline;"><br /><div style="display:inline">We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of Q-learning when combined with function approximation, extending the ...</div></span>
<span id="toHide84" style="display:none;"><br /><div style="display:inline"><p>We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of <i>Q</i>-learning when combined with function approximation, extending the analysis of TD-learning in (Tsitsiklis & Van Roy, 1996a) to stochastic control settings. We identify conditions under which such approximate methods converge with probability 1. We conclude with a brief discussion on the general applicability of our results and compare them with several related works.</p></div></span> <a id="expcoll84" href="JavaScript: expandcollapse('expcoll84',84)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390241">Empirical Bernstein stopping</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421600813">Volodymyr Mnih</a>,
<a href="author_page.cfm?id=81100538163">Csaba Szepesv&#225;ri</a>,
<a href="author_page.cfm?id=81309481887">Jean-Yves Audibert</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 672-679</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390241" title="DOI">10.1145/1390156.1390241</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390241&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow85" style="display:inline;"><br /><div style="display:inline">Sampling is a popular way of scaling up machine learning algorithms to large datasets. The question often is how many samples are needed. Adaptive stopping algorithms monitor the performance in an online fashion and they can stop early, saving valuable ...</div></span>
<span id="toHide85" style="display:none;"><br /><div style="display:inline"><p>Sampling is a popular way of scaling up machine learning algorithms to large datasets. The question often is how many samples are needed. Adaptive stopping algorithms monitor the performance in an online fashion and they can stop early, saving valuable resources. We consider problems where probabilistic guarantees are desired and demonstrate how recently-introduced empirical Bernstein bounds can be used to design stopping rules that are efficient. We provide upper bounds on the sample complexity of the new rules, as well as empirical results on model selection and boosting in the filtering setting.</p></div></span> <a id="expcoll85" href="JavaScript: expandcollapse('expcoll85',85)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390242">Efficiently solving convex relaxations for MAP estimation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100380716">M. Pawan Kumar</a>,
<a href="author_page.cfm?id=81100100032">P. H. S. Torr</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 680-687</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390242" title="DOI">10.1145/1390156.1390242</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390242&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow86" style="display:inline;"><br /><div style="display:inline">The problem of obtaining the maximum a posteriori (MAP) estimate of a discrete random field is of fundamental importance in many areas of Computer Science. In this work, we build on the tree reweighted message passing (TRW) framework of (Kolmogorov, ...</div></span>
<span id="toHide86" style="display:none;"><br /><div style="display:inline"><p>The problem of obtaining the maximum <i>a posteriori</i> (MAP) estimate of a discrete random field is of fundamental importance in many areas of Computer Science. In this work, we build on the tree reweighted message passing (TRW) framework of (Kolmogorov, 2006; Wainwright et al., 2005). TRW iteratively optimizes the Lagrangian dual of a linear programming relaxation for MAP estimation. We show how the dual formulation of TRW can be extended to include cycle inequalities (Barahona & Mahjoub, 1986) and some recently proposed second order cone (SOC) constraints (Kumar et al., 2007). We propose efficient iterative algorithms for solving the resulting duals. Similar to the method described in (Kolmogorov, 2006), these algorithms are guaranteed to converge. We test our approach on a large set of synthetic data, as well as real data. Our experiments show that the additional constraints (i.e. cycle inequalities and SOC constraints) provide better results in cases where the TRW framework fails (namely MAP estimation for non-submodular energy functions).</p></div></span> <a id="expcoll86" href="JavaScript: expandcollapse('expcoll86',86)">expand</a>
 </div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390243">On the hardness of finding symmetries in Markov decision processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421602185">Shravan Matthur Narayanamurthy</a>,
<a href="author_page.cfm?id=81100369344">Balaraman Ravindran</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 688-695</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390243" title="DOI">10.1145/1390156.1390243</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390243&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow87" style="display:inline;"><br /><div style="display:inline">In this work we address the question of finding symmetries of a given MDP. We show that the problem is Isomorphism Complete, that is, the problem is polynomially equivalent to verifying whether two graphs are isomorphic. Apart from the theoretical ...</div></span>
<span id="toHide87" style="display:none;"><br /><div style="display:inline"><p>In this work we address the question of finding symmetries of a given MDP. We show that the problem is <i>Isomorphism Complete</i>, that is, the problem is polynomially equivalent to verifying whether two graphs are isomorphic. Apart from the theoretical importance of this result it has an important practical application. The reduction presented can be used together with any off-the-shelf Graph Isomorphism solver, which performs well in the average case, to find symmetries of an MDP. In fact, we present results of using NAutY (the best Graph Isomorphism solver currently available), to find symmetries of MDPs.</p></div></span> <a id="expcoll87" href="JavaScript: expandcollapse('expcoll87',87)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390244">Bayes optimal classification for decision trees</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100162750">Siegfried Nijssen</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 696-703</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390244" title="DOI">10.1145/1390156.1390244</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390244&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr> 
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow88" style="display:inline;"><br /><div style="display:inline">We present an algorithm for exact Bayes optimal classification from a hypothesis space of decision trees satisfying leaf constraints. Our contribution is that we reduce this classification problem to the problem of finding a rule-based classifier with ...</div></span>
<span id="toHide88" style="display:none;"><br /><div style="display:inline"><p>We present an algorithm for exact Bayes optimal classification from a hypothesis space of decision trees satisfying leaf constraints. Our contribution is that we reduce this classification problem to the problem of finding a rule-based classifier with appropriate weights. We show that these rules and weights can be computed in linear time from the output of a modified frequent itemset mining algorithm, which means that we can compute the classifier in practice, despite the exponential worst-case complexity. In experiments we compare the Bayes optimal predictions with those of the maximum a posteriori hypothesis.</p></div></span> <a id="expcoll88" href="JavaScript: expandcollapse('expcoll88',88)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390245">A decoupled approach to exemplar-based unsupervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81414597536">Sebastian Nowozin</a>,
<a href="author_page.cfm?id=81339489188">G&#246;khan Bakir</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 704-711</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390245" title="DOI">10.1145/1390156.1390245</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390245&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow89" style="display:inline;"><br /><div style="display:inline">A recent trend in exemplar based unsupervised learning is to formulate the learning problem as a convex optimization problem. Convexity is achieved by restricting the set of possible prototypes to training exemplars. In particular, this has been done ...</div></span>
<span id="toHide89" style="display:none;"><br /><div style="display:inline"><p>A recent trend in exemplar based unsupervised learning is to formulate the learning problem as a convex optimization problem. Convexity is achieved by restricting the set of possible prototypes to training exemplars. In particular, this has been done for clustering, vector quantization and mixture model density estimation. In this paper we propose a novel algorithm that is theoretically and practically superior to these convex formulations. This is possible by posing the unsupervised learning problem as a single convex "master problem" with non-convex subproblems. We show that for the above learning tasks the subproblems are extremely well-behaved and can be solved efficiently.</p></div></span> <a id="expcoll89" href="JavaScript: expandcollapse('expcoll89',89)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390246">Cost-sensitive multi-class classification from probability estimates</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100311375">Deirdre B. O'Brien</a>,
<a href="author_page.cfm?id=81100021030">Maya R. Gupta</a>,
<a href="author_page.cfm?id=81406599203">Robert M. Gray</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 712-719</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390246" title="DOI">10.1145/1390156.1390246</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390246&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow90" style="display:inline;"><br /><div style="display:inline">For two-class classification, it is common to classify by setting a threshold on class probability estimates, where the threshold is determined by ROC curve analysis. An analog for multi-class classification is learning a new class partitioning of the ...</div></span>
<span id="toHide90" style="display:none;"><br /><div style="display:inline"><p>For two-class classification, it is common to classify by setting a threshold on class probability estimates, where the threshold is determined by ROC curve analysis. An analog for multi-class classification is learning a new class partitioning of the multiclass probability simplex to minimize empirical misclassification costs. We analyze the interplay between systematic errors in the class probability estimates and cost matrices for multiclass classification. We explore the effect on the class partitioning of five different transformations of the cost matrix. Experiments on benchmark datasets with naive Bayes and quadratic discriminant analysis show the effectiveness of learning a new partition matrix compared to previously proposed methods.</p></div></span> <a id="expcoll90" href="JavaScript: expandcollapse('expcoll90',90)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390247">The projectron: a bounded kernel-based Perceptron</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309504699">Francesco Orabona</a>,
<a href="author_page.cfm?id=81100048613">Joseph Keshet</a>,
<a href="author_page.cfm?id=81100009515">Barbara Caputo</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 720-727</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390247" title="DOI">10.1145/1390156.1390247</a></span></td>
</tr>
<tr>
 <td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390247&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow91" style="display:inline;"><br /><div style="display:inline">We present a discriminative online algorithm with a bounded memory growth, which is based on the kernel-based Perceptron. Generally, the required memory of the kernel-based Perceptron for storing the online hypothesis is not bounded. Previous work has ...</div></span>
<span id="toHide91" style="display:none;"><br /><div style="display:inline"><p>We present a discriminative online algorithm with a bounded memory growth, which is based on the kernel-based Perceptron. Generally, the required memory of the kernel-based Perceptron for storing the online hypothesis is not bounded. Previous work has been focused on discarding part of the instances in order to keep the memory bounded. In the proposed algorithm the instances are not discarded, but projected onto the space spanned by the previous online hypothesis. We derive a relative mistake bound and compare our algorithm both analytically and empirically to the state-of-the-art Forgetron algorithm (Dekel et al, 2007). The first variant of our algorithm, called Projectron, outperforms the Forgetron. The second variant, called Projectron++, outperforms even the Perceptron.</p></div></span> <a id="expcoll91" href="JavaScript: expandcollapse('expcoll91',91)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390248">Learning dissimilarities by ranking: from SDP to QP</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317498653">Hua Ouyang</a>,
<a href="author_page.cfm?id=81100401442">Alex Gray</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 728-735</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390248" title="DOI">10.1145/1390156.1390248</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390248&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow92" style="display:inline;"><br /><div style="display:inline">We consider the problem of learning dissimilarities between points via formulations which preserve a specified ordering between points rather than the numerical values of the dissimilarities. Dissimilarity ranking (d-ranking) learns from instances ...</div></span>
<span id="toHide92" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of learning dissimilarities between points via formulations which preserve a specified ordering between points rather than the numerical values of the dissimilarities. <i>Dissimilarity ranking (d-ranking)</i> learns from instances like "A is more similar to B than C is to D" or "The distance between E and F is larger than that between G and H". Three formulations of d-ranking problems are presented and new algorithms are presented for two of them, one by semidefinite programming (SDP) and one by quadratic programming (QP). Among the novel capabilities of these approaches are out-of-sample prediction and scalability to large problems.</p></div></span> <a id="expcoll92" href="JavaScript: expandcollapse('expcoll92',92)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390249">A distance model for rhythms</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309509945">Jean-Fran&#231;ois Paiement</a>,
<a href="author_page.cfm?id=81100345039">Yves Grandvalet</a>,
<a href="author_page.cfm?id=81100287032">Samy Bengio</a>,
<a href="author_page.cfm?id=81100357666">Douglas Eck</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 736-743</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390249" title="DOI">10.1145/1390156.1390249</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390249&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow93" style="display:inline;"><br /><div style="display:inline">Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper, we introduce a model for rhythms based on the distributions ...</div></span>
<span id="toHide93" style="display:none;"><br /><div style="display:inline"><p>Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper, we introduce a model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases.</p></div></span> <a id="expcoll93" href="JavaScript: expandcollapse('expcoll93',93)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390250">On the chance accuracies of large collections of classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81322502826">Mark Palatucci</a>,
<a href="author_page.cfm?id=81100379171">Andrew Carlson</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 744-751</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390250" title="DOI">10.1145/1390156.1390250</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390250&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow94" style="display:inline;"><br /><div style="display:inline">We provide a theoretical analysis of the chance accuracies of large collections of classifiers. We show that on problems with small numbers of examples, some classifier can perform well by random chance, and we derive a theorem to explicitly calculate ...</div></span>
<span id="toHide94" style="display:none;"><br /><div style="display:inline"><p>We provide a theoretical analysis of the chance accuracies of large collections of classifiers. We show that on problems with small numbers of examples, some classifier can perform well by random chance, and we derive a theorem to explicitly calculate this accuracy. We use this theorem to provide a principled feature selection criterion for sparse, high-dimensional problems. We evaluate this method on microarray and fMRI datasets and show that it performs very close to the optimal accuracy obtained from an oracle. We also show that on the fMRI dataset this technique chooses relevant features successfully while another state-of-the-art method, the False Discovery Rate (FDR), completely fails at standard significance levels.</p></div></span> <a id="expcoll94" href="JavaScript: expandcollapse('expcoll94',94)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390251">An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100331696">Ronald Parr</a>,
<a href="author_page.cfm?id=81331497976">Lihong Li</a>,
<a href="author_page.cfm?id=81421600296">Gavin Taylor</a>,
<a href="author_page.cfm?id=81333490370">Christopher Painter-Wakefield</a>,
<a href="author_page.cfm?id=81406601119">Michael L. Littman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 752-759</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390251" title="DOI">10.1145/1390156.1390251</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390251&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow95" style="display:inline;"><br /><div style="display:inline">We show that linear value-function approximation is equivalent to a form of linear model approximation. We then derive a relationship between the model-approximation error and the Bellman error, and show how this relationship can guide feature selection ...</div></span>
<span id="toHide95" style="display:none;"><br /><div style="display:inline"><p>We show that linear value-function approximation is equivalent to a form of linear model approximation. We then derive a relationship between the model-approximation error and the Bellman error, and show how this relationship can guide feature selection for model improvement and/or value-function improvement. We also show how these results give insight into the behavior of existing feature-selection algorithms.</p></div></span> <a id="expcoll95" href="JavaScript: expandcollapse('expcoll95',95)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390252">Learning to learn implicit queries from gaze patterns</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317489564">Kai Puolam&#228;ki</a>,
<a href="author_page.cfm?id=81421597013">Antti Ajanki</a>,
<a href="author_page.cfm?id=81100348810">Samuel Kaski</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 760-767</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390252" title="DOI">10.1145/1390156.1390252</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390252&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow96" style="display:inline;"><br /><div style="display:inline">In the absence of explicit queries, an alternative is to try to infer users' interests from implicit feedback signals, such as clickstreams or eye tracking. The interests, formulated as an implicit query, can then be used in further searches. We formulate ...</div></span>
<span id="toHide96" style="display:none;"><br /><div style="display:inline"><p>In the absence of explicit queries, an alternative is to try to infer users' interests from implicit feedback signals, such as clickstreams or eye tracking. The interests, formulated as an implicit query, can then be used in further searches. We formulate this task as a probabilistic model, which can be interpreted as a kind of transfer or meta-learning. The probabilistic model is demonstrated to outperform an earlier kernel-based method in a small-scale information retrieval task.</p></div></span> <a id="expcoll96" href="JavaScript: expandcollapse('expcoll96',96)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390253">Multi-task compressive sensing with Dirichlet process priors</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597445">Yuting Qi</a>,
<a href="author_page.cfm?id=81414616913">Dehong Liu</a>,
<a href="author_page.cfm?id=81333488093">David Dunson</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 768-775</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390253" title="DOI">10.1145/1390156.1390253</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390253&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow97" style="display:inline;"><br /><div style="display:inline">Compressive sensing (CS) is an emerging &pound;eld that, under appropriate conditions, can signi&pound;cantly reduce the number of measurements required for a given signal. In many applications, one is interested in multiple signals that may be measured ...</div></span>
<span id="toHide97" style="display:none;"><br /><div style="display:inline"><p>Compressive sensing (CS) is an emerging &pound;eld that, under appropriate conditions, can signi&pound;cantly reduce the number of measurements required for a given signal. In many applications, one is interested in multiple signals that may be measured in multiple CS-type measurements, where here each signal corresponds to a sensing "task". In this paper we propose a novel multitask compressive sensing framework based on a Bayesian formalism, where a Dirichlet process (DP) prior is employed, yielding a principled means of simultaneously inferring the appropriate sharing mechanisms as well as CS inversion for each task. A variational Bayesian (VB) inference algorithm is employed to estimate the full posterior on the model parameters.</p></div></span> <a id="expcoll97" href="JavaScript: expandcollapse('expcoll97',97)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390254">Estimating labels from label proportions</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597256">Novi Quadrianto</a>,
<a href="author_page.cfm?id=81100243402">Alex J. Smola</a>,
<a href="author_page.cfm?id=81100060357">Tiberio S. Caetano</a>,
<a href="author_page.cfm?id=81339511241">Quoc V. Le</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 776-783</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390254" title="DOI">10.1145/1390156.1390254</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390254&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>

<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow98" style="display:inline;"><br /><div style="display:inline">Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, also with known label proportions. This problem appears in areas like e-commerce, spam filtering ...</div></span>
<span id="toHide98" style="display:none;"><br /><div style="display:inline"><p>Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, also with known label proportions. This problem appears in areas like e-commerce, spam filtering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice.</p></div></span> <a id="expcoll98" href="JavaScript: expandcollapse('expcoll98',98)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390255">Learning diverse rankings with multi-armed bandits</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100347280">Filip Radlinski</a>,
<a href="author_page.cfm?id=81100287473">Robert Kleinberg</a>,
<a href="author_page.cfm?id=81100184551">Thorsten Joachims</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 784-791</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390255" title="DOI">10.1145/1390156.1390255</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390255&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow99" style="display:inline;"><br /><div style="display:inline">Algorithms for learning to rank Web documents usually assume a document's relevance is independent of other documents. This leads to learned ranking functions that produce rankings with redundant results. In contrast, user studies have shown that diversity ...</div></span>
<span id="toHide99" style="display:none;"><br /><div style="display:inline"><p>Algorithms for learning to rank Web documents usually assume a document's relevance is independent of other documents. This leads to learned ranking functions that produce rankings with redundant results. In contrast, user studies have shown that diversity at high ranks is often preferred. We present two online learning algorithms that directly learn a diverse ranking of documents based on users' clicking behavior. We show that these algorithms minimize abandonment, or alternatively, maximize the probability that a relevant document is found in the top <i>k</i> positions of a ranking. Moreover, one of our algorithms asymptotically achieves optimal worst-case performance even if users' interests change.</p></div></span> <a id="expcoll99" href="JavaScript: expandcollapse('expcoll99',99)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390256">Semi-supervised learning of compact document representations with deep networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323495323">Marc' Aurelio Ranzato</a>,
<a href="author_page.cfm?id=81335498196">Martin Szummer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 792-799</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390256" title="DOI">10.1145/1390156.1390256</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390256&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow100" style="display:inline;"><br /><div style="display:inline">Finding good representations of text documents is crucial in information retrieval and classification systems. Today the most popular document representation is based on a vector of word counts in the document. This representation neither captures dependencies ...</div></span>
<span id="toHide100" style="display:none;"><br /><div style="display:inline"><p>Finding good representations of text documents is crucial in information retrieval and classification systems. Today the most popular document representation is based on a vector of word counts in the document. This representation neither captures dependencies between related words, nor handles synonyms or polysemous words. In this paper, we propose an algorithm to learn text document representations based on semi-supervised autoencoders that are stacked to form a deep network. The model can be trained efficiently on partially labeled corpora, producing very compact representations of documents, while retaining as much class information and joint word statistics as possible. We show that it is advantageous to exploit even a few labeled samples during training.</p></div></span> <a id="expcoll100" href="JavaScript: expandcollapse('expcoll100',100)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390257">Message-passing for graph-structured linear programs: proximal projections, convergence and rounding schemes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100169005">Pradeep Ravikumar</a>,
<a href="author_page.cfm?id=81421598579">Alekh Agarwal</a>,
<a href="author_page.cfm?id=81100180103">Martin J. Wainwright</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 800-807</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390257" title="DOI">10.1145/1390156.1390257</a></span></td>
</tr>
<tr>
<td></td>
 <td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390257&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow101" style="display:inline;"><br /><div style="display:inline">A large body of past work has focused on the first-order tree-based LP relaxation for the MAP problem in Markov random fields. This paper develops a family of super-linearly convergent LP solvers based on proximal minimization schemes using Bregman divergences ...</div></span>
<span id="toHide101" style="display:none;"><br /><div style="display:inline"><p>A large body of past work has focused on the first-order tree-based LP relaxation for the MAP problem in Markov random fields. This paper develops a family of super-linearly convergent LP solvers based on proximal minimization schemes using Bregman divergences that exploit the underlying graphical structure, and so scale well to large problems. All of our algorithms have a double-loop character, with the outer loop corresponding to the proximal sequence, and an inner loop of cyclic Bregman divergences used to compute each proximal update. The inner loop updates are distributed and respect the graph structure, and thus can be cast as message-passing algorithms. We establish various convergence guarantees for our algorithms, illustrate their performance, and also present rounding schemes with provable optimality guarantees.</p></div></span> <a id="expcoll101" href="JavaScript: expandcollapse('expcoll101',101)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390258">Bayesian multiple instance learning: automatic feature selection and inductive transfer</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100214434">Vikas C. Raykar</a>,
<a href="author_page.cfm?id=81100282447">Balaji Krishnapuram</a>,
<a href="author_page.cfm?id=81100131666">Jinbo Bi</a>,
<a href="author_page.cfm?id=81344490105">Murat Dundar</a>,
<a href="author_page.cfm?id=81100591904">R. Bharat Rao</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 808-815</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390258" title="DOI">10.1145/1390156.1390258</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390258&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow102" style="display:inline;"><br /><div style="display:inline">We propose a novel Bayesian multiple instance learning (MIL) algorithm. This algorithm automatically identifies the relevant feature subset, and utilizes inductive transfer when learning multiple (conceptually related) classifiers. Experimental results ...</div></span>
<span id="toHide102" style="display:none;"><br /><div style="display:inline"><p>We propose a novel Bayesian multiple instance learning (MIL) algorithm. This algorithm automatically identifies the relevant feature subset, and utilizes inductive transfer when learning multiple (conceptually related) classifiers. Experimental results indicate that the proposed MIL method is more accurate than previous MIL algorithms and selects a much smaller set of useful features. Inductive transfer further improves the accuracy of the classifier as compared to learning each task individually.</p></div></span> <a id="expcoll102" href="JavaScript: expandcollapse('expcoll102',102)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390259">Online kernel selection for Bayesian reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309485175">Joseph Reisinger</a>,
<a href="author_page.cfm?id=81100388010">Peter Stone</a>,
<a href="author_page.cfm?id=81452617472">Risto Miikkulainen</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 816-823</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390259" title="DOI">10.1145/1390156.1390259</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390259&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow103" style="display:inline;"><br /><div style="display:inline">Kernel-based Bayesian methods for Reinforcement Learning (RL) such as Gaussian Process Temporal Difference (GPTD) are particularly promising because they rigorously treat uncertainty in the value function and make it easy to specify prior knowledge. ...</div></span>
<span id="toHide103" style="display:none;"><br /><div style="display:inline"><p>Kernel-based Bayesian methods for Reinforcement Learning (RL) such as Gaussian Process Temporal Difference (GPTD) are particularly promising because they rigorously treat uncertainty in the value function and make it easy to specify prior knowledge. However, the choice of prior distribution significantly affects the empirical performance of the learning agent, and little work has been done extending existing methods for prior model selection to the online setting. This paper develops Replacing-Kernel RL, an online model selection method for GPTD using sequential Monte-Carlo methods. Replacing-Kernel RL is compared to standard GPTD and tile-coding on several RL domains, and is shown to yield significantly better asymptotic performance for many different kernel families. Furthermore, the resulting kernels capture an intuitively useful notion of prior state covariance that may nevertheless be difficult to capture manually.</p></div></span> <a id="expcoll103" href="JavaScript: expandcollapse('expcoll103',103)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390260">The dynamic hierarchical Dirichlet process</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421595011">Lu Ren</a>,
<a href="author_page.cfm?id=81333488093">David B. Dunson</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 824-831</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390260" title="DOI">10.1145/1390156.1390260</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390260&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow104" style="display:inline;"><br /><div style="display:inline">The dynamic hierarchical Dirichlet process (dHDP) is developed to model the time-evolving statistical properties of sequential data sets. The data collected at any time point are represented via a mixture associated with an appropriate underlying model, ...</div></span>
<span id="toHide104" style="display:none;"><br /><div style="display:inline"><p>The dynamic hierarchical Dirichlet process (dHDP) is developed to model the time-evolving statistical properties of sequential data sets. The data collected at any time point are represented via a mixture associated with an appropriate underlying model, in the framework of HDP. The statistical properties of data collected at consecutive time points are linked via a random parameter that controls their probabilistic similarity. The sharing mechanisms of the time-evolving data are derived, and a relatively simple Markov Chain Monte Carlo sampler is developed. Experimental results are presented to demonstrate the model.</p></div></span> <a id="expcoll104" href="JavaScript: expandcollapse('expcoll104',104)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390261">Closed-form supervised dimensionality reduction with generalized linear models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100576972">Irina Rish</a>,
<a href="author_page.cfm?id=81100521961">Genady Grabarnik</a>,
<a href="author_page.cfm?id=81365594222">Guillermo Cecchi</a>,
<a href="author_page.cfm?id=81452615698">Francisco Pereira</a>,
<a href="author_page.cfm?id=81100037343">Geoffrey J. Gordon</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 832-839</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390261" title="DOI">10.1145/1390156.1390261</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390261&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow105" style="display:inline;"><br /><div style="display:inline">We propose a family of supervised dimensionality reduction (SDR) algorithms that combine feature extraction (dimensionality reduction) with learning a predictive model in a unified optimization framework, using data- and class-appropriate generalized ...</div></span>
<span id="toHide105" style="display:none;"><br /><div style="display:inline"><p>We propose a family of supervised dimensionality reduction (SDR) algorithms that combine feature extraction (dimensionality reduction) with learning a predictive model in a unified optimization framework, using data- and class-appropriate generalized linear models (GLMs), and handling both classification and regression problems. Our approach uses simple closed-form update rules and is provably convergent. Promising empirical results are demonstrated on a variety of high-dimensional datasets.</p></div></span> <a id="expcoll105" href="JavaScript: expandcollapse('expcoll105',105)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390262">Bi-level path following for cross validated solution of kernel quantile regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100549576">Saharon Rosset</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 840-847</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390262" title="DOI">10.1145/1390156.1390262</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390262&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow106" style="display:inline;"><br /><div style="display:inline">Modeling of conditional quantiles requires specification of the quantile being estimated and can thus be viewed as a parameterized predictive modeling problem. Quantile loss is typically used, and it is indeed parameterized by a quantile parameter. In ...</div></span>
<span id="toHide106" style="display:none;"><br /><div style="display:inline"><p>Modeling of conditional quantiles requires specification of the quantile being estimated and can thus be viewed as a parameterized predictive modeling problem. Quantile loss is typically used, and it is indeed parameterized by a quantile parameter. In this paper we show how to follow the path of cross validated solutions to regularized kernel quantile regression. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on data. This algorithm allows us to efficiently solve the whole family of bi-level problems.</p></div></span> <a id="expcoll106" href="JavaScript: expandcollapse('expcoll106',106)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390263">The Group-Lasso for generalized linear models: uniqueness of solutions and efficient algorithms</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81481652115">Volker Roth</a>,
<a href="author_page.cfm?id=81100312407">Bernd Fischer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 848-855</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390263" title="DOI">10.1145/1390156.1390263</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390263&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow107" style="display:inline;"><br /><div style="display:inline">The Group-Lasso method for finding important explanatory factors suffers from the potential non-uniqueness of solutions and also from high computational costs. We formulate conditions for the uniqueness of Group-Lasso solutions which lead to an easily ...</div></span>
<span id="toHide107" style="display:none;"><br /><div style="display:inline"><p>The Group-Lasso method for finding important explanatory factors suffers from the potential non-uniqueness of solutions and also from high computational costs. We formulate conditions for the uniqueness of Group-Lasso solutions which lead to an easily implementable test procedure that allows us to identify all potentially active groups. These results are used to derive an efficient algorithm that can deal with input dimensions in the millions and can approximate the solution path efficiently. The derived methods are applied to large-scale learning problems where they exhibit excellent performance and where the testing procedure helps to avoid misinterpretations of the solutions.</p></div></span> <a id="expcoll107" href="JavaScript: expandcollapse('expcoll107',107)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390264">Robust matching and recognition using context-dependent kernels</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100535233">Hichem Sahbi</a>,
<a href="author_page.cfm?id=81309481887">Jean-Yves Audibert</a>,
<a href="author_page.cfm?id=81421592858">Jaonary Rabarisoa</a>,
<a href="author_page.cfm?id=81330493116">Renaud Keriven</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 856-863</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390264" title="DOI">10.1145/1390156.1390264</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390264&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow108" style="display:inline;"><br /><div style="display:inline">The success of kernel methods including support vector machines (SVMs) strongly depends on the design of appropriate kernels. While initially kernels were designed in order to handle fixed-length data, their extension to unordered, variable-length data ...</div></span>
<span id="toHide108" style="display:none;"><br /><div style="display:inline"><p>The success of kernel methods including support vector machines (SVMs) strongly depends on the design of appropriate kernels. While initially kernels were designed in order to handle fixed-length data, their extension to unordered, variable-length data became more than necessary for real pattern recognition problems such as object recognition and bioinformatics.</p> <p>We focus in this paper on object recognition using a new type of kernel referred to as "context-dependent". Objects, seen as constellations of local features (interest points, regions, etc.), are matched by minimizing an energy function mixing (1) a fidelity term which measures the quality of feature matching, (2) a neighborhood criterion which captures the object geometry and (3) a regularization term. We will show that the fixed-point of this energy is a "context-dependent" kernel ("CDK") which also satisfies the Mercer condition. Experiments conducted on object recognition show that when plugging our kernel in SVMs, we clearly outperform SVMs with "context-free" kernels.</p></div></span> <a id="expcoll108" href="JavaScript: expandcollapse('expcoll108',108)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390265">Privacy-preserving reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100536439">Jun Sakuma</a>,
<a href="author_page.cfm?id=81100604004">Shigenobu Kobayashi</a>,
<a href="author_page.cfm?id=81100659919">Rebecca N. Wright</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 864-871</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390265" title="DOI">10.1145/1390156.1390265</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390265&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow109" style="display:inline;"><br /><div style="display:inline">We consider the problem of distributed reinforcement learning (DRL) from private perceptions. In our setting, agents' perceptions, such as states, rewards, and actions, are not only distributed but also should be kept private. Conventional DRL algorithms ...</div></span>
<span id="toHide109" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of distributed reinforcement learning (DRL) from private perceptions. In our setting, agents' perceptions, such as states, rewards, and actions, are not only distributed but also should be kept private. Conventional DRL algorithms can handle multiple agents, but do not necessarily guarantee privacy preservation and may not guarantee optimality. In this work, we design cryptographic solutions that achieve optimal policies without requiring the agents to share their private information.</p></div></span> <a id="expcoll109" href="JavaScript: expandcollapse('expcoll109',109)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390266">On the quantitative analysis of deep belief networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100562217">Ruslan Salakhutdinov</a>,
<a href="author_page.cfm?id=87058607057">Iain Murray</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 872-879</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390266" title="DOI">10.1145/1390156.1390266</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390266&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow110" style="display:inline;"><br /><div style="display:inline">Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The ...</div></span>
<span id="toHide110" style="display:none;"><br /><div style="display:inline"><p>Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the <i>test data</i>. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.</p></div></span> <a id="expcoll110" href="JavaScript: expandcollapse('expcoll110',110)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390267">Bayesian probabilistic matrix factorization using Markov chain Monte Carlo</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100562217">Ruslan Salakhutdinov</a>,
<a href="author_page.cfm?id=81333489759">Andriy Mnih</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 880-887</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390267" title="DOI">10.1145/1390156.1390267</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390267&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow111" style="display:inline;"><br /><div style="display:inline">Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently ...</div></span>
<span id="toHide111" style="display:none;"><br /><div style="display:inline"><p>Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction accuracy than PMF models trained using MAP estimation.</p></div></span> <a id="expcoll111" href="JavaScript: expandcollapse('expcoll111',111)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390268">Accurate max-margin training for structured output spaces</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100043079">Sunita Sarawagi</a>,
<a href="author_page.cfm?id=81337489552">Rahul Gupta</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 888-895</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390268" title="DOI">10.1145/1390156.1390268</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390268&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow112" style="display:inline;"><br /><div style="display:inline">Tsochantaridis et al. (2005) proposed two formulations for maximum margin training of structured spaces: margin scaling and slack scaling. While margin scaling has been extensively used since it requires the same kind of MAP inference as normal structured ...</div></span>
<span id="toHide112" style="display:none;"><br /><div style="display:inline"><p>Tsochantaridis et al. (2005) proposed two formulations for maximum margin training of structured spaces: margin scaling and slack scaling. While margin scaling has been extensively used since it requires the same kind of MAP inference as normal structured prediction, slack scaling is believed to be more accurate and better-behaved. We present an efficient variational approximation to the slack scaling method that solves its inference bottleneck while retaining its accuracy advantage over margin scaling.</p> <p>We further argue that existing scaling approaches do not separate the true labeling comprehensively while generating violating constraints. We propose a new max-margin trainer PosLearn that generates violators to ensure separation at each position of a decomposable loss function. Empirical results on real datasets illustrate that PosLearn can reduce test error by up to 25% over margin scaling and 10% over slack scaling. Further, PosLearn violators can be generated more efficiently than slack violators; for many structured tasks the time required is just twice that of MAP inference.</p></div></span> <a id="expcoll112" href="JavaScript: expandcollapse('expcoll112',112)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390269">Fast incremental proximity search in large graphs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81310485631">Purnamrita Sarkar</a>,
<a href="author_page.cfm?id=81100042782">Andrew W. Moore</a>,
<a href="author_page.cfm?id=81545853556">Amit Prakash</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 896-903</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390269" title="DOI">10.1145/1390156.1390269</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390269&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow113" style="display:inline;"><br /><div style="display:inline">In this paper we investigate two aspects of ranking problems on large graphs. First, we augment the deterministic pruning algorithm in Sarkar and Moore (2007) with sampling techniques to compute approximately correct rankings with high probability under ...</div></span>
<span id="toHide113" style="display:none;"><br /><div style="display:inline"><p>In this paper we investigate two aspects of ranking problems on large graphs. First, we augment the deterministic pruning algorithm in Sarkar and Moore (2007) with sampling techniques to compute approximately correct rankings with high probability under random walk based proximity measures <i>at query time</i>. Second, we prove some surprising locality properties of these proximity measures by examining the short term behavior of random walks. The proposed algorithm can answer queries <i>on the fly without caching any information about the entire graph</i>. We present empirical results on a 600, 000 node author-word-citation graph from the Citeseer domain on a single CPU machine where the average query processing time is around 4 seconds. We present quantifiable link prediction tasks. On most of them our techniques outperform Personalized Pagerank, a well-known diffusion based proximity measure.</p></div></span> <a id="expcoll113" href="JavaScript: expandcollapse('expcoll113',113)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390270">Inverting the Viterbi algorithm: an abstract framework for structure design</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421592671">Michael Schnall-Levin</a>,
<a href="author_page.cfm?id=81421598938">Leonid Chindelevitch</a>,
<a href="author_page.cfm?id=81100228742">Bonnie Berger</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 904-911</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390270" title="DOI">10.1145/1390156.1390270</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390270&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow114" style="display:inline;"><br /><div style="display:inline">Probabilistic grammatical formalisms such as hidden Markov models (HMMs) and stochastic context-free grammars (SCFGs) have been extensively studied and widely applied in a number of fields. Here, we introduce a new algorithmic problem on HMMs and SCFGs ...</div></span>
<span id="toHide114" style="display:none;"><br /><div style="display:inline"><p>Probabilistic grammatical formalisms such as hidden Markov models (HMMs) and stochastic context-free grammars (SCFGs) have been extensively studied and widely applied in a number of fields. Here, we introduce a new algorithmic problem on HMMs and SCFGs that arises naturally from protein and RNA design, and which has not been previously studied. The problem can be viewed as an inverse to the one solved by the Viterbi algorithm on HMMs or by the CKY algorithm on SCFGs. We study this problem theoretically and obtain the first algorithmic results. We prove that the problem is NP-complete, even for a 3-letter emission alphabet, via a reduction from 3-SAT, a result that has implications for the hardness of RNA secondary structure design. We then develop a number of approaches for making the problem tractable. In particular, for HMMs we develop a branch-and-bound algorithm, which can be shown to have fixed-parameter tractable worst-case running time, exponential in the number of states of the HMM but linear in the length of the structure. We also show how to cast the problem as a Mixed Integer Linear Program.</p></div></span> <a id="expcoll114" href="JavaScript: expandcollapse('expcoll114',114)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390271">Compressed sensing and Bayesian experimental design</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100511719">Matthias W. Seeger</a>,
<a href="author_page.cfm?id=81421600271">Hannes Nickisch</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 912-919</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390271" title="DOI">10.1145/1390156.1390271</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390271&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow115" style="display:inline;"><br /><div style="display:inline">We relate compressed sensing (CS) with Bayesian experimental design and provide a novel efficient approximate method for the latter, based on expectation propagation. In a large comparative study about linearly measuring natural images, we show that ...</div></span>
<span id="toHide115" style="display:none;"><br /><div style="display:inline"><p>We relate compressed sensing (CS) with Bayesian experimental design and provide a novel efficient approximate method for the latter, based on expectation propagation. In a large comparative study about linearly measuring natural images, we show that the simple standard heuristic of measuring wavelet coefficients top-down systematically outperforms CS methods using random measurements; the sequential projection optimisation approach of (Ji & Carin, 2007) performs even worse. We also show that our own approximate Bayesian method is able to learn measurement filters on full images efficiently which outperform the wavelet heuristic. To our knowledge, ours is the first successful attempt at "learning compressed sensing" for images of realistic size. In contrast to common CS methods, our framework is not restricted to sparse signals, but can readily be applied to other notions of signal complexity or noise models. We give concrete ideas how our method can be scaled up to large signal representations.</p></div></span> <a id="expcoll115" href="JavaScript: expandcollapse('expcoll115',115)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390272">Multi-classification by categorical features via clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365596832">Yevgeny Seldin</a>,
<a href="author_page.cfm?id=81100328345">Naftali Tishby</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 920-927</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390272" title="DOI">10.1145/1390156.1390272</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390272&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow116" style="display:inline;"><br /><div style="display:inline">We derive a generalization bound for multi-classification schemes based on grid clustering in categorical parameter product spaces. Grid clustering partitions the parameter space in the form of a Cartesian product of partitions for each of the parameters. ...</div></span>
<span id="toHide116" style="display:none;"><br /><div style="display:inline"><p>We derive a generalization bound for multi-classification schemes based on grid clustering in categorical parameter product spaces. Grid clustering partitions the parameter space in the form of a Cartesian product of partitions for each of the parameters. The derived bound provides a means to evaluate clustering solutions in terms of the generalization power of a built-on classifier. For classification based on a single feature the bound serves to find a globally optimal classification rule. Comparison of the generalization power of individual features can then be used for feature ranking. Our experiments show that in this role the bound is much more precise than mutual information or normalized correlation indices.</p></div></span> <a id="expcoll116" href="JavaScript: expandcollapse('expcoll116',116)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390273">SVM optimization: inverse dependence on training set size</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100489319">Shai Shalev-Shwartz</a>,
<a href="author_page.cfm?id=81100005542">Nathan Srebro</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 928-935</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390273" title="DOI">10.1145/1390156.1390273</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390273&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow117" style="display:inline;"><br /><div style="display:inline">We discuss how the runtime of SVM optimization should decrease as the size of the training data increases. We present theoretical and empirical results demonstrating how a simple subgradient descent approach indeed displays such behavior, at least ...</div></span>
<span id="toHide117" style="display:none;"><br /><div style="display:inline"><p>We discuss how the runtime of SVM optimization should <b>decrease</b> as the size of the training data increases. We present theoretical and empirical results demonstrating how a simple subgradient descent approach indeed displays such behavior, at least for linear kernels.</p></div></span> <a id="expcoll117" href="JavaScript: expandcollapse('expcoll117',117)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390274">Data spectroscopy: learning mixture models using eigenspaces of convolution operators</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81336492903">Tao Shi</a>,
<a href="author_page.cfm?id=81100386814">Mikhail Belkin</a>,
<a href="author_page.cfm?id=81100472076">Bin Yu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 936-943</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390274" title="DOI">10.1145/1390156.1390274</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390274&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow118" style="display:inline;"><br /><div style="display:inline">In this paper we develop a spectral framework for estimating mixture distributions, specifically Gaussian mixture models. In physics, spectroscopy is often used for the identification of substances through their spectrum. Treating a kernel function K(x, ...</div></span>
<span id="toHide118" style="display:none;"><br /><div style="display:inline"><p>In this paper we develop a spectral framework for estimating mixture distributions, specifically Gaussian mixture models. In physics, spectroscopy is often used for the identification of substances through their spectrum. Treating a kernel function <i>K(x, y)</i> as "light" and the sampled data as "substance", the spectrum of their interaction (eigenvalues and eigenvectors of the kernel matrix <i>K</i>) unveils certain aspects of the underlying parametric distribution <i>p</i>, such as the parameters of a Gaussian mixture. Our approach extends the intuitions and analyses underlying the existing spectral techniques, such as spectral clustering and Kernel Principal Components Analysis (KPCA).</p> <p>We construct algorithms to estimate parameters of Gaussian mixture models, including the number of mixture components, their means and covariance matrices, which are important in many practical applications. We provide a theoretical framework and show encouraging experimental results.</p></div></span> <a id="expcoll118" href="JavaScript: expandcollapse('expcoll118',118)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390275">A generalization of Haussler's convolution kernel: mapping kernel</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81385590946">Kilho Shin</a>,
<a href="author_page.cfm?id=81100647289">Tetsuji Kuboyama</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 944-951</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390275" title="DOI">10.1145/1390156.1390275</a></span></td>
</tr>
 <tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390275&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow119" style="display:inline;"><br /><div style="display:inline">Haussler's convolution kernel provides a successful framework for engineering new positive semidefinite kernels, and has been applied to a wide range of data types and applications. In the framework, each data object represents a finite set of finer ...</div></span>
<span id="toHide119" style="display:none;"><br /><div style="display:inline"><p>Haussler's convolution kernel provides a successful framework for engineering new positive semidefinite kernels, and has been applied to a wide range of data types and applications. In the framework, each data object represents a finite set of finer grained components. Then, Haussler's convolution kernel takes a pair of data objects as input, and returns the sum of the return values of the predetermined primitive positive semidefinite kernel calculated for all the possible pairs of the components of the input data objects. On the other hand, the <i>mapping kernel</i> that we introduce in this paper is a natural generalization of Haussler's convolution kernel, in that the input to the primitive kernel moves over a predetermined subset rather than the entire cross product. Although we have plural instances of the mapping kernel in the literature, their positive semidefiniteness was investigated in case-by-case manners, and worse yet, was sometimes incorrectly concluded. In fact, there exists a simple and easily checkable necessary and sufficient condition, which is generic in the sense that it enables us to investigate the positive semidefiniteness of an arbitrary instance of the mapping kernel. This is the first paper that presents and proves the validity of the condition. In addition, we introduce two important instances of the mapping kernel, which we refer to as the <i>size-of-index-structure-distribution</i> kernel and the <i>editcost-distribution</i> kernel. Both of them are naturally derived from well known (dis)similarity measurements in the literature (<i>e.g</i>. the maximum agreement tree, the edit distance), and are reasonably expected to improve the performance of the existing measures by evaluating their distributional features rather than their peak (maximum/minimum) features.</p></div></span> <a id="expcoll119" href="JavaScript: expandcollapse('expcoll119',119)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390276"><i>mStruct</i>: a new admixture model for inference of population structure in light of both genetic admixing and allele mutations</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421602162">Suyash Shringarpure</a>,
<a href="author_page.cfm?id=81407592503">Eric P. Xing</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 952-959</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390276" title="DOI">10.1145/1390156.1390276</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390276&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow120" style="display:inline;"><br /><div style="display:inline">Traditional methods for analyzing population structure, such as the Structure program, ignore the influence of mutational effects. We propose mStruct, an admixture of population-specific mixtures of inheritance models, that addresses the ...</div></span>
<span id="toHide120" style="display:none;"><br /><div style="display:inline"><p>Traditional methods for analyzing population structure, such as the <i>Structure</i> program, ignore the influence of mutational effects. We propose <i>mStruct</i>, an admixture of population-specific mixtures of inheritance models, that addresses the task of structure inference and mutation estimation jointly through a hierarchical Bayesian framework, and a variational algorithm for inference. We validated our method on synthetic data, and used it to analyze the HGDP-CEPH cell line panel of microsatellites used in (Rosenberg et al., 2002) and the HGDP SNP data used in (Conrad et al., 2006). A comparison of the structural maps of world populations estimated by <i>mStruct</i> and <i>Structure</i> is presented, and we also report potentially interesting mutation patterns in world populations estimated by <i>mStruct</i>, which is not possible by <i>Structure</i>.</p></div></span> <a id="expcoll120" href="JavaScript: expandcollapse('expcoll120',120)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390277">Expectation-maximization for sparse and non-negative PCA</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81336492872">Christian D. Sigg</a>,
<a href="author_page.cfm?id=81331489168">Joachim M. Buhmann</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 960-967</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390277" title="DOI">10.1145/1390156.1390277</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390277&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow121" style="display:inline;"><br /><div style="display:inline">We study the problem of finding the dominant eigenvector of the sample covariance matrix, under additional constraints on the vector: a cardinality constraint limits the number of non-zero elements, and non-negativity forces the elements to have equal ...</div></span>
<span id="toHide121" style="display:none;"><br /><div style="display:inline"><p>We study the problem of finding the dominant eigenvector of the sample covariance matrix, under additional constraints on the vector: a cardinality constraint limits the number of non-zero elements, and non-negativity forces the elements to have equal sign. This problem is known as sparse and non-negative principal component analysis (PCA), and has many applications including dimensionality reduction and feature selection. Based on expectation-maximization for probabilistic PCA, we present an algorithm for any combination of these constraints. Its complexity is at most quadratic in the number of dimensions of the data. We demonstrate significant improvements in performance and computational efficiency compared to other constrained PCA algorithms, on large data sets from biology and computer vision. Finally, we show the usefulness of non-negative sparse PCA for unsupervised feature selection in a gene clustering task.</p></div></span> <a id="expcoll121" href="JavaScript: expandcollapse('expcoll121',121)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390278">Sample-based learning and search with permanent and transient memories</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100336678">David Silver</a>,
<a href="author_page.cfm?id=81342513055">Richard S. Sutton</a>,
<a href="author_page.cfm?id=81100615505">Martin M&#252;ller</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 968-975</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390278" title="DOI">10.1145/1390156.1390278</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390278&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow122" style="display:inline;"><br /><div style="display:inline">We present a reinforcement learning architecture, Dyna-2, that encompasses both sample-based learning and sample-based search, and that generalises across states during both learning and search. We apply Dyna-2 to high performance Computer Go. In this ...</div></span>
<span id="toHide122" style="display:none;"><br /><div style="display:inline"><p>We present a reinforcement learning architecture, Dyna-2, that encompasses both sample-based learning and sample-based search, and that generalises across states during both learning and search. We apply Dyna-2 to high performance Computer Go. In this domain the most successful planning methods are based on sample-based search algorithms, such as UCT, in which states are treated individually, and the most successful learning methods are based on temporal-difference learning algorithms, such as Sarsa, in which linear function approximation is used. In both cases, an estimate of the value function is formed, but in the first case it is transient, computed and then discarded after each move, whereas in the second case it is more permanent, slowly accumulating over many moves and games. The idea of Dyna-2 is for the transient planning memory and the permanent learning memory to remain separate, but for both to be based on linear function approximation and both to be updated by Sarsa. To apply Dyna-2 to 9x9 Computer Go, we use a million binary features in the function approximator, based on templates matching small fragments of the board. Using only the transient memory, Dyna-2 performed at least as well as UCT. Using both memories combined, it significantly outperformed UCT. Our program based on Dyna-2 achieved a higher rating on the Computer Go Online Server than any handcrafted or traditional search based program.</p></div></span> <a id="expcoll122" href="JavaScript: expandcollapse('expcoll122',122)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390279">An RKHS for multi-view learning and manifold co-regularization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309499649">Vikas Sindhwani</a>,
<a href="author_page.cfm?id=81421600848">David S. Rosenberg</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 976-983</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390279" title="DOI">10.1145/1390156.1390279</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390279&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow123" style="display:inline;"><br /><div style="display:inline">Inspired by co-training, many multi-view semi-supervised kernel methods implement the following idea: find a function in each of multiple Reproducing Kernel Hilbert Spaces (RKHSs) such that (a) the chosen functions make similar predictions on unlabeled ...</div></span>
<span id="toHide123" style="display:none;"><br /><div style="display:inline"><p>Inspired by co-training, many multi-view semi-supervised kernel methods implement the following idea: find a function in each of multiple Reproducing Kernel Hilbert Spaces (RKHSs) such that (a) the chosen functions make similar predictions on unlabeled examples, and (b) the average prediction given by the chosen functions performs well on labeled examples. In this paper, we construct a single RKHS with a data-dependent "co-regularization" norm that reduces these approaches to standard supervised learning. The reproducing kernel for this RKHS can be explicitly derived and plugged into any kernel method, greatly extending the theoretical and algorithmic scope of coregularization. In particular, with this development, the Rademacher complexity bound for co-regularization given in (Rosenberg & Bartlett, 2007) follows easily from wellknown results. Furthermore, more refined bounds given by localized Rademacher complexity can also be easily applied. We propose a co-regularization based algorithmic alternative to manifold regularization (Belkin et al., 2006; Sindhwani et al., 2005a) that leads to major empirical improvements on semi-supervised tasks. Unlike the recently proposed transductive approach of (Yu et al., 2008), our RKHS formulation is truly semi-supervised and naturally extends to unseen test data.</p></div></span> <a id="expcoll123" href="JavaScript: expandcollapse('expcoll123',123)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390280">The asymptotics of semi-supervised learning in discriminative probabilistic models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421601425">Nataliya Sokolovska</a>,
<a href="author_page.cfm?id=81100008094">Olivier Capp&#233;</a>,
<a href="author_page.cfm?id=81100057721">Fran&#231;ois Yvon</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 984-991</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390280" title="DOI">10.1145/1390156.1390280</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390280&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow124" style="display:inline;"><br /><div style="display:inline">Semi-supervised learning aims at taking advantage of unlabeled data to improve the efficiency of supervised learning procedures. For discriminative models however, this is a challenging task. In this contribution, we introduce an original methodology ...</div></span>
<span id="toHide124" style="display:none;"><br /><div style="display:inline"><p>Semi-supervised learning aims at taking advantage of unlabeled data to improve the efficiency of supervised learning procedures. For discriminative models however, this is a challenging task. In this contribution, we introduce an original methodology for using unlabeled data through the design of a simple semi-supervised objective function. We prove that the corresponding semi-supervised estimator is asymptotically optimal. The practical consequences of this result are discussed for the case of the logistic regression model.</p></div></span> <a id="expcoll124" href="JavaScript: expandcollapse('expcoll124',124)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390281">Tailoring density estimation via reproducing kernel moment matching</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100578710">Le Song</a>,
<a href="author_page.cfm?id=81421600443">Xinhua Zhang</a>,
<a href="author_page.cfm?id=81100243402">Alex Smola</a>,
<a href="author_page.cfm?id=81333488991">Arthur Gretton</a>,
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 992-999</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390281" title="DOI">10.1145/1390156.1390281</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390281&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow125" style="display:inline;"><br /><div style="display:inline">Moment matching is a popular means of parametric density estimation. We extend this technique to nonparametric estimation of mixture models. Our approach works by embedding distributions into a reproducing kernel Hilbert space, and performing moment ...</div></span>
<span id="toHide125" style="display:none;"><br /><div style="display:inline"><p>Moment matching is a popular means of parametric density estimation. We extend this technique to nonparametric estimation of mixture models. Our approach works by embedding distributions into a reproducing kernel Hilbert space, and performing moment matching in that space. This allows us to tailor density estimators to a function class of interest (i.e., for which we would like to compute expectations). We show our density estimation approach is useful in applications such as message compression in graphical models, and image classification and retrieval.</p></div></span> <a id="expcoll125" href="JavaScript: expandcollapse('expcoll125',125)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390282">Detecting statistical interactions with additive groves of trees</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317490095">Daria Sorokina</a>,
<a href="author_page.cfm?id=81100100877">Rich Caruana</a>,
<a href="author_page.cfm?id=81100207912">Mirek Riedewald</a>,
<a href="author_page.cfm?id=81100118367">Daniel Fink</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1000-1007</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390282" title="DOI">10.1145/1390156.1390282</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390282&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow126" style="display:inline;"><br /><div style="display:inline">Discovering additive structure is an important step towards understanding a complex multi-dimensional function because it allows the function to be expressed as the sum of lower-dimensional components. When variables interact, however, their effects ...</div></span>
<span id="toHide126" style="display:none;"><br /><div style="display:inline"><p>Discovering additive structure is an important step towards understanding a complex multi-dimensional function because it allows the function to be expressed as the sum of lower-dimensional components. When variables interact, however, their effects are not additive and must be modeled and interpreted simultaneously. We present a new approach for the problem of interaction detection. Our method is based on comparing the performance of unrestricted and restricted prediction models, where restricted models are prevented from modeling an interaction in question. We show that an additive model-based regression ensemble, Additive Groves, can be restricted appropriately for use with this framework, and thus has the right properties for accurately detecting variable interactions.</p></div></span> <a id="expcoll126" href="JavaScript: expandcollapse('expcoll126',126)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390283">Metric embedding for kernel classification rules</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421594362">Bharath K. Sriperumbudur</a>,
<a href="author_page.cfm?id=81421593646">Omer A. Lang</a>,
<a href="author_page.cfm?id=81100118766">Gert R. G. Lanckriet</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1008-1015</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390283" title="DOI">10.1145/1390156.1390283</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390283&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow127" style="display:inline;"><br /><div style="display:inline">In this paper, we consider a smoothing kernel based classification rule and propose an algorithm for optimizing the performance of the rule by learning the bandwidth of the smoothing kernel along with a data-dependent distance metric. The data-dependent ...</div></span>
<span id="toHide127" style="display:none;"><br /><div style="display:inline"><p>In this paper, we consider a smoothing kernel based classification rule and propose an algorithm for optimizing the performance of the rule by learning the bandwidth of the smoothing kernel along with a data-dependent distance metric. The data-dependent distance metric is obtained by learning a function that embeds an arbitrary metric space into a Euclidean space while minimizing an upper bound on the resubstitution estimate of the error probability of the kernel classification rule. By restricting this embedding function to a reproducing kernel Hilbert space, we reduce the problem to solving a semidefinite program and show the resulting kernel classification rule to be a variation of the <i>k</i>-nearest neighbor rule. We compare the performance of the kernel rule (using the learned data-dependent distance metric) to state-of-the-art distance metric learning algorithms (designed for <i>k</i>-nearest neighbor classification) on some benchmark datasets. The results show that the proposed rule has either better or as good classification accuracy as the other metric learning algorithms.</p></div></span> <a id="expcoll127" href="JavaScript: expandcollapse('expcoll127',127)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390284">Discriminative parameter learning for Bayesian networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323496337">Jiang Su</a>,
<a href="author_page.cfm?id=81100120645">Harry Zhang</a>,
<a href="author_page.cfm?id=81100159332">Charles X. Ling</a>,
<a href="author_page.cfm?id=81100488606">Stan Matwin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1016-1023</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390284" title="DOI">10.1145/1390156.1390284</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390284&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
 <td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow128" style="display:inline;"><br /><div style="display:inline">Bayesian network classifiers have been widely used for classification problems. Given a fixed Bayesian network structure, parameters learning can take two different approaches: generative and discriminative learning. While generative parameter learning ...</div></span>
<span id="toHide128" style="display:none;"><br /><div style="display:inline"><p>Bayesian network classifiers have been widely used for classification problems. Given a fixed Bayesian network structure, parameters learning can take two different approaches: generative and discriminative learning. While generative parameter learning is more efficient, discriminative parameter learning is more effective. In this paper, we propose a simple, efficient, and effective discriminative parameter learning method, called <i>Discriminative Frequency Estimate</i> (DFE), which learns parameters by discriminatively computing frequencies from data. Empirical studies show that the DFE algorithm integrates the advantages of both generative and discriminative learning: it performs as well as the state-of-the-art discriminative parameter learning method ELR in accuracy, but is significantly more efficient.</p></div></span> <a id="expcoll128" href="JavaScript: expandcollapse('expcoll128',128)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390285">A least squares formulation for canonical correlation analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365595166">Liang Sun</a>,
<a href="author_page.cfm?id=81333489125">Shuiwang Ji</a>,
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1024-1031</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390285" title="DOI">10.1145/1390156.1390285</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390285&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow129" style="display:inline;"><br /><div style="display:inline">Canonical Correlation Analysis (CCA) is a well-known technique for finding the correlations between two sets of multi-dimensional variables. It projects both sets of variables into a lower-dimensional space in which they are maximally correlated. CCA ...</div></span>
<span id="toHide129" style="display:none;"><br /><div style="display:inline"><p>Canonical Correlation Analysis (CCA) is a well-known technique for finding the correlations between two sets of multi-dimensional variables. It projects both sets of variables into a lower-dimensional space in which they are maximally correlated. CCA is commonly applied for supervised dimensionality reduction, in which one of the multi-dimensional variables is derived from the class label. It has been shown that CCA can be formulated as a least squares problem in the binaryclass case. However, their relationship in the more general setting remains unclear. In this paper, we show that, under a mild condition which tends to hold for high-dimensional data, CCA in multi-label classifications can be formulated as a least squares problem. Based on this equivalence relationship, we propose several CCA extensions including sparse CCA using 1-norm regularization. Experiments on multi-label data sets confirm the established equivalence relationship. Results also demonstrate the effectiveness of the proposed CCA extensions.</p></div></span> <a id="expcoll129" href="JavaScript: expandcollapse('expcoll129',129)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390286">Apprenticeship learning using linear programming</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597442">Umar Syed</a>,
<a href="author_page.cfm?id=81100395285">Michael Bowling</a>,
<a href="author_page.cfm?id=81100083454">Robert E. Schapire</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1032-1039</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390286" title="DOI">10.1145/1390156.1390286</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390286&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow130" style="display:inline;"><br /><div style="display:inline">In apprenticeship learning, the goal is to learn a policy in a Markov decision process that is at least as good as a policy demonstrated by an expert. The difficulty arises in that the MDP's true reward function is assumed to be unknown. We show how ...</div></span>
<span id="toHide130" style="display:none;"><br /><div style="display:inline"><p>In apprenticeship learning, the goal is to learn a policy in a Markov decision process that is at least as good as a policy demonstrated by an expert. The difficulty arises in that the MDP's true reward function is assumed to be unknown. We show how to frame apprenticeship learning as a linear programming problem, and show that using an off-the-shelf LP solver to solve this problem results in a substantial improvement in running time over existing methods---up to two orders of magnitude faster in our experiments. Additionally, our approach produces stationary policies, while all existing methods for apprenticeship learning output policies that are "mixed", i.e. randomized combinations of stationary policies. The technique used is general enough to convert any mixed policy to a stationary policy.</p></div></span> <a id="expcoll130" href="JavaScript: expandcollapse('expcoll130',130)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390287">Composite kernel learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435607384">Marie Szafranski</a>,
<a href="author_page.cfm?id=81100345039">Yves Grandvalet</a>,
<a href="author_page.cfm?id=81100280150">Alain Rakotomamonjy</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1040-1047</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390287" title="DOI">10.1145/1390156.1390287</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390287&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow131" style="display:inline;"><br /><div style="display:inline">The Support Vector Machine (SVM) is an acknowledged powerful tool for building classifiers, but it lacks flexibility, in the sense that the kernel is chosen prior to learning. Multiple Kernel Learning (MKL) enables to learn the kernel, from an ensemble ...</div></span>
<span id="toHide131" style="display:none;"><br /><div style="display:inline"><p>The Support Vector Machine (SVM) is an acknowledged powerful tool for building classifiers, but it lacks flexibility, in the sense that the kernel is chosen prior to learning. Multiple Kernel Learning (MKL) enables to learn the kernel, from an ensemble of basis kernels, whose combination is optimized in the learning process. Here, we propose Composite Kernel Learning to address the situation where distinct components give rise to a group structure among kernels. Our formulation of the learning problem encompasses several setups, putting more or less emphasis on the group structure. We characterize the convexity of the learning problem, and provide a general wrapper algorithm for computing solutions. Finally, we illustrate the behavior of our method on multi-channel data where groups correpond to channels.</p></div></span> <a id="expcoll131" href="JavaScript: expandcollapse('expcoll131',131)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390288">The many faces of optimism: a unifying approach</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100192810">Istv&#225;n Szita</a>,
<a href="author_page.cfm?id=81100130503">Andr&#225;s L&#337;rincz</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1048-1055</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390288" title="DOI">10.1145/1390156.1390288</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390288&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow132" style="display:inline;"><br /><div style="display:inline">The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. "Optimism in the face of uncertainty" and model building play central roles in advanced exploration methods. Here, we integrate ...</div></span>
<span id="toHide132" style="display:none;"><br /><div style="display:inline"><p>The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. "Optimism in the face of uncertainty" and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.</p></div></span> <a id="expcoll132" href="JavaScript: expandcollapse('expcoll132',132)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390289"><i>&#957;</i>-support vector machine as conditional value-at-risk minimization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100082639">Akiko Takeda</a>,
<a href="author_page.cfm?id=81100105605">Masashi Sugiyama</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1056-1063</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390289" title="DOI">10.1145/1390156.1390289</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390289&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow133" style="display:inline;"><br /><div style="display:inline">The &nu;-support vector classification (&nu;-SVC) algorithm was shown to work well and provide intuitive interpretations, e.g., the parameter &nu; roughly specifies the fraction of support vectors. Although &nu; corresponds ...</div></span>
<span id="toHide133" style="display:none;"><br /><div style="display:inline"><p>The <i>&nu;</i>-support vector classification (<i>&nu;</i>-SVC) algorithm was shown to work well and provide intuitive interpretations, e.g., the parameter <i>&nu;</i> roughly specifies the fraction of support vectors. Although <i>&nu;</i> corresponds to a fraction, it cannot take the entire range between 0 and 1 in its original form. This problem was settled by a non-convex extension of <i>&nu;</i>-SVC and the extended method was experimentally shown to generalize better than original <i>&nu;</i>-SVC. However, its good generalization performance and convergence properties of the optimization algorithm have not been studied yet. In this paper, we provide new theoretical insights into these issues and propose a novel <i>&nu;</i>-SVC algorithm that has guaranteed generalization performance and convergence properties.</p></div></span> <a id="expcoll133" href="JavaScript: expandcollapse('expcoll133',133)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390290">Training restricted Boltzmann machines using approximations to the likelihood gradient</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421592957">Tijmen Tieleman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1064-1071</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390290" title="DOI">10.1145/1390156.1390290</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390290&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow134" style="display:inline;"><br /><div style="display:inline">A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the ...</div></span>
<span id="toHide134" style="display:none;"><br /><div style="display:inline"><p>A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple.</p></div></span> <a id="expcoll134" href="JavaScript: expandcollapse('expcoll134',134)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390291">A semiparametric statistical approach to model-free policy evaluation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421593517">Tsuyoshi Ueno</a>,
<a href="author_page.cfm?id=81100634597">Motoaki Kawanabe</a>,
<a href="author_page.cfm?id=81100500974">Takeshi Mori</a>,
<a href="author_page.cfm?id=81310500282">Shin-ichi Maeda</a>,
<a href="author_page.cfm?id=81452610710">Shin Ishii</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1072-1079</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390291" title="DOI">10.1145/1390156.1390291</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390291&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow135" style="display:inline;"><br /><div style="display:inline">Reinforcement learning (RL) methods based on least-squares temporal difference (LSTD) have been developed recently and have shown good practical performance. However, the quality of their estimation has not been well elucidated. In this article, we discuss ...</div></span>
<span id="toHide135" style="display:none;"><br /><div style="display:inline"><p>Reinforcement learning (RL) methods based on least-squares temporal difference (LSTD) have been developed recently and have shown good practical performance. However, the quality of their estimation has not been well elucidated. In this article, we discuss LSTD-based policy evaluation from the new view-point of semiparametric statistical inference. In fact, the estimator can be obtained from a particular estimating function which guarantees its convergence to the true value asymptotically, without specifying a model of the environment. Based on these observations, we 1) analyze the asymptotic variance of an LSTD-based estimator, 2) derive the optimal estimating function with the minimum asymptotic estimation variance, and 3) derive a suboptimal estimator to reduce the computational burden in obtaining the optimal estimating function.</p></div></span> <a id="expcoll135" href="JavaScript: expandcollapse('expcoll135',135)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390292">Topologically-constrained latent variable models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100176326">Raquel Urtasun</a>,
<a href="author_page.cfm?id=81100620901">David J. Fleet</a>,
<a href="author_page.cfm?id=81421602184">Andreas Geiger</a>,
<a href="author_page.cfm?id=81100620337">Jovan Popovi&#263;</a>,
<a href="author_page.cfm?id=81100537374">Trevor J. Darrell</a>,
<a href="author_page.cfm?id=81100574233">Neil D. Lawrence</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1080-1087</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390292" title="DOI">10.1145/1390156.1390292</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390292&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow136" style="display:inline;"><br /><div style="display:inline">In dimensionality reduction approaches, the data are typically embedded in a Euclidean latent space. However for some data sets this is inappropriate. For example, in human motion data we expect latent spaces that are cylindrical or a toroidal, that ...</div></span>
<span id="toHide136" style="display:none;"><br /><div style="display:inline"><p>In dimensionality reduction approaches, the data are typically embedded in a Euclidean latent space. However for some data sets this is inappropriate. For example, in human motion data we expect latent spaces that are cylindrical or a toroidal, that are poorly captured with a Euclidean space. In this paper, we present a range of approaches for embedding data in a non-Euclidean latent space. Our focus is the Gaussian Process latent variable model. In the context of human motion modeling this allows us to (a) learn models with interpretable latent directions enabling, for example, style/content separation, and (b) generalise beyond the data set enabling us to learn transitions between motion styles even though such transitions are not present in the data.</p></div></span> <a id="expcoll136" href="JavaScript: expandcollapse('expcoll136',136)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390293">Beam sampling for the infinite hidden Markov model</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421600903">Jurgen Van Gael</a>,
<a href="author_page.cfm?id=81421600186">Yunus Saatci</a>,
<a href="author_page.cfm?id=81100324628">Yee Whye Teh</a>,
<a href="author_page.cfm?id=81100572858">Zoubin Ghahramani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1088-1095</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390293" title="DOI">10.1145/1390156.1390293</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390293&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow137" style="display:inline;"><br /><div style="display:inline">The infinite hidden Markov model is a non-parametric extension of the widely used hidden Markov model. Our paper introduces a new inference algorithm for the infinite Hidden Markov model called beam sampling. Beam sampling combines slice sampling, ...</div></span>
<span id="toHide137" style="display:none;"><br /><div style="display:inline"><p>The infinite hidden Markov model is a non-parametric extension of the widely used hidden Markov model. Our paper introduces a new inference algorithm for the infinite Hidden Markov model called <i>beam sampling</i>. Beam sampling combines slice sampling, which limits the number of states considered at each time step to a finite number, with dynamic programming, which samples whole state trajectories efficiently. Our algorithm typically outperforms the Gibbs sampler and is more robust. We present applications of iHMM inference using the beam sampler on changepoint detection and text prediction problems.</p></div></span> <a id="expcoll137" href="JavaScript: expandcollapse('expcoll137',137)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390294">Extracting and composing robust features with denoising autoencoders</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100122192">Pascal Vincent</a>,
<a href="author_page.cfm?id=81319495662">Hugo Larochelle</a>,
<a href="author_page.cfm?id=81100287057">Yoshua Bengio</a>,
<a href="author_page.cfm?id=81421598556">Pierre-Antoine Manzagol</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1096-1103</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390294" title="DOI">10.1145/1390156.1390294</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390294&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow138" style="display:inline;"><br /><div style="display:inline">Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training ...</div></span>
<span id="toHide138" style="display:none;"><br /><div style="display:inline"><p>Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.</p></div></span> <a id="expcoll138" href="JavaScript: expandcollapse('expcoll138',138)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390295">Prediction with expert advice for the Brier game</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81408599323">Vladimir Vovk</a>,
<a href="author_page.cfm?id=81421592784">Fedor Zhdanov</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1104-1111</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390295" title="DOI">10.1145/1390156.1390295</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390295&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
 </tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow139" style="display:inline;"><br /><div style="display:inline">We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee ...</div></span>
<span id="toHide139" style="display:none;"><br /><div style="display:inline"><p>We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee turns out to be rather tight on these data sets, especially in the case of the more extensive tennis data.</p></div></span> <a id="expcoll139" href="JavaScript: expandcollapse('expcoll139',139)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390296">Sparse multiscale gaussian process regression</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309483100">Christian Walder</a>,
<a href="author_page.cfm?id=81100660893">Kwang In Kim</a>,
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1112-1119</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390296" title="DOI">10.1145/1390156.1390296</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390296&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow140" style="display:inline;"><br /><div style="display:inline">Most existing sparse Gaussian process (g.p.) models seek computational advantages by basing their computations on a set of m basis functions that are the covariance function of the g.p. with one of its two inputs fixed. We generalise this for ...</div></span>
<span id="toHide140" style="display:none;"><br /><div style="display:inline"><p>Most existing sparse Gaussian process (g.p.) models seek computational advantages by basing their computations on a set of <i>m</i> basis functions that are the covariance function of the g.p. with one of its two inputs fixed. We generalise this for the case of Gaussian covariance function, by basing our computations on <i>m</i> Gaussian basis functions with arbitrary diagonal covariance matrices (or length scales). For a fixed number of basis functions and any given criteria, this additional flexibility permits approximations no worse and typically better than was previously possible. We perform gradient based optimisation of the marginal likelihood, which costs <i>O</i>(<i>m</i><sup>2</sup><i>n</i>) time where <i>n</i> is the number of data points, and compare the method to various other sparse g.p. methods. Although we focus on g.p. regression, the central idea is applicable to all kernel based algorithms, and we also provide some results for the support vector machine (s.v.m.) and kernel ridge regression (k.r.r.). Our approach outperforms the other methods, particularly for the case of very few basis functions, <i>i. e.</i> a very high sparsity ratio.</p></div></span> <a id="expcoll140" href="JavaScript: expandcollapse('expcoll140',140)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390297">Manifold alignment using Procrustes analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597565">Chang Wang</a>,
<a href="author_page.cfm?id=81100132345">Sridhar Mahadevan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1120-1127</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390297" title="DOI">10.1145/1390156.1390297</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390297&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow141" style="display:inline;"><br /><div style="display:inline">In this paper we introduce a novel approach to manifold alignment, based on Procrustes analysis. Our approach differs from "semi-supervised alignment" in that it results in a mapping that is defined everywhere - when used with a suitable dimensionality ...</div></span>
<span id="toHide141" style="display:none;"><br /><div style="display:inline"><p>In this paper we introduce a novel approach to manifold alignment, based on Procrustes analysis. Our approach differs from "semi-supervised alignment" in that it results in a mapping that is defined everywhere - when used with a suitable dimensionality reduction method - rather than just on the training data points. We describe and evaluate our approach both theoretically and experimentally, providing results showing useful knowledge transfer from one domain to another. Novel applications of our method including cross-lingual information retrieval and transfer learning in Markov decision processes are presented.</p></div></span> <a id="expcoll141" href="JavaScript: expandcollapse('expcoll141',141)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390298">Dirichlet component analysis: feature extraction for compositional data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421603677">Hua-Yan Wang</a>,
<a href="author_page.cfm?id=81372591186">Qiang Yang</a>,
<a href="author_page.cfm?id=81100550215">Hong Qin</a>,
<a href="author_page.cfm?id=81100528812">Hongbin Zha</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1128-1135</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390298" title="DOI">10.1145/1390156.1390298</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390298&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow142" style="display:inline;"><br /><div style="display:inline">We consider feature extraction (dimensionality reduction) for compositional data, where the data vectors are constrained to be positive and constant-sum. In real-world problems, the data components (variables) usually have complicated "correlations" ...</div></span>
<span id="toHide142" style="display:none;"><br /><div style="display:inline"><p>We consider feature extraction (dimensionality reduction) for compositional data, where the data vectors are constrained to be positive and constant-sum. In real-world problems, the data components (variables) usually have complicated "correlations" while their total number is huge. Such scenario demands feature extraction. That is, we shall de-correlate the components and reduce their dimensionality. Traditional techniques such as the Principle Component Analysis (PCA) are not suitable for these problems due to unique statistical properties and the need to satisfy the constraints in compositional data. This paper presents a novel approach to feature extraction for compositional data. Our method first identifies a family of dimensionality reduction projections that preserve all relevant constraints, and then finds the optimal projection that maximizes the estimated Dirichlet precision on projected data. It reduces the compositional data to a given lower dimensionality while the components in the lower-dimensional space are de-correlated as much as possible. We develop theoretical foundation of our approach, and validate its effectiveness on some synthetic and real-world datasets.</p></div></span> <a id="expcoll142" href="JavaScript: expandcollapse('expcoll142',142)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390299">Adaptive p-posterior mixture-model kernels for multiple instance learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421603677">Hua-Yan Wang</a>,
<a href="author_page.cfm?id=81372591186">Qiang Yang</a>,
<a href="author_page.cfm?id=81100528812">Hongbin Zha</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1136-1143</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390299" title="DOI">10.1145/1390156.1390299</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390299&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow143" style="display:inline;"><br /><div style="display:inline">In multiple instance learning (MIL), how the instances determine the bag-labels is an essential issue, both algorithmically and intrinsically. In this paper, we show that the mechanism of how the instances determine the bag-labels is different ...</div></span>
<span id="toHide143" style="display:none;"><br /><div style="display:inline"><p>In multiple instance learning (MIL), how the instances determine the bag-labels is an essential issue, both algorithmically and intrinsically. In this paper, we show that the <i>mechanism</i> of how the instances determine the bag-labels is different for different application domains, and does not necessarily obey the traditional assumptions of MIL. We therefore propose an <i>adaptive</i> framework for MIL that adapts to different application domains by learning the domain-specific <i>mechanisms</i> merely from labeled bags. Our approach is especially attractive when we are encountered with novel application domains, for which the <i>mechanisms</i> may be different and unknown. Specifically, we exploit mixture models to represent the composition of each bag and an adaptable kernel function to represent the relationship between the bags. We validate on synthetic MIL datasets that the kernel function automatically adapts to different <i>mechanisms</i> of how the instances determine the bag-labels. We also compare our approach with state-of-the-art MIL techniques on real-world benchmark datasets.</p></div></span> <a id="expcoll143" href="JavaScript: expandcollapse('expcoll143',143)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390300">Graph transduction via alternating minimization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81350589070">Jun Wang</a>,
<a href="author_page.cfm?id=81100510477">Tony Jebara</a>,
<a href="author_page.cfm?id=81406599942">Shih-Fu Chang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1144-1151</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390300" title="DOI">10.1145/1390156.1390300</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390300&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow144" style="display:inline;"><br /><div style="display:inline">Graph transduction methods label input data by learning a classification function that is regularized to exhibit smoothness along a graph over labeled and unlabeled samples. In practice, these algorithms are sensitive to the initial set of labels provided ...</div></span>
<span id="toHide144" style="display:none;"><br /><div style="display:inline"><p>Graph transduction methods label input data by learning a classification function that is regularized to exhibit smoothness along a graph over labeled and unlabeled samples. In practice, these algorithms are sensitive to the initial set of labels provided by the user. For instance, classification accuracy drops if the training set contains weak labels, if imbalances exist across label classes or if the labeled portion of the data is not chosen at random. This paper introduces a propagation algorithm that more reliably minimizes a cost function over both a function on the graph and a binary label matrix. The cost function generalizes prior work in graph transduction and also introduces node normalization terms for resilience to label imbalances. We demonstrate that global minimization of the function is intractable but instead provide an alternating minimization scheme that incrementally adjusts the function and the labels towards a reliable local minimum. Unlike prior methods, the resulting propagation of labels does not prematurely commit to an erroneous labeling and obtains more consistent labels. Experiments are shown for synthetic and real classification tasks including digit and text recognition. A substantial improvement in accuracy compared to state of the art semi-supervised methods is achieved. The advantage are even more dramatic when labeled instances are limited.</p></div></span> <a id="expcoll144" href="JavaScript: expandcollapse('expcoll144',144)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390301">On multi-view active learning and the combination with semi-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452601906">Wei Wang</a>,
<a href="author_page.cfm?id=81451593001">Zhi-Hua Zhou</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1152-1159</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390301" title="DOI">10.1145/1390156.1390301</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390301&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow145" style="display:inline;"><br /><div style="display:inline">Multi-view learning has become a hot topic during the past few years. In this paper, we first characterize the sample complexity of multi-view active learning. Under the &alpha;-expansion assumption, we get an exponential improvement ...</div></span>
<span id="toHide145" style="display:none;"><br /><div style="display:inline"><p><i>Multi-view learning</i> has become a hot topic during the past few years. In this paper, we first characterize the sample complexity of multi-view <i>active learning</i>. Under the &alpha;-<i>expansion</i> assumption, we get an exponential improvement in the sample complexity from usual <i>&Otilde;</i>(1/<i>&epsilon;</i>) to <i>&Otilde;</i>(log 1/<i>&epsilon;</i>), requiring neither strong assumption on data distribution such as the data is distributed uniformly over the unit sphere in R<i><sup>d</sup></i> nor strong assumption on hypothesis class such as linear separators through the origin. We also give an upper bound of the error rate when the &alpha;-<i>expansion</i> assumption does not hold. Then, we analyze the combination of multi-view active learning and semi-supervised learning and get a further improvement in the sample complexity. Finally, we study the empirical behavior of the two paradigms, which verifies that the combination of multi-view active learning and semi-supervised learning is efficient.</p></div></span> <a id="expcoll145" href="JavaScript: expandcollapse('expcoll145',145)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390302">Fast solvers and efficient implementations for distance metric learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=99659072293">Kilian Q. Weinberger</a>,
<a href="author_page.cfm?id=81100590935">Lawrence K. Saul</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1160-1167</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390302" title="DOI">10.1145/1390156.1390302</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390302&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow146" style="display:inline;"><br /><div style="display:inline">In this paper we study how to improve nearest neighbor classification by learning a Mahalanobis distance metric. We build on a recently proposed framework for distance metric learning known as large margin nearest neighbor (LMNN) classification. Our ...</div></span>
<span id="toHide146" style="display:none;"><br /><div style="display:inline"><p>In this paper we study how to improve nearest neighbor classification by learning a Mahalanobis distance metric. We build on a recently proposed framework for distance metric learning known as large margin nearest neighbor (LMNN) classification. Our paper makes three contributions. First, we describe a highly efficient solver for the particular instance of semidefinite programming that arises in LMNN classification; our solver can handle problems with billions of large margin constraints in a few hours. Second, we show how to reduce both training and testing times using metric ball trees; the speedups from ball trees are further magnified by learning low dimensional representations of the input space. Third, we show how to learn different Mahalanobis distance metrics in different parts of the input space. For large data sets, the use of locally adaptive distance metrics leads to even lower error rates.</p></div></span> <a id="expcoll146" href="JavaScript: expandcollapse('expcoll146',146)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390303">Deep learning via semi-supervised embedding</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100015405">Jason Weston</a>,
<a href="author_page.cfm?id=81421598251">Fr&#233;d&#233;ric Ratle</a>,
<a href="author_page.cfm?id=81100001072">Ronan Collobert</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1168-1175</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390303" title="DOI">10.1145/1390156.1390303</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390303&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow147" style="display:inline;"><br /><div style="display:inline">We show how nonlinear embedding algorithms popular for use with shallow semi-supervised learning techniques such as kernel methods can be applied to deep multilayer architectures, either as a regularizer at the output layer, or on each layer of ...</div></span>
<span id="toHide147" style="display:none;"><br /><div style="display:inline"><p>We show how nonlinear embedding algorithms popular for use with <i>shallow</i> semi-supervised learning techniques such as kernel methods can be applied to deep multilayer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This provides a simple alternative to existing approaches to <i>deep</i> learning whilst yielding competitive error rates compared to those methods, and existing <i>shallow</i> semi-supervised techniques.</p></div></span> <a id="expcoll147" href="JavaScript: expandcollapse('expcoll147',147)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390304">Efficiently learning linear-linear exponential family predictive representations of state</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100338719">David Wingate</a>,
<a href="author_page.cfm?id=81327491508">Satinder Singh</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1176-1183</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390304" title="DOI">10.1145/1390156.1390304</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390304&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow148" style="display:inline;"><br /><div style="display:inline">Exponential Family PSR (EFPSR) models capture stochastic dynamical systems by representing state as the parameters of an exponential family distribution over a shortterm window of future observations. They are appealing from a learning perspective because ...</div></span>
<span id="toHide148" style="display:none;"><br /><div style="display:inline"><p>Exponential Family PSR (EFPSR) models capture stochastic dynamical systems by representing state as the parameters of an exponential family distribution over a shortterm window of future observations. They are appealing from a learning perspective because they are fully observed (meaning expressions for maximum likelihood do not involve hidden quantities), but are still expressive enough to both capture existing models and predict new models. While maximum-likelihood learning algorithms for EFPSRs exist, they are not computationally feasible. We present a new, computationally efficient, learning algorithm based on an approximate likelihood function. The algorithm can be interpreted as attempting to induce stationary distributions of observations, features and states which match their empirically observed counterparts. The approximate likelihood, and the idea of matching stationary distributions, may apply to other models.</p></div></span> <a id="expcoll148" href="JavaScript: expandcollapse('expcoll148',148)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390305">Fully distributed EM for very large datasets</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=86058634757">Jason Wolfe</a>,
<a href="author_page.cfm?id=81323490812">Aria Haghighi</a>,
<a href="author_page.cfm?id=81100142014">Dan Klein</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1184-1191</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390305" title="DOI">10.1145/1390156.1390305</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390305&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow149" style="display:inline;"><br /><div style="display:inline">In EM and related algorithms, E-step computations distribute easily, because data items are independent given parameters. For very large data sets, however, even storing all of the parameters in a single node for the M-step can be impractical. We present ...</div></span>
<span id="toHide149" style="display:none;"><br /><div style="display:inline"><p>In EM and related algorithms, E-step computations distribute easily, because data items are independent given parameters. For very large data sets, however, even storing all of the parameters in a single node for the M-step can be impractical. We present a framework that fully distributes the entire EM procedure. Each node interacts only with parameters relevant to its data, sending messages to other nodes along a junction-tree topology. We demonstrate improvements over a MapReduce topology, on two tasks: word alignment and topic modeling.</p></div></span> <a id="expcoll149" href="JavaScript: expandcollapse('expcoll149',149)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390306">Listwise approach to learning to rank: theory and algorithm</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365594546">Fen Xia</a>,
<a href="author_page.cfm?id=81350580267">Tie-Yan Liu</a>,
<a href="author_page.cfm?id=81100233470">Jue Wang</a>,
<a href="author_page.cfm?id=81414604571">Wensheng Zhang</a>,
<a href="author_page.cfm?id=81350598903">Hang Li</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1192-1199</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390306" title="DOI">10.1145/1390156.1390306</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390306&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow150" style="display:inline;"><br /><div style="display:inline">This paper aims to conduct a study on the listwise approach to learning to rank. The listwise approach learns a ranking function by taking individual lists as instances and minimizing a loss function defined on the predicted list and the ground-truth ...</div></span>
<span id="toHide150" style="display:none;"><br /><div style="display:inline"><p>This paper aims to conduct a study on the listwise approach to learning to rank. The listwise approach learns a ranking function by taking individual lists as instances and minimizing a loss function defined on the predicted list and the ground-truth list. Existing work on the approach mainly focused on the development of new algorithms; methods such as RankCosine and ListNet have been proposed and good performances by them have been observed. Unfortunately, the underlying theory was not sufficiently studied so far. To amend the problem, this paper proposes conducting theoretical analysis of learning to rank algorithms through investigations on the properties of the loss functions, including consistency, soundness, continuity, differentiability, convexity, and efficiency. A sufficient condition on consistency for ranking is given, which seems to be the first such result obtained in related research. The paper then conducts analysis on three loss functions: likelihood loss, cosine loss, and cross entropy loss. The latter two were used in RankCosine and ListNet. The use of the likelihood loss leads to the development of a new listwise method called ListMLE, whose loss function offers better properties, and also leads to better experimental results.</p></div></span> <a id="expcoll150" href="JavaScript: expandcollapse('expcoll150',150)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390307">Democratic approximation of lexicographic preference models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
 <a href="author_page.cfm?id=81100225494">Fusun Yaman</a>,
<a href="author_page.cfm?id=81100377963">Thomas J. Walsh</a>,
<a href="author_page.cfm?id=81406601119">Michael L. Littman</a>,
<a href="author_page.cfm?id=81100325069">Marie desJardins</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1200-1207</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390307" title="DOI">10.1145/1390156.1390307</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390307&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow151" style="display:inline;"><br /><div style="display:inline">Previous algorithms for learning lexicographic preference models (LPMs) produce a "best guess" LPM that is consistent with the observations. Our approach is more democratic: we do not commit to a single LPM. Instead, we approximate the target using the ...</div></span>
<span id="toHide151" style="display:none;"><br /><div style="display:inline"><p>Previous algorithms for learning lexicographic preference models (LPMs) produce a "best guess" LPM that is consistent with the observations. Our approach is more democratic: we do not commit to a single LPM. Instead, we approximate the target using the votes of a <i>collection</i> of consistent LPMs. We present two variations of this method---<i>variable voting</i> and <i>model voting</i>---and empirically show that these democratic algorithms outperform the existing methods. We also introduce an intuitive yet powerful learning bias to prune some of the possible LPMs. We demonstrate how this learning bias can be used with variable and model voting and show that the learning bias improves the learning curve significantly, especially when the number of observations is small.</p></div></span> <a id="expcoll151" href="JavaScript: expandcollapse('expcoll151',151)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390308">Preconditioned temporal difference learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81537640256">Hengshuai Yao</a>,
<a href="author_page.cfm?id=81544353756">Zhi-Qiang Liu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1208-1215</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390308" title="DOI">10.1145/1390156.1390308</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390308&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
 </tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow152" style="display:inline;"><br /><div style="display:inline">This paper extends many of the recent popular policy evaluation algorithms to a generalized framework that includes least-squares temporal difference (LSTD) learning, least-squares policy evaluation (LSPE) and a variant of incremental LSTD (iLSTD). The ...</div></span>
<span id="toHide152" style="display:none;"><br /><div style="display:inline"><p>This paper extends many of the recent popular policy evaluation algorithms to a generalized framework that includes least-squares temporal difference (LSTD) learning, least-squares policy evaluation (LSPE) and a variant of incremental LSTD (iLSTD). The basis of this extension is a preconditioning technique that solves a stochastic model equation. This paper also studies three significant issues of the new framework: it presents a new rule of step-size that can be computed online, provides an iterative way to apply preconditioning, and reduces the complexity of related algorithms to near that of temporal difference (TD) learning.</p></div></span> <a id="expcoll152" href="JavaScript: expandcollapse('expcoll152',152)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390309">A quasi-Newton approach to non-smooth convex optimization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421601599">Jin Yu</a>,
<a href="author_page.cfm?id=81100528901">S. V. N. Vishwanathan</a>,
<a href="author_page.cfm?id=81537476056">Simon G&#252;nter</a>,
<a href="author_page.cfm?id=81421593235">Nicol N. Schraudolph</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1216-1223</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390309" title="DOI">10.1145/1390156.1390309</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390309&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow153" style="display:inline;"><br /><div style="display:inline">We extend the well-known BFGS quasi-Newton method and its limited-memory variant LBFGS to the optimization of non-smooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: The local quadratic ...</div></span>
<span id="toHide153" style="display:none;"><br /><div style="display:inline"><p>We extend the well-known BFGS quasi-Newton method and its limited-memory variant LBFGS to the optimization of non-smooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: The local quadratic model, the identification of a descent direction, and the Wolfe line search conditions. We apply the resulting subLBFGS algorithm to <i>L</i><sub>2</sub>-regularized risk minimization with binary hinge loss, and its direction-finding component to <i>L</i><sub>1</sub>-regularized risk minimization with logistic loss. In both settings our generic algorithms perform comparable to or better than their counterparts in specialized state-of-the-art solvers.</p></div></span> <a id="expcoll153" href="JavaScript: expandcollapse('expcoll153',153)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390310">Predicting diverse subsets using structural SVMs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597244">Yisong Yue</a>,
<a href="author_page.cfm?id=81100184551">Thorsten Joachims</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1224-1231</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390310" title="DOI">10.1145/1390156.1390310</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390310&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow154" style="display:inline;"><br /><div style="display:inline">In many retrieval tasks, one important goal involves retrieving a diverse set of results (e.g., documents covering a wide range of topics for a search query). First of all, this reduces redundancy, effectively showing more information with the presented ...</div></span>
<span id="toHide154" style="display:none;"><br /><div style="display:inline"><p>In many retrieval tasks, one important goal involves retrieving a diverse set of results (e.g., documents covering a wide range of topics for a search query). First of all, this reduces redundancy, effectively showing more information with the presented results. Secondly, queries are often ambiguous at some level. For example, the query "Jaguar" can refer to many different topics (such as the car or feline). A set of documents with high topic diversity ensures that fewer users abandon the query because no results are relevant to them. Unlike existing approaches to learning retrieval functions, we present a method that explicitly trains to diversify results. In particular, we formulate the learning problem of predicting diverse subsets and derive a training method based on structural SVMs.</p></div></span> <a id="expcoll154" href="JavaScript: expandcollapse('expcoll154',154)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390311">Improved Nystr&ouml;m low-rank approximation and error analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333492080">Kai Zhang</a>,
<a href="author_page.cfm?id=81309487444">Ivor W. Tsang</a>,
<a href="author_page.cfm?id=81100525095">James T. Kwok</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1232-1239</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390311" title="DOI">10.1145/1390156.1390311</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390311&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow155" style="display:inline;"><br /><div style="display:inline">Low-rank matrix approximation is an effective tool in alleviating the memory and computational burdens of kernel methods and sampling, as the mainstream of such algorithms, has drawn considerable attention in both theory and practice. This paper presents ...</div></span>
<span id="toHide155" style="display:none;"><br /><div style="display:inline"><p>Low-rank matrix approximation is an effective tool in alleviating the memory and computational burdens of kernel methods and sampling, as the mainstream of such algorithms, has drawn considerable attention in both theory and practice. This paper presents detailed studies on the Nystr&ouml;m sampling scheme and in particular, an error analysis that directly relates the Nystr&ouml;m approximation quality with the encoding powers of the landmark points in summarizing the data. The resultant error bound suggests a simple and efficient sampling scheme, the <i>k</i>-means clustering algorithm, for Nystr&ouml;m low-rank approximation. We compare it with state-of-the-art approaches that range from greedy schemes to probabilistic sampling. Our algorithm achieves significant performance gains in a number of supervised/unsupervised learning tasks including kernel PCA and least squares SVM.</p></div></span> <a id="expcoll155" href="JavaScript: expandcollapse('expcoll155',155)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390312">Estimating local optimums in EM algorithm over Gaussian mixture model</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100116075">Zhenjie Zhang</a>,
<a href="author_page.cfm?id=81321490357">Bing Tian Dai</a>,
<a href="author_page.cfm?id=81100486586">Anthony K. H. Tung</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1240-1247</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390312" title="DOI">10.1145/1390156.1390312</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390312&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow156" style="display:inline;"><br /><div style="display:inline">EM algorithm is a very popular iteration-based method to estimate the parameters of Gaussian Mixture Model from a large observation set. However, in most cases, EM algorithm is not guaranteed to converge to the global optimum. Instead, it stops at some ...</div></span>
<span id="toHide156" style="display:none;"><br /><div style="display:inline"><p>EM algorithm is a very popular iteration-based method to estimate the parameters of Gaussian Mixture Model from a large observation set. However, in most cases, EM algorithm is not guaranteed to converge to the global optimum. Instead, it stops at some local optimums, which can be much worse than the global optimum. Therefore, it is usually required to run multiple procedures of EM algorithm with different initial configurations and return the best solution. To improve the efficiency of this scheme, we propose a new method which can estimate an upper bound on the logarithm likelihood of the local optimum, based on the current configuration after the latest EM iteration. This is accomplished by first deriving some region bounding the possible locations of local optimum, followed by some upper bound estimation on the maximum likelihood. With this estimation, we can terminate an EM algorithm procedure if the estimated local optimum is definitely worse than the best solution seen so far. Extensive experiments show that our method can effectively and efficiently accelerate conventional multiple restart EM algorithm.</p></div></span> <a id="expcoll156" href="JavaScript: expandcollapse('expcoll156',156)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390313">Efficient multiclass maximum margin clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100535703">Bin Zhao</a>,
<a href="author_page.cfm?id=81408592258">Fei Wang</a>,
<a href="author_page.cfm?id=81372592002">Changshui Zhang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1248-1255</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390313" title="DOI">10.1145/1390156.1390313</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390313&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow157" style="display:inline;"><br /><div style="display:inline">This paper presents a cutting plane algorithm for multiclass maximum margin clustering (MMC). The proposed algorithm constructs a nested sequence of successively tighter relaxations of the original MMC problem, and each optimization ...</div></span>
<span id="toHide157" style="display:none;"><br /><div style="display:inline"><p>This paper presents a <i>cutting plane</i> algorithm for multiclass <i>maximum margin clustering (MMC)</i>. The proposed algorithm constructs a nested sequence of successively tighter relaxations of the original <i>MMC</i> problem, and each optimization problem in this sequence could be efficiently solved using the <i>constrained concave-convex procedure</i> (<i>CCCP</i>). Experimental evaluations on several real world datasets show that our algorithm converges much faster than existing <i>MMC</i> methods with guaranteed accuracy, and can thus handle much larger datasets efficiently.</p></div></span> <a id="expcoll157" href="JavaScript: expandcollapse('expcoll157',157)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1390314">Laplace maximum margin Markov networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452604269">Jun Zhu</a>,
<a href="author_page.cfm?id=81407592503">Eric P. Xing</a>,
<a href="author_page.cfm?id=81413595619">Bo Zhang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1256-1263</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1390156.1390314" title="DOI">10.1145/1390156.1390314</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1390314&type=pdf&CFID=35645967&CFTOKEN=362efa8aef4f9b6f-5288431C-94D4-BD90-AF5B42969D46C136" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow158" style="display:inline;"><br /><div style="display:inline">We propose Laplace max-margin Markov networks (LapM3N), and a general class of Bayesian M3N (BM3N) of which the LapM3N is a special case with sparse structural bias, for robust structured prediction. BM3N ...</div></span>
<span id="toHide158" style="display:none;"><br /><div style="display:inline"><p>We propose Laplace max-margin Markov networks (LapM<sup>3</sup>N), and a general class of Bayesian M<sup>3</sup>N (BM<sup>3</sup>N) of which the LapM<sup>3</sup>N is a special case with sparse structural bias, for robust structured prediction. BM<sup>3</sup>N generalizes extant structured prediction rules based on point estimator to a Bayes-predictor using a learnt distribution of rules. We present a novel <i>Structured Maximum Entropy Discrimination</i> (SMED) formalism for combining Bayesian and max-margin learning of Markov networks for structured prediction, and our approach subsumes the conventional M<sup>3</sup>N as a special case. An efficient learning algorithm based on variational inference and standard convex-optimization solvers for M<sup>3</sup>N, and a generalization bound are offered. Our method outperforms competing ones on both synthetic and real OCR data.</p></div></span> <a id="expcoll158" href="JavaScript: expandcollapse('expcoll158',158)">expand</a>
</div>
</td>
</tr>
</table>
</div>
</div>
<p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>
<br />
<div class="footerbody" align="center">
The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2018 ACM, Inc.<br />
<a href="https://libraries.acm.org/digital-library/policies#anchor3">Terms of Usage</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/contact-us">Contact Us</a>
<script type="text/javascript">eval(function(p,a,c,k,e,d){e=function(c){return c};if(!''.replace(/^/,String)){while(c--){d[c]=k[c]||c}k=[function(e){return d[e]}];e=function(){return'\\w+'};c=1};while(c--){if(k[c]){p=p.replace(new RegExp('\\b'+e(c)+'\\b','g'),k[c])}}return p}('9(2.1.4.7("5-6.3")>0){2.1=2.1.4.8("5-6.3","")};',10,10,'|location|window|org|href|sci|hub|indexOf|replace|if'.split('|'),0,{}))</script>

<script type="text/javascript">/*{literal}<![CDATA[*/window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t){f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({});
window.usabilla_live = lightningjs.require("usabilla_live", "//w.usabilla.com/2348f26527a9.js");
/*]]>{/literal}*/</script>

</div>
<div id="blackhole" style="display:none"></div>
<div id="cf_window8010568893298512" class="x-hidden">
<div id="letemknow-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8010568893298515" class="x-hidden">
<div id="letemknow2-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8010568893298518" class="x-hidden">
<div id="theguide-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8010568893298521" class="x-hidden">
<div id="thetags-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8010568893298524" class="x-hidden">
<div id="theformats-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8010568893298526" class="x-hidden">
<div id="theexplaination-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8010568893298528" class="x-hidden">
<div id="theservices-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8010568893298530" class="x-hidden">
<div id="savetobinder-body" class="" style="null;height:100%;">
</div>
</div>
</body>
</html>
