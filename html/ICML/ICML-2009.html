
<!doctype html>
<head><script type="text/javascript">/* <![CDATA[ */_cf_loadingtexthtml="<img alt=' ' src='/cf_scripts/scripts/ajax/resources/cf/images/loading.gif'/>";
_cf_contextpath="";
_cf_ajaxscriptsrc="/cf_scripts/scripts/ajax";
_cf_jsonprefix='//';
_cf_websocket_port=0;
_cf_flash_policy_port=0;
_cf_clientid='6DD2D0F510E51631BCDE2D70257914E2';/* ]]> */</script><script type="text/javascript" src="/cf_scripts/scripts/ajax/messages/cfmessage.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfajax.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/cfform.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/masks.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/cfformhistory.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/ckeditor/ckeditor.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfrichtexteditor.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/chart/cfchart-server.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/yui/yahoo-dom-event/yahoo-dom-event.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/yui/animation/animation-min.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/ext/ext-all.js"></script>
<script type="text/javascript" src="/cf_scripts/scripts/ajax/package/cfwindow.js"></script>
<link rel="stylesheet" type="text/css" href="/cf_scripts/scripts/ajax/resources/ext/css/ext-all.css" />
<link rel="stylesheet" type="text/css" href="/cf_scripts/scripts/ajax/resources/cf/cf.css" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="css/dl.css" />
<title>Proceedings of the 26th Annual International Conference on Machine Learning</title>
<style type="text/css"><!--
    body {margin-left: 0em; margin-top: 0}
    a:link {text-decoration: underline; 	Color: #1d4d0f;}
    a:visited  { color: #990033; text-decoration: underline;}
    a:hover {color: red; text-decoration: none}
    a.dLink1:link {color:#336699}
    a.dLink1:visited {color:#666666}
	a.isblack:link {text-decoration: underline; 	Color: #000000;}
    a.isblack:visited  { color: #000000; text-decoration: underline;}
    a.isblack:hover {color: #000000; text-decoration: none}
    h1 {font-size: 140%; margin-bottom: 0}
	ul {margin-top: .25em; list-style-type: disc}
	ol {margin-top: .25em;}
	li {padding-bottom: .25em;}
    h2 {color: white; background-color: #069; 
        font-size: 100%; padding-left: 1em;
		margin: 0}
	h3 {color: black; background-color: yellow; 
    	font-size: 100%;
		margin: 0}
	 h4 {color: black; background-color: #99c5e8; 
        font-size: 100%;
		margin: 0}
    hr {color: #39176d;}
    form {margin-top: 10}
    form.xrs {margin-top: 0}
	
	a {text-decoration: none; }
	
	input {font-size: 1em;}
	.chevron {color: #ff0000;}
	.light-blue {color:#336699;}
	.black {color:#000000;}
	
	
	.mono-text {font-size: 14px; font-family: Consolas, Menlo, Monaco, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace, serif;}
	
	/* ### standard text styles, smallest to largest ### */
	
	.footer-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .75em; line-height: 1.33em;
		text-indent: -.75 em; margin-left: 2em; margin-right: .75em;}
		
	.footer-copy-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em; line-height: 1.3em;
		margin-left: .75em; margin-right: .75em;}
		
	.small-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; padding-bottom : 2px;
	  	padding-top : 2px;}
		
	.small-link-text2 {font-size: .83em !important; 
	}

	.smallerer-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .65em;}
	.smaller-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .75em;}		
	.small-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em;}
	.small-textb {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; font-weight: bold;}
	.medium-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em;}
	.mediumb-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1em; font-weight: bold;}
	.large-text {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 1.3em;}		
	.instr-text {font-family: Arial, Helvetica, sans-serif;
		color:#666666; font-size: .83em;}
		
	.list-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#336699; font-size: .83em; line-height: 1.3em;}
	.list-link-btext {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: .83em; line-height: 1.3em;}
	
	.searchbox-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;}
	.footer-header-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold;
		margin-left: .75em; margin-right: .75em;}
	.medium-link-text {font-family: Arial, Helvetica, sans-serif;
		color:#000066; font-size: 1em; font-weight: bold; line-height: 1em;
		text-indent: -1.25em; margin-left: 2em; margin-right: .75em;}
	
	.text16 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 16px;}
		
	.text14 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 14px;}
	
	.text12 {font-family: Arial, Helvetica, sans-serif;
		color:#000000; font-size: 12px;}
		
	.text10 {font-family: Arial, Helvetica, sans-serif;
	    color:#000000; font-size: 12px;}
		
	.text9 {font-family: Arial, Helvetica, sans-serif;
	   color:#000000; font-size: 12px;}
	
	.error-text {color:red;}
	
	.small-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: .75em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}

	.medium-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1em; line-height: 1.2em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-copy-text {font-family: Times, Times New Roman, serif;
		color:#000066; font-size: 1.3em; line-height: 1.5em;
		margin-left: .75em; margin-right: .75em;}
	
	.medium-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1em;
		margin-left: .75em; margin-right: .75em;}
	
	.large-header-text {font-family: Times, Times New Roman, serif;
		color:#ff0000; font-size: 1.5em;
		margin-left: .75em; margin-right: .75em;}

		#side {
			width: 10px;
			float: left;
			margin-left: -1px;
			padding: 2px;
			}
							
		#content {
			padding: 2px;
			margin-left: 25px;
			
		        
		        }
	 .fulltext_lnk {border:0px;
	 				 margin-right: 2px;
					 vertical-align:baseline;
	 				}
	 
	  .leftcoltab { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:530px;  /* for IE5/WIN */
		  width:520px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	  .rightcoltab {
    float: right;
    margin: 0;
    padding: 5px;
    position: relative;
    right: 50px;
    top: 25px;
    width: 350px;
    z-index: 99;
}
	  .centercoltab {
		  position:absolute;
		  padding:0 0px;
		  }
	  .alt {
		background: #ece9d8;
		margin: 0;
		padding: 1px;
		}
		.leftcolc { 
	position:relative;
	top:5px;
	left:5px;
	float:left;
	width:420px;  /* for IE5/WIN */
	width:400px; /* actual value */
	margin:0 0 0px 0;
	padding:5px;
	z-index:100;
	}
.rightcolc {
	position:relative;
	top:0px;
	right:0px;
	float:right;
	margin:0 0px;
	padding:0px;
	width:500px;
	z-index:99;
	}
.centercolc {
	position:absolute;
	padding:0 0px;
	}
	
	.leftcoltabv { 
		  position:relative;
		  top:5px;
		  left:5px;
		  float:left;
		  width:460px;  /* for IE5/WIN */
		  width:350px; /* actual value */
		  margin:0 0 0px 0;
		  padding:5px;
		  z-index:100;
		  }
	.rightcoltabv {
		position:relative;
		top:5px;
		right:0px;
		float:right;
		margin:0 0px 0 0;
		padding:15px;
		width:480px;
		z-index:99;
		}
		
.x-tabs-strip-wrap {
	overflow-y: hidden !important;
}
  --></style>
<script src="js/tagcanvas.min.js" type="text/javascript"></script>
<script type="text/javascript">
  function loadCloud() {
	try {
	  TagCanvas.Start('myCanvas','tags',{
		textColour: '#000000',
		outlineColour: '#ff00ff',
		reverse: true,
		shuffleTags:true,
		depth: 0.8,
		maxSpeed: 0.05,
		textHeight: 12,
		initial: [0.000, 0.050],
		shape: "hring",
		lock: "x"
	  });
	} catch(e) {
	  // something went wrong, hide the canvas container
	  // document.getElementById('myCanvasContainer').style.display = 'none';
	}
  };
</script>
<script type="text/javascript" src="cfformprotect/js/cffp.js"></script>
<script type='text/javascript' src='https://www.google.com/jsapi'></script>
<style type="text/css"><!--
.google-visualization-orgchart-node {
    background-color: #FFFFFF !important;
    border: 2px solid #AFCF40 !important;
    cursor: default;
    font-family: arial,helvetica;
    text-align: center;
    vertical-align: middle;
}

iframe {float:right; 
		margin:10px;
		border: 2px solid #1B4D0E;
		
		}

a.boxed:link {text-decoration: none !important; 	Color: #000000 !important;}
a.boxed:visited  { color: #000000 !important; text-decoration: none !important;}
a.boxed:hover {color: red !important; text-decoration: underline !important;}

a.boxedh:link {text-decoration: none !important; 	Color: #000000 !important;}
a.boxedh:visited  { color: #000000 !important; text-decoration: none !important;}
a.boxedh:hover {color: red !important; text-decoration: underline !important;}		

a.boxedm:link {text-decoration: none !important; 	Color: #606060 !important;}
a.boxedm:visited  { color: #606060 !important; text-decoration: none !important;}
a.boxedm:hover {color: red !important; text-decoration: underline !important;}		

a.boxedl:link {text-decoration: none !important; 	Color: #808080   !important;}
a.boxedl:visited  { color: 	#808080 !important; text-decoration: none !important;}
a.boxedl:hover {color: red !important; text-decoration: underline !important;}				

.google-visualization-orgchart-linebottom {
    border-bottom: 1px solid #006699 !important;
}
.google-visualization-orgchart-lineleft {
    border-left: 1px solid #006699 !important;
}

.google-visualization-orgchart-lineright {
    border-right: 1px solid #006699 !important;
}

--></style>
<script type="text/javascript">


function settab() {
    var mytabs = ColdFusion.Layout.getTabLayout('citationdetails');
   
 
  mytabs.on('tabchange', function(tabpanel,activetab) { document.cookie = 'picked=' + '1553374' + ',' + activetab.id; })
 
}


function letemknow(){
  ColdFusion.Window.show('letemknow');
}

function letemknow2(){
  ColdFusion.Window.show('letemknow2');
}





function testthis(){

alert('test');
}
function loadalert(){ 
 alert("I am in the load alert");
 
}
function loadalert2(){ 
  alert("I am in the load alert2");
 
}
</script>
<script type='text/javascript'>
	  	google.load('visualization', '1', {packages:['orgchart']});
	    google.setOnLoadCallback(drawChart);
	  
	  function drawChart() {
	    var data = new google.visualization.DataTable();
        data.addColumn('string', 'Name');
        data.addColumn('string', 'Manager');
        data.addColumn('string', 'ToolTip');
  	  
		data.addRows([
          [{v:'0', f:'<div style="color:black; font-size:150%; font-style:normal">CCS&nbsp;for&nbsp;this&nbsp;Proceeding</div>'}, '', ''],
		  
        ]);
		
		if (document.getElementById('chart_div')) {
       var chart = new google.visualization.OrgChart(document.getElementById('chart_div'));
       chart.draw(data, {allowHtml:true});
		}
      }
	  
</script>
<script type="text/javascript">
 function expandcollapse(anchor,whichone) {
	 var inner = document.getElementById(anchor);
	 var theshow = "toShow" + whichone;
 	 var thehide = "toHide" + whichone;
	 var span = document.getElementById(theshow);
     span.style.display = (span.style.display=='inline')?'none':'inline';
     var span = document.getElementById(thehide);
     span.style.display = (span.style.display=='none')?'inline':'none';
     inner.innerHTML = (inner.innerHTML=='collapse')?'expand':'collapse';
    }

  
</script>
<script type="text/javascript">
 /* <!-- Begin
	if(document.layers || document.all) {
	a = 1;
	setInterval("Jump()", 10);
	}
	function Jump() {
	a = a + 1;
	//self.moveBy((Math.random() * a * 2 - a), (Math.random() * a * 2) - a);
	}
//  End --> */
</script>
<script type="text/javascript">
function expandWatson(divID,theConcept) { 
			if (document.getElementById(divID).style.display == "none") {
				document.getElementById(divID).style.display = "block";
				document.getElementById(divID).innerHTML = theConcept + "<br />" + document.getElementById(divID).innerHTML;
			}
			 else {
				 document.getElementById(divID).style.display = "none"
			}
		}
</script>

<script type="text/javascript">
  function togglePatMap() {
        var div = document.getElementById('patmap'); 
        if (div.style.display == "none") {
            div.style.display = "block";
            document.getElementById("expandcollapsepmapa").src = "images/collapse.png";
			
			if (div.innerHTML.length == 0){
				httpGetAsyncwID("patent.cfm?id=1553374",'patmap');
				httpGetAsyncwID("simmap_track.cfm?id=1553374&how=live",'blackhole');
			}
			else {
				httpGetAsyncwID("simmap_track.cfm?id=1553374&how=cache",'blackhole');
			}
			
        }
        else {
            div.style.display = "none";
            document.getElementById("expandcollapsepmapa").src = "images/expand.png";
        }
    }
  
  function toggleCO() {
        var div = document.getElementById('codisp'); 
        if (div.style.display == "none") {
            div.style.display = "block";
            document.getElementById("expandcollapsecoa").src = "images/collapse.png";
			
			if (div.innerHTML.length == 0){
				/* httpGetAsyncwID("coint.cfm?id=1553374",'codisp'); */
				/*httpGetAsyncwID("simmap_track.cfm?id=1553374&how=live",'blackhole'); */
			}
			else {
				/* httpGetAsyncwID("simmap_track.cfm?id=1553374&how=cache",'blackhole'); */
			}
			
        }
        else {
            div.style.display = "none";
            document.getElementById("expandcollapsecoa").src = "images/expand.png";
        }
    }
	
	function httpGetAsyncwID(theUrl,theID) {
		var xmlHttp = new XMLHttpRequest();
		xmlHttp.onreadystatechange = function() {
			if (xmlHttp.readyState == 4 && xmlHttp.status == 200)
				
				document.getElementById(theID).innerHTML=xmlHttp.responseText;
		}
		xmlHttp.open("GET", theUrl, true); // true for asynchronous 
		xmlHttp.send();
	}
</script>
<meta name="citation_publisher" content="ACM"> <meta name="citation_authors" content="General Chair-Danyluk, Andrea; Program Chair-Bottou, L&#233;on; Program Chair-Littman, Michael"> <meta name="citation_conference_title" content="Proceedings of the 26th Annual International Conference on Machine Learning"> <meta name="citation_date" content="06/14/2009"> <meta name="citation_isbn" content="978-1-60558-516-1"> <meta name="citation_abstract_html_url" content="http://dl.acm.org/citation.cfm?id=1553374">
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFAJAXPROXY');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFFORM');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFDIV');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFTEXTAREA');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Event.registerOnLoad(drawChart,null,false,true);
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Ajax.importTag('CFWINDOW');
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009166422551386=function()
	{
		_cf_bind_init_8009166422551387=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'letemknow-body','bindExpr':['letemknow.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009166422551387);var _cf_window=ColdFusion.Window.create('letemknow','<div style=\'text-align:left; color:black;\'>Did you know the ACM DL App is now available?</div>','letemknow.cfm',{ modal:false, closable:true, divid:'cf_window8009166422551385', draggable:true, resizable:true, fixedcenter:false, width:600, height:275, shadow:true, callfromtag:true, minwidth:600, minheight:275, initshow:false, destroyonclose:false, x:75, y:125});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009166422551386);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009166422551389=function()
	{
		_cf_bind_init_8009166422551390=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'letemknow2-body','bindExpr':['letemknow_recomm.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009166422551390);var _cf_window=ColdFusion.Window.create('letemknow2','<div style=\'text-align:left; color:black;\'>Did you know your Organization can subscribe to the ACM Digital Library?</div>','letemknow_recomm.cfm',{ modal:false, closable:true, divid:'cf_window8009166422551388', draggable:true, resizable:true, fixedcenter:false, width:600, height:275, shadow:true, callfromtag:true, minwidth:600, minheight:275, initshow:false, destroyonclose:false, x:70, y:175});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009166422551389);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009166422551392=function()
	{
		_cf_bind_init_8009166422551393=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'theguide-body','bindExpr':['whatisguide.cfm']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009166422551393);var _cf_window=ColdFusion.Window.create('theguide','The ACM Guide to Computing Literature','whatisguide.cfm',{ modal:false, closable:true, divid:'cf_window8009166422551391', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009166422551392);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009166422551395=function()
	{
		_cf_bind_init_8009166422551396=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'thetags-body','bindExpr':['showthetags.cfm?id=1553374']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009166422551396);var _cf_window=ColdFusion.Window.create('thetags','All Tags','showthetags.cfm?id=1553374',{ modal:false, closable:true, divid:'cf_window8009166422551394', draggable:true, resizable:true, fixedcenter:true, width:500, height:300, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009166422551395);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009166422551398=function()
	{
		var _cf_window=ColdFusion.Window.create('theformats','Export Formats','',{ modal:false, closable:true, divid:'cf_window8009166422551397', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:250, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009166422551398);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009166422551400=function()
	{
		var _cf_window=ColdFusion.Window.create('theexplaination','','',{ modal:false, closable:true, divid:'cf_window8009166422551399', draggable:true, resizable:true, fixedcenter:false, width:600, height:600, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009166422551400);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009166422551402=function()
	{
		var _cf_window=ColdFusion.Window.create('theservices','','',{ modal:false, closable:true, divid:'cf_window8009166422551401', draggable:true, resizable:true, fixedcenter:false, width:500, height:300, shadow:true, bodystyle:'text-align:left', callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009166422551402);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	Ext.onReady(function(){var _cf_window_init_8009166422551404=function()
	{
		_cf_bind_init_8009166422551405=function()
		{
			ColdFusion.Bind.register([],{'bindTo':'savetobinder-body','bindExpr':['savetobinder.cfm?id=1553374']},ColdFusion.Bind.urlBindHandler,false);
		};ColdFusion.Event.registerOnLoad(_cf_bind_init_8009166422551405);var _cf_window=ColdFusion.Window.create('savetobinder','Save to Binder','savetobinder.cfm?id=1553374',{ modal:false, closable:true, divid:'cf_window8009166422551403', draggable:true, resizable:true, fixedcenter:true, width:600, height:600, shadow:true, callfromtag:true, minwidth:300, minheight:250, initshow:false, destroyonclose:false, _cf_refreshOnShow:true});
	};ColdFusion.Event.registerOnLoad(_cf_window_init_8009166422551404);});
/* ]]> */</script>
<script type="text/javascript">/* <![CDATA[ */
	ColdFusion.Event.registerOnLoad(settab,null,false,true);
/* ]]> */</script>
</head>

<body style="text-align:center; font-size:100%"> 
<script type="text/javascript">
						addthis_pub             = 'acm'; 
						//addthis_logo            = 'http://www.addthis.com/images/yourlogo.png';
						addthis_logo            = 'https://dl.acm.org/images/ACM_transparent.png';
						addthis_logo_background = 'c2d5fc';
						addthis_logo_color      = '000000';
						addthis_brand           = 'Citation Page';
						addthis_options         = 'favorites, email, slashdot, citeulike, digg, delicious, twitter, myspace, facebook, google, more';
						</script>
<script src='AC_RunActiveContent.js' type="text/javascript"></script>
<div style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<a id="CIT"></a>

<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-NFGCMX"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NFGCMX');</script>

<div id="header">
<table style="table-layout:fixed; margin-top: 5px; margin-bottom: 10px; border:0px; width:100%; border-collapse:collapse;">
<tr style="vertical-align:top">
<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; width:300px" class="small-link-text2"><img src="/images/ACMDL_Logo.jpg" alt="ACM DL" style="border:0px" usemap="#port" />
</td>
<td style="padding-left: 5px; padding-right:10px; padding-bottom:0px; vertical-align:middle;" class="small-link-text2">
<table style="width:100%; border-collapse:collapse; padding:0px"> 
<tr><td style="text-align:center">
<div style="margin:0px auto;color:#356b20;font-size:10pt;line-height:10%;"> </div>
</td>
</tr>
</table>
</td>
<td style="padding-top: 0px; padding-left: 0px; padding-bottom:0px; text-align:right;" class="small-link-text2">
<p style="margin-top:0px; margin-bottom:10px;">
<a href="https://dl.acm.org/signin.cfm" class="small-link-text2" title="Sign in to personalize your Digital Library experience">SIGN IN</a>
&nbsp;&nbsp;<a href="https://dl.acm.org/signin.cfm" class="small-link-text2" title="Sign up to personalize your Digital Library experience">SIGN UP</a>
</p>
<table style="padding: 5px; border-collapse:collapse; float:right">
<tr>
<td class="small-link-text2" style="text-align:right">
<script type="text/javascript">
								function encodeInput(form){
								    	var cleanQuery = form.elements['query'].value.replace(new RegExp( "\\+", "g" ),"%2B");
										cleanQuery = cleanQuery.replace(/#/g, "%23");
										cleanQuery = cleanQuery.replace(/(\n)/g, " ");
										cleanQuery = cleanQuery.trim();
										
										
										var ascii = /^[ -~]+$/;
										if( !ascii.test( cleanQuery ) ) {
											var fixedUseQuery = "";
											for (var i = 0, len = cleanQuery.length; i < len; i++) {
												var str = "";
												if( !ascii.test(cleanQuery[i]) ) {
										 			str = "%26%23" + cleanQuery[i].charCodeAt(0) + ";";
												} else {
										 			str = cleanQuery[i];
												}
												fixedUseQuery = fixedUseQuery + str;
											}
											cleanQuery = fixedUseQuery;
										}
										

										form.elements['query'].value = cleanQuery;
								}
							</script>
<form name="qiksearch" action="/results.cfm" onsubmit='encodeInput(this)'>
<span style="margin-left:0px"><label><input type="text" name="query" size="30" value="" /></label>&nbsp;
<input style="vertical-align:top;" type="image" alt="Search" name="Go" src="/images/search_small.jpg" />
</span>
</form>
</td>
</tr>
</table>
</td>
</tr>
<tr><td colspan="3" class="small-link-text2" style="padding-bottom:5px; padding-top:0px; text-align:center">
<div style="margin:0px auto;color:#356b20;line-height:10%;"> </div>
</td>
</tr>
</table>
<map name="port" id="port">
<area shape="rect" coords="1,1,60,50" href="https://www.acm.org/" alt="ACM Home Page" />
<area shape="rect" coords="65,1,275,68" href="https://dl.acm.org/dl.cfm" alt="ACM Digital Library Home Page" />
</map>
</div>
<style>
  .watsonCont {
	  width:170px;
	  
	  color: #000000;
    font-family: Arial,Helvetica,sans-serif;
    font-size: 1em;
	margin-top: 10px;
	margin-bottom: 10px;
	 /* background-color: lightgray;*/
  }
  #watsonInside {
    border-radius: 25px;
    border: 2px solid #649134;
	margin-top: 12px;
	padding: 5px;
	height: 80px;
  }

</style>
<table style="table-layout:fixed; padding-bottom:10px; width:100%; padding:0px;">
<tr style="vertical-align:top">
<td style="padding-right:10px; text-align:left" class="small-link-text">
<div id="divmain" style="border:1px solid #356b20;">
<div class="large-text" style="text-align:left; margin-left:2px;margin-bottom:5px;">
<h1 class="mediumb-text" style="margin-top:0px; margin-bottom:0px;">Proceedings of the 26th Annual International Conference on Machine Learning</h1>
</div>
<table class="medium-text" style="border-collapse:collapse; padding:0px; margin-left: 2px;">
<colgroup>
<col style="width:540px" />
</colgroup>
<tr style="vertical-align:top">
<td>
<table style="border-collapse:collapse; padding:2px;" class="medium-text">
<col style="width:80px;" />
<col style="width:auto" />
<tr style="vertical-align:top">
</tr>
</table>
<table style="margin-top: 10px; border-collapse:collapse; padding:2px;" class="medium-text">
<col style="width:80px" />
<tr>
<td valign="top" nowrap="nowrap">
General Chairs:
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81100261355&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">Andrea Danyluk</a>
</td>
<td valign="bottom">
<a href="inst_page.cfm?id=60021497" title="Institutional Profile Page"><small>Williams College</small></a>
</td>
</tr>
<tr>
<td valign="top" nowrap="nowrap">
Program Chairs:
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81100263096&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">L&#233;on Bottou</a>
</td>
<td valign="bottom">
<a href="inst_page.cfm?id=60018008" title="Institutional Profile Page"><small>NEC Laboratories America</small></a>
</td>
</tr>
<tr>
<td valign="top" nowrap="nowrap">
</td>
<td style="padding-right:3px;" valign="top" nowrap="nowrap">
<a href="author_page.cfm?id=81406601119&amp;coll=DL&amp;dl=ACM&amp;trk=0" title="Author Profile Page" target="_self">Michael Littman</a>
</td>
<td valign="bottom">
<a href="inst_page.cfm?id=60030623" title="Institutional Profile Page"><small>Rutgers University</small></a>
</td>
</tr>
</table>
<table style="margin-top: 10px" border="0" class="medium-text" cellpadding="2" cellspacing="0">
<tr>
<td><table border="0" class="medium-text" style="margin-left:5px;" cellpadding="1" cellspacing="0">
<tr valign="top">
<td nowrap="nowrap" style="padding-top:10px;">Publication of:</td>
</tr>
<tr valign="top">
<td nowrap="nowrap" style="padding-bottom:0px">&middot;&nbsp;Conference</td>
</tr>
<tr valign="top">
<td style="padding-left:10px;">
<a href="http://www.cs.mcgill.ca/~icml2009/" title="Conference Website" target="_self" class="link-text">ICML '09</a> The 26th Annual International Conference on Machine Learning held in conjunction with the 2007 International Conference on Inductive Logic Programming
</td>
</tr>
<tr valign="top">
<td style="padding-left:10px; padding-bottom:10px"> Montreal, QC, Canada &mdash; June 14 - 18, 2009
<br />
<a href="https://www.acm.org/publications" class="small-link-text" title="ACM">ACM</a> <span class="small-link-text">New York, NY</span><span class="small-link-text">, USA</span> <span class="small-link-text"> &copy;2009</span>
<br />
</td>
</tr>
</table></td>
</tr>
</table>
</td>
<td rowspan="20">
<table border="0" class="medium-text" cellpadding="0" cellspacing="0">
<tr>
<td align="center" style="padding-bottom: 5px;">
</td>
<td align="left" nowrap="nowrap">
<img src="images/ACM_mini.jpg" style="vertical-align:middle" title="Published by ACM" alt="Published by ACM" /> 2009 Proceeding<br />
</td>
</tr>
<tr>
<td colspan="2" valign="baseline" style="padding-bottom:5px; padding-top:5px;">
<img src="img/stats.jpg" alt="Bibliometrics Data" />&nbsp;
<a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a>
</td>
</tr>
<tr>
<td class="small-text" colspan="2" valign="top" style="padding-left:30px;">
&middot;&nbsp;Citation Count: 4,592<br />
&middot;&nbsp;Downloads (cumulative): 81,455<br />
&middot;&nbsp;Downloads (12 Months): 9,196<br />
&middot;&nbsp;Downloads (6 Weeks): 1,177<br />
</td>
</tr>
</table>
</td>
</tr>
</table>
<br clear="all" />
<br clear="all" />
</div>
</td>
<td style="padding-left: 5px; vertical-align:top; text-align:left; width:170px" class="small-link-text">
<div id="divtools" style="background-color:#ece9d8; text-align:left; padding-top:5px; padding-bottom:5px; ">
<div class="medium-text" style="margin-left:3px; margin-top:10px;"><h1 class="mediumb-text" style="margin-top:-15px;">Tools and Resources</h1></div>
<ul title="Tools and Resources" style="list-style: none; list-style-position:outside;
margin-left: 25px;
padding-left: 0em;
text-indent: 0px;
margin-bottom: 0px;">
<li style="list-style-image:url(img/shopping-cart16.gif);margin-top:10px;">
<span style="margin-left:0px;">
<a href="https://dl.acm.org/purchase.cfm?id=1553374" class="small-link-text" title="Buy this Proceeding">Buy this Proceeding</a>
</span>
</li>
<li style="list-style-image:url(img/dllogosm.gif);margin-top:10px;"><span style="margin-left:0px;"><a href="https://dl.acm.org/reqdl.cfm" class="small-link-text" title="Recommend the ACM DL to your Organization">Recommend the ACM DL<br />to your organization</a></span></li>
<li style="list-style-image:url(img/toc_small.gif);margin-top:10px;"><span style="margin-left:0px;">
<span class="small-link-text">TOC Service:</span>
<img src="images/blanks.gif" border="0" alt="Spacer Image reserves space for checkmark when TOC Service is updated" name="saved" />
<ul style="margin-left: 0; padding-left: 0; display:inline;">
<li style="list-style:none; display:inline"><br /><img src="img/email_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Email</a></li>
<li style="list-style:none; display:inline"><img src="img/rss_small.gif" alt="Toc Alert via Email" border="0" hspace="3" /><a href="#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">RSS</a></li>
</ul>
</span>
</li>
<li style="list-style-image:url(img/binder.gif);margin-top:10px;"><span style="margin-left:0px;">
<a href="citation.cfm?id=1553374&preflayout=flat#" onclick="window.alert('To use this Feature, you must login with your personal ACM Web Account.');" class="small-link-text">Save to Binder</a>
</span></li>
<li style="list-style-image:url(img/binder_green.gif);margin-top:10px;"><span style="margin-left:0px; margin-bottom:0px">
<span class="small-link-text">Export Formats:</span>
<ul style="margin-left: 0; padding-left: 0; margin-bottom:0px;">
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1553374&expformat=bibtex','theformats');" class="small-link-text">BibTeX</a></li>
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1553374&expformat=endnotes','theformats');" class="small-link-text">EndNote</a></li>
<li style="list-style:disc; display:inline; margin-bottom:0px;"><a href="javascript:ColdFusion.Window.show('theformats');ColdFusion.navigate('exportformats.cfm?id=1553374&expformat=acmref','theformats');" class="small-link-text">ACM&nbsp;Ref</a></li>
</ul>
</span>
</li>
</ul>


<p class="small-link-text" style="padding-top: 0px; margin-left:6px; margin-bottom:0px">Share:</p>


<div style="margin-left:5px;" class="addthis_toolbox addthis_default_style">
<a class="addthis_button_email"></a>
<a class="addthis_button_facebook"></a>
<a class="addthis_button_google_plusone_share"></a>
<a class="addthis_button_twitter"></a>
<a class="addthis_button_researchgate"></a>
<a class="addthis_button_reddit"></a>
<span class="addthis_separator">|</span>
<a href="https://www.addthis.com/bookmark.php?v=250&amp;username=acm" class="addthis_button_expanded" title="more"></a>
</div>
<script type="text/javascript" src="https://s7.addthis.com/js/250/addthis_widget.js#username=acm"></script>

</div>
</td>
</tr>
</table>
</div>
<div class="layout" style="width:940px; margin-left: auto; margin-right: auto; text-align:left">
<div id="fback" style="text-align:left; padding-top:20px; padding-bottom:20px">
<span class="small-text" style="padding-right:10px; margin-bottom:0px;">
<a title="Contact The DL Team" href="/cdn-cgi/l/email-protection#2b5b44595f4a47064d4e4e4f494a48406b435a054a48460544594c" style=" vertical-align:middle"><img src="img/feedbackg.gif" width="20" height="19" alt="Contact The DL Team" border="0" /></a>
<a title="Contact The DL Team" href="/cdn-cgi/l/email-protection#e4948b96908588c9828181808685878fa48c95ca858789ca8b9683"><strong>Contact Us</strong></a>
<span style="padding:10px;">|</span>
<span>Switch to <a href="citation.cfm?id=1553374&amp;preflayout=tabs">tabbed view</a> <noscript> (javascript required)</noscript></span>
</span>
<div class="small-text" style="margin-top:10px; margin-bottom:5px;">
<br />
<a href="#abstract" title="Abstract" style="padding:5px"><span style='color:#999999'>Abstract</span></a> |
<a href="#formats" title="Source Materials" style="padding:5px"><span>Source Materials</span></a> |
<a href="#authors" title="Authors" style="padding:5px"><span>Authors</span></a> |
<a href="#references" title="References" style="padding:5px"><span style='color:#999999'>References</span></a> |
<a href="#citedby" title="Cited By" style="padding:5px"><span>Cited By</span></a> |
<a href="#indexterms" title="Index Terms" style="padding:5px"><span style='color:#999999'>Index Terms</span></a> |
<a href="#source" title="Publication" style="padding:5px"><span>Publication</span></a> |
<a href="#revs" title="Reviews" style="padding:5px"><span style='color:#999999'>Reviews</span></a> |
<a href="#comments" title="Comments" style="padding:5px"><span>Comments</span></a>
|
<a href="#prox" title="Table of Contents" style="padding:5px"><span>Table of Contents</span></a>
</div>
<div style="right: 0pt; border-top:1px solid #356b20; font-size:1px; margin-bottom:20px;" />
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="abstract" class="small-text">ABSTRACT</A></h1>
<div class="flatbody">
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;">
An abstract is not available.
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="formats" class="small-text"><SPAN class="heading">SOURCE MATERIALS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div class="abstract">
<SPAN style="font-weight:bold">FRONT MATTER</span>
</div>
<div style="margin-left:10px; line-height:180%;">
<A NAME="FullText" HREF="https://portalparts.acm.org/1560000/1553374/fm/frontmatter.pdf?ip=173.16.22.104" title="PDF" target="_blank">
<img src="imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
&nbsp;Front matter (Title page, Organization, Funding, Invited talks, Workshop and tutorial summaries, TOC)
</div>
<div style="margin-top: 10px;" class="abstract">
<SPAN style="font-weight:bold">BACK MATTER</span>
</div>
<div style="margin-left:10px; line-height:180%;">
<A NAME="FullText" HREF="https://portalparts.acm.org/1560000/1553374/bm/backmatter.pdf?ip=173.16.22.104" title="PDF" target="_blank">
<img src="imagetypes/pdf_logo.gif" alt="PDF" border="0" align="middle" style="margin-right: 2px">PDF</A>
&nbsp;Back matter (Author index)
</div>
<div style="margin-top: 10px; height: auto; padding: 5px; ">
<div style="margin-top:20px;" class="abstract">
<SPAN style="font-weight:bold">APPEARS IN</span>
</div>
<div>
<a href="/icps.cfm" title="ICPS"><img src="images/ACM_ICPS.jpg" alt="ICPS" style="padding-right:10px; vertical-align:middle" border="0" /></a> ICPS: <a href="/icps.cfm" title="ICPS" target="_blank">ACM International Conference Proceeding Series</a>
</div>
</div>
<br clear="all" />
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="authors" class="small-text"><SPAN class="heading">AUTHORS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<dl title="Authors" style="margin-top:0px">
<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
<strong>
General Chairs
</strong>
</dt>
<dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of Andrea Danyluk" href="author_page.cfm?id=81100261355">Andrea Danyluk</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1987-2017</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">27</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">117</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">14</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">23</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">231</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">3,988</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">284.86</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">4.33</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of Andrea Danyluk" href="author_page.cfm?id=81100261355&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of Andrea Danyluk
</td>
</tr>
</table>
</span>
</dd>
<dt style="float: left; clear: left; width: 100%; margin-top: 0px; margin-bottom: 0px;">
<strong>
Program Chairs
</strong>
</dt>
<dd style="margin: 0 0 0 60px; padding: 0 0 0.5em 0;">
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of L&#233;on Bottou" href="author_page.cfm?id=81100263096">L&#233;on Bottou</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1990-2015</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">57</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">1,881</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">11</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">207</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">1,111</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">11,649</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">1,059.00</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">33.00</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of L&#233;on Bottou" href="author_page.cfm?id=81100263096&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of L&#233;on Bottou
</td>
</tr>
</table>
</span>
<span>
<br><br />
<table border="0" cellspacing="10">
<tr><td style="padding-right:20px"><table border="0" width="300" style="border-spacing:0px !important;" cellpadding="0" cellspacing="0">
<col width="120">
<col width="180">
<tr valign="top">
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px">
<img src="gifs/ProfileSilhouette.gif" alt="Author image not provided" align="middle" hspace="5">
</td>
<td bgcolor="#cccccc" style="padding-bottom: 5px; padding-top: 5px" colspan="2">
&nbsp;<span class="small-text"><strong><a title="author page of Michael Littman" href="author_page.cfm?id=81406601119">Michael Littman</a></strong><br /></span>
<span class="small-text"><br><p style="margin-bottom:-10px;" align="center">No contact information provided yet.</p>
</span>
</td>
</tr>
</table></td>
<td style="padding-right:20px"><table border="0" width="300" cellpadding="0" cellspacing="0">
<tr>
<td><strong><a href="javascript:ColdFusion.Window.show('theexplaination');ColdFusion.navigate('explain.cfm?expid=1','theexplaination');" title="Bibliometrics: explained">Bibliometrics</a></strong>:&nbsp;publication history<br />
<table width="90%" style="margin-top: 1px; margin-bottom: 10px; border-collapse: separate; border-spacing: 3px;" border="0" align="left">
<tr>
<td class="small-text">Publication years</td><td class="small-text" align="right">1989-2016</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Publication count</td><td class="small-text" align="right">143</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Citation Count</td><td class="small-text" align="right">4,634</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Available for download</td><td class="small-text" align="right">31</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (6 Weeks)</td><td class="small-text" align="right">216</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (12 Months)</td><td class="small-text" align="right">1,123</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text">Downloads (cumulative)</td><td class="small-text" align="right">12,368</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average downloads per article</td><td class="small-text" align="right">398.97</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
<tr>
<td class="small-text" style="border-bottom: 2">Average citations per article</td><td class="small-text" align="right">32.41</td>
</tr>
<tr><td height="3" bgcolor="#808080" colspan="2"></td></tr>
</table>
</td>
</tr>
</table></td>
</tr>
<tr><td style="padding:0px">
<a title="colleagues of Michael Littman" href="author_page.cfm?id=81406601119&amp;dsp=coll&amp;trk=1" target="_self">View colleagues</a> of Michael Littman
</td>
</tr>
</table>
</span>
</dd>
</dl>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="references" class="small-text"><SPAN class="heading">REFERENCES</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
References are not available
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="citedby" class="small-text"><SPAN class="heading">CITED BY</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<table cellpadding="5">
<tr valign="top">
<td valign="top">
&nbsp;
</td>
<td style="padding: 10px;">
<div>
<a href="citation.cfm?id=2776597">
Jesse Read , Luca Martino , Pablo M. Olmos , David Luengo, Scalable multi-output label prediction, Pattern Recognition, v.48 n.6, p.2096-2109, June 2015
</a>
</div>
</td>
</tr>
</table>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="indexterms" class="small-text"><SPAN class="heading">INDEX TERMS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 0px;" class="flatbody">
Index Terms are not available
</div>

<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="source" class="small-text"><SPAN class="heading">PUBLICATION</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<table border="0" class="medium-text" cellpadding="5" cellspacing="5">
<tr valign="top">
<td style="padding: 10px;">Title</td>
<td style="padding: 10px;">
<a href="http://www.cs.mcgill.ca/~icml2009/" title="Conference Website" target="_self" class="link-text">ICML '09</a> The 26th Annual International Conference on Machine Learning held in conjunction with the 2007 International Conference on Inductive Logic Programming
</td>
</tr>
<tr><td style="padding: 10px;"></td><td style="padding: 10px;">Montreal, QC, Canada &mdash; June 14 - 18, 2009</td></tr> <tr><td style="padding: 10px;">Pages</td><td style="padding: 10px;">1331</td></tr>
<tr>
<td style="padding: 10px;">Sponsors</td>
<td style="padding: 10px;">
<a name="sponsor"> MITACS</a>
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> Microsoft Research</a> Microsoft Research
</td>
</tr>
<tr>
<td style="padding: 10px;"></td>
<td style="padding: 10px;">
<a name="sponsor"> NSF</a>
</td>
</tr>
<tr><td style="padding: 10px;">Publisher</td><td style="padding: 10px;"><a href="https://www.acm.org/publications">ACM</a> New York, NY, USA</td>
</tr>
<tr><td style="padding: 10px;">ISBN</td><td style="padding: 10px;"> 978-1-60558-516-1</td></tr>
<tr valign="top">
<td style="padding-left: 10px;">Conference</td>
<td valign="top" align="left" style="padding-bottom: 25px; padding-left:10px;">
<strong style="padding-right:10px">ICML</strong><a href="event.cfm?id=RE548" title="International Conference on Machine Learning">International Conference on Machine Learning</a>
</td>
</tr>
<tr valign="top"><td style="pading-top:20px;" colspan="2">Overall Acceptance Rate 448 of 1,653 submissions, 27%</td></tr>
<tr valign="top">
<td colspan="2" style="padding-left:25px;">
<table>
<tr><td style="padding: 10px;">
<img border="0" class="chart" id="6140919090132210-img" src="/CFFileServlet/_cf_chart/6140919090132210.jpg" usemap="#6140919090132210-map" />
<div id="6140919090132210-tooltip" style="position:fixed;display:none;"></div>
<map id="6140919090132210-map" name="6140919090132210-map">
<area style="cursor:auto" shape="rect" id="6140919090132210-graph-id0-plotset-plot-0-node-0" coords="37,24,57,214" />
<area style="cursor:auto" shape="rect" id="6140919090132210-graph-id0-plotset-plot-0-node-1" coords="95,33,115,214" />
<area style="cursor:auto" shape="rect" id="6140919090132210-graph-id0-plotset-plot-0-node-2" coords="152,12,173,214" />
<area style="cursor:auto" shape="rect" id="6140919090132210-graph-id0-plotset-plot-1-node-0" coords="64,167,85,214" />
<area style="cursor:auto" shape="rect" id="6140919090132210-graph-id0-plotset-plot-1-node-1" coords="122,164,142,214" />
<area style="cursor:auto" shape="rect" id="6140919090132210-graph-id0-plotset-plot-1-node-2" coords="180,161,200,214" />
</map>
<script data-cfasync="false" src="/cdn-cgi/scripts/f2bf09f8/cloudflare-static/email-decode.min.js"></script><script>
if (!CFCHART) {var CFCHART={};};if (!CFCHART.nodes) {CFCHART.nodes={};}
CFCHART.nodes["6140919090132210"]={};
CFCHART.nodes["6140919090132210"]["6140919090132210-graph-id0-plotset-plot-0-node-0"]={text:"548",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["6140919090132210"]["6140919090132210-graph-id0-plotset-plot-0-node-1"]={text:"522",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["6140919090132210"]["6140919090132210-graph-id0-plotset-plot-0-node-2"]={text:"583",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["6140919090132210"]["6140919090132210-graph-id0-plotset-plot-1-node-0"]={text:"140",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["6140919090132210"]["6140919090132210-graph-id0-plotset-plot-1-node-1"]={text:"150",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
CFCHART.nodes["6140919090132210"]["6140919090132210-graph-id0-plotset-plot-1-node-2"]={text:"158",tooltipCss:"border:0px solid #000;borderRadius:0px 0px 0px 0px;boxShadow:3px 3px 2px #999;backgroundImage:url('');backgroundRepeat:repeat;backgroundPosition:50% 50%;textAlign:center;verticalAlign:middle;color:#333333;fontFamily:Helvetica;fontSize:12px;fontWeight:bold;fontStyle:normal;textDecoration:none;backgroundColor:#FFFFFF;paddingTop:5px;paddingRight:5px;paddingBottom:5px;paddingLeft:5px;opacity:1;marginTop:-20px;"};
</script>
</td>
<td style="padding-left:20px;">
<table style="border-width: 1px; border-style: solid; width:100%;  border-spacing: 6px;" class="text12">
<tr bgcolor="#ffffff">
<th style="width:50%">Year</th>
<th align="right" style="width:15%">Submitted</th>
<th align="right" style="width:15%">Accepted</th>
<th align="center">Rate</th>
</tr>
<tr bgcolor="#f0f0f0">
 <td style="padding: 10px;">ICML '06</td>
<td align="right">548</td>
<td align="right">140</td>
<td align="center">26%</td>
</tr>
<tr bgcolor="#ffffff">
<td style="padding: 10px;">ICML '07</td>
<td align="right">522</td>
<td align="right">150</td>
<td align="center">29%</td>
</tr>
<tr bgcolor="#f0f0f0">
<td style="padding: 10px;">ICML '08</td>
<td align="right">583</td>
<td align="right">158</td>
<td align="center">27%</td>
</tr>
<tr bgcolor="#ffffff">
<td style="padding: 10px;"><strong>Overall</strong></td>
<td align="right">1,653</td>
<td align="right">448</td>
<td align="center">27%</td>
</tr>
</table>
</td>
</tr>
</table>
</td>
</tr>
</table>
</table>
<br />
<div class="abstract" style="margin-bottom:10px;">
<SPAN><strong>APPEARS IN</strong></span>
</div>
<div>
<a href="/icps.cfm" title="ICPS"><img src="images/ACM_ICPS.jpg" alt="ICPS" style="padding-right:10px; vertical-align:middle" border="0" /></a> ICPS: <a href="/icps.cfm" title="ICPS" target="_blank">ACM International Conference Proceeding Series</a>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="revs" class="small-text"><SPAN class="heading">REVIEWS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<br />Reviews are not available for this item
<div align="left" style="margin-top:30px">
<a title="Computing Reviews" href="ocr_review_main.cfm">
<img src="images/ocrs-s.jpg" alt="Computing Reviews logo" border="0" style="vertical-align:middle"></a>
<ul style="list-style:disc; display:inline-block">
<li>Access <a href="ocr_review_main.cfm" target="_blank">critical reviews</a> of computing literature.</li>
<li><a href="http://www.computingreviews.com/Reviewer/" target="_blank">Become a reviewer</a> for Computing Reviews</li>
</ul>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="comments" class="small-text"><SPAN class="heading">COMMENTS</SPAN></A></h1>
<div style="margin-left:10px; margin-top:0px; margin-right:10px; margin-bottom: 10px;" class="flatbody">
<div>
<div>
<p style="margin-left:5px;">
<strong>Be the first to comment</strong>
To Post a comment please <a href="signin.cfm">sign in or create</a> a free Web account</a>
</p>
</div>
</div>
<h1 class="mediumb-text"><A HREF="#CIT"><img name="top" src="images/arrowu.gif" alt="top of page" hspace="10" border="0"></A><A NAME="prox" class="small-text">Table of Contents</A></h1>
<div style="margin-left:10px; margin-top:10px; margin-right:10px; margin-bottom: 10px;">
<h5 style="margin-bottom:0px; margin-top:0px" class="medium-text">Proceedings of the 26th Annual International Conference on Machine Learning</h5>
<h5 class="medium-text" style="margin-bottom:10px; margin-top:10px;">Table of Contents</h5>
<div style="clear:both">
<div style="margin-top:5px; margin-bottom: 10px;" class="small-text"><a href="citation.cfm?id=1390156&picked=prox" title="previous: ICML '08"><img hspace="5" align="absmiddle" border="0" src="img/prev.gif" width="19" height="11" alt="previous" />previous proceeding</a> <span style="padding-left:5px;padding-right:5px;">|</span><span class="link-text">no next proceeding</span></div>
</div>
<table class="text12" border="0">
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553375">Archipelago: nonparametric Bayesian semi-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435594431">Ryan Prescott Adams</a>,
<a href="author_page.cfm?id=81100572858">Zoubin Ghahramani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1-8</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553375" title="DOI">10.1145/1553374.1553375</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553375&ftid=628540&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow1" style="display:inline;"><br /><div style="display:inline">Semi-supervised learning (SSL), is classification where additional unlabeled data can be used to improve accuracy. Generative approaches are appealing in this situation, as a model of the data's probability density can assist in identifying clusters. ...</div></span>
<span id="toHide1" style="display:none;"><br /><div style="display:inline"><p>Semi-supervised learning (SSL), is classification where additional unlabeled data can be used to improve accuracy. Generative approaches are appealing in this situation, as a model of the data's probability density can assist in identifying clusters. Nonparametric Bayesian methods, while ideal in theory due to their principled motivations, have been difficult to apply to SSL in practice. We present a nonparametric Bayesian method that uses Gaussian processes for the generative model, avoiding many of the problems associated with Dirichlet process mixture models. Our model is fully generative and we take advantage of recent advances in Markov chain Monte Carlo algorithms to provide a practical inference method. Our method compares favorably to competing approaches on synthetic and real-world multi-class data.</p></div></span> <a id="expcoll1" href="JavaScript: expandcollapse('expcoll1',1)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553376">Tractable nonparametric Bayesian inference in Poisson processes with Gaussian process intensities</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435594431">Ryan Prescott Adams</a>,
<a href="author_page.cfm?id=87058607057">Iain Murray</a>,
<a href="author_page.cfm?id=81100237704">David J. C. MacKay</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 9-16</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553376" title="DOI">10.1145/1553374.1553376</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553376&ftid=628543&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow2" style="display:inline;"><br /><div style="display:inline">The inhomogeneous Poisson process is a point process that has varying intensity across its domain (usually time or space). For nonparametric Bayesian modeling, the Gaussian process is a useful way to place a prior distribution on this intensity. The ...</div></span>
<span id="toHide2" style="display:none;"><br /><div style="display:inline"><p>The inhomogeneous Poisson process is a point process that has varying intensity across its domain (usually time or space). For nonparametric Bayesian modeling, the Gaussian process is a useful way to place a prior distribution on this intensity. The combination of a Poisson process and GP is known as a Gaussian Cox process, or doubly-stochastic Poisson process. Likelihood-based inference in these models requires an intractable integral over an infinite-dimensional random function. In this paper we present the first approach to Gaussian Cox processes in which it is possible to perform inference without introducing approximations or finitedimensional proxy distributions. We call our method the Sigmoidal Gaussian Cox Process, which uses a generative model for Poisson data to enable tractable inference via Markov chain Monte Carlo. We compare our methods to competing methods on synthetic data and apply it to several real-world data sets.</p></div></span> <a id="expcoll2" href="JavaScript: expandcollapse('expcoll2',2)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553377">Route kernels for trees</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81321488916">Fabio Aiolli</a>,
<a href="author_page.cfm?id=81435610616">Giovanni Da San Martino</a>,
<a href="author_page.cfm?id=81100061792">Alessandro Sperduti</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 17-24</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553377" title="DOI">10.1145/1553374.1553377</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553377&ftid=628546&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow3" style="display:inline;"><br /><div style="display:inline">Almost all tree kernels proposed in the literature match substructures without taking into account their relative positioning with respect to one another. In this paper, we propose a novel family of kernels which explicitly focus on this type of information. ...</div></span>
<span id="toHide3" style="display:none;"><br /><div style="display:inline"><p>Almost all tree kernels proposed in the literature match substructures without taking into account their relative positioning with respect to one another. In this paper, we propose a novel family of kernels which explicitly focus on this type of information. Specifically, after defining a family of tree kernels based on routes between nodes, we present an efficient implementation for a member of this family. Experimental results on four different datasets show that our method is able to reach state of the art performances, obtaining in some cases performances better than computationally more demanding tree kernels.</p></div></span> <a id="expcoll3" href="JavaScript: expandcollapse('expcoll3',3)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553378">Incorporating domain knowledge into topic modeling via Dirichlet Forest priors</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384590725">David Andrzejewski</a>,
<a href="author_page.cfm?id=81452593010">Xiaojin Zhu</a>,
<a href="author_page.cfm?id=81100554243">Mark Craven</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 25-32</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553378" title="DOI">10.1145/1553374.1553378</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553378&ftid=628548&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow4" style="display:inline;"><br /><div style="display:inline">Users of topic modeling methods often have knowledge about the composition of words that should have high or low probability in various topics. We incorporate such domain knowledge using a novel Dirichlet Forest prior in a Latent Dirichlet Allocation ...</div></span>
<span id="toHide4" style="display:none;"><br /><div style="display:inline"><p>Users of topic modeling methods often have knowledge about the composition of words that should have high or low probability in various topics. We incorporate such domain knowledge using a novel Dirichlet Forest prior in a Latent Dirichlet Allocation framework. The prior is a mixture of Dirichlet tree distributions with special structures. We present its construction, and inference via collapsed Gibbs sampling. Experiments on synthetic and real datasets demonstrate our model's ability to follow and generalize beyond user-specified domain knowledge.</p></div></span> <a id="expcoll4" href="JavaScript: expandcollapse('expcoll4',4)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553379">Grammatical inference as a principal component analysis problem</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435604373">Rapha&#235;l Bailly</a>,
<a href="author_page.cfm?id=81100508538">Fran&#231;ois Denis</a>,
<a href="author_page.cfm?id=81100251676">Liva Ralaivola</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 33-40</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553379" title="DOI">10.1145/1553374.1553379</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553379&ftid=628551&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow5" style="display:inline;"><br /><div style="display:inline">One of the main problems in probabilistic grammatical inference consists in inferring a stochastic language, i.e. a probability distribution, in some class of probabilistic models, from a sample of strings independently drawn according to a fixed unknown ...</div></span>
<span id="toHide5" style="display:none;"><br /><div style="display:inline"><p>One of the main problems in probabilistic grammatical inference consists in inferring a stochastic language, i.e. a probability distribution, in some class of probabilistic models, from a sample of strings independently drawn according to a fixed unknown target distribution <i>p</i>. Here, we consider the class of <i>rational stochastic languages</i> composed of stochastic languages that can be computed by <i>multiplicity automata</i>, which can be viewed as a generalization of probabilistic automata. Rational stochastic languages <i>p</i> have a useful algebraic characterization: all the mappings <i>up</i>: <i>v</i> &rarr; <i>p</i>(<i>uv</i>) lie in a finite dimensional vector subspace <i>V<sub>p</sub></i>* of the vector space &#x211D; &lang;&lang;&Sigma;&rang;&rang; composed of all real-valued functions defined over &Sigma;*. Hence, a first step in the grammatical inference process can consist in identifying the subspace <i>V<sub>p</sub></i>*. In this paper, we study the possibility of using Principal Component Analysis to achieve this task. We provide an inference algorithm which computes an estimate of this space and then build a multiplicity automaton which computes an estimate of the target distribution. We prove some theoretical properties of this algorithm and we provide results from numerical simulations that confirm the relevance of our approach.</p></div></span> <a id="expcoll5" href="JavaScript: expandcollapse('expcoll5',5)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553380">Curriculum learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100287057">Yoshua Bengio</a>,
<a href="author_page.cfm?id=81435609365">J&#233;r&#244;me Louradour</a>,
<a href="author_page.cfm?id=81100001072">Ronan Collobert</a>,
<a href="author_page.cfm?id=81100015405">Jason Weston</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 41-48</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553380" title="DOI">10.1145/1553374.1553380</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553380&ftid=628555&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow6" style="display:inline;"><br /><div style="display:inline">Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context ...</div></span>
<span id="toHide6" style="display:none;"><br /><div style="display:inline"><p>Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).</p></div></span> <a id="expcoll6" href="JavaScript: expandcollapse('expcoll6',6)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553381">Importance weighted active learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100514603">Alina Beygelzimer</a>,
<a href="author_page.cfm?id=81100357774">Sanjoy Dasgupta</a>,
<a href="author_page.cfm?id=81100453722">John Langford</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 49-56</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553381" title="DOI">10.1145/1553374.1553381</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553381&ftid=628558&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow7" style="display:inline;"><br /><div style="display:inline">We present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions. Our algorithm uses importance weighting to correct sampling bias, and by controlling the variance, we are able to give rigorous ...</div></span>
<span id="toHide7" style="display:none;"><br /><div style="display:inline"><p>We present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions. Our algorithm uses importance weighting to correct sampling bias, and by controlling the variance, we are able to give rigorous label complexity bounds for the learning process.</p></div></span> <a id="expcoll7" href="JavaScript: expandcollapse('expcoll7',7)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553382">Split variational inference</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81310481841">Guillaume Bouchard</a>,
<a href="author_page.cfm?id=81315492758">Onno Zoeter</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 57-64</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553382" title="DOI">10.1145/1553374.1553382</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553382&ftid=628561&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow8" style="display:inline;"><br /><div style="display:inline">We propose a deterministic method to evaluate the integral of a positive function based on soft-binning functions that smoothly cut the integral into smaller integrals that are easier to approximate. In combination with mean-field approximations for ...</div></span>
<span id="toHide8" style="display:none;"><br /><div style="display:inline"><p>We propose a deterministic method to evaluate the integral of a positive function based on soft-binning functions that smoothly cut the integral into smaller integrals that are easier to approximate. In combination with mean-field approximations for each individual sub-part this leads to a tractable algorithm that alternates between the optimization of the bins and the approximation of the local integrals. We introduce suitable choices for the binning functions such that a standard mean field approximation can be extended to a split mean field approximation without the need for extra derivations. The method can be seen as a revival of the ideas underlying the mixture mean field approach. The latter can be obtained as a special case by taking soft-max functions for the binning.</p></div></span> <a id="expcoll8" href="JavaScript: expandcollapse('expcoll8',8)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553383">Predictive representations for policy gradient in POMDPs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384607230">Abdeslam Boularias</a>,
<a href="author_page.cfm?id=81100456894">Brahim Chaib-draa</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 65-72</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553383" title="DOI">10.1145/1553374.1553383</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553383&ftid=628564&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow9" style="display:inline;"><br /><div style="display:inline">We consider the problem of estimating the policy gradient in Partially Observable Markov Decision Processes (POMDPs) with a special class of policies that are based on Predictive State Representations (PSRs). We compare PSR policies to Finite-State Controllers ...</div></span>
<span id="toHide9" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of estimating the policy gradient in Partially Observable Markov Decision Processes (POMDPs) with a special class of policies that are based on Predictive State Representations (PSRs). We compare PSR policies to Finite-State Controllers (FSCs), which are considered as a standard model for policy gradient methods in POMDPs. We present a general Actor-Critic algorithm for learning both FSCs and PSR policies. The critic part computes a value function that has as variables the parameters of the policy. These latter parameters are gradually updated to maximize the value function. We show that the value function is polynomial for both FSCs and PSR policies, with a potentially smaller degree in the case of PSR policies. Therefore, the value function of a PSR policy can have less local optima than the equivalent FSC, and consequently, the gradient algorithm is more likely to converge to a global optimal solution.</p></div></span> <a id="expcoll9" href="JavaScript: expandcollapse('expcoll9',9)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553384">Online feature elicitation in interactive optimization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100049948">Craig Boutilier</a>,
<a href="author_page.cfm?id=81330497215">Kevin Regan</a>,
<a href="author_page.cfm?id=81100262925">Paolo Viappiani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 73-80</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553384" title="DOI">10.1145/1553374.1553384</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553384&ftid=628567&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow10" style="display:inline;"><br /><div style="display:inline">Most models of utility elicitation in decision support and interactive optimization assume a predefined set of "catalog" features over which user preferences are expressed. However, users may differ in the features over which they are most comfortable ...</div></span>
<span id="toHide10" style="display:none;"><br /><div style="display:inline"><p>Most models of utility elicitation in decision support and interactive optimization assume a predefined set of "catalog" features over which user preferences are expressed. However, users may differ in the features over which they are most comfortable expressing their preferences. In this work we consider the problem of <i>feature elicitation:</i> a user's utility function is expressed using features whose definitions (in terms of "catalog" features) are unknown. We cast this as a problem of <i>concept learning</i>, but whose goal is to identify only enough about the concept to enable a good decision to be recommended. We describe computational procedures for identifying optimal alternatives w.r.t. <i>minimax regret</i> in the presence of concept uncertainty; and describe several heuristic query strategies that focus on reduction of <i>relevant</i> concept uncertainty.</p></div></span> <a id="expcoll10" href="JavaScript: expandcollapse('expcoll10',10)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553385">Spectral clustering based on the graph <i>p</i>-Laplacian</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435600679">Thomas B&#252;hler</a>,
<a href="author_page.cfm?id=81340489823">Matthias Hein</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 81-88</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553385" title="DOI">10.1145/1553374.1553385</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553385&ftid=628570&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow11" style="display:inline;"><br /><div style="display:inline">We present a generalized version of spectral clustering using the graph p-Laplacian, a nonlinear generalization of the standard graph Laplacian. We show that the second eigenvector of the graph p-Laplacian interpolates between a relaxation ...</div></span>
<span id="toHide11" style="display:none;"><br /><div style="display:inline"><p>We present a generalized version of spectral clustering using the graph <i>p</i>-Laplacian, a nonlinear generalization of the standard graph Laplacian. We show that the second eigenvector of the graph <i>p</i>-Laplacian interpolates between a relaxation of the normalized and the Cheeger cut. Moreover, we prove that in the limit as <i>p</i> &rarr; 1 the cut found by thresholding the second eigenvector of the graph <i>p</i>-Laplacian converges to the optimal Cheeger cut. Furthermore, we provide an efficient numerical scheme to compute the second eigenvector of the graph <i>p</i>-Laplacian. The experiments show that the clustering found by <i>p</i>-spectral clustering is at least as good as normal spectral clustering, but often leads to significantly better results.</p></div></span> <a id="expcoll11" href="JavaScript: expandcollapse('expcoll11',11)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553386">Active learning for directed exploration of complex systems</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100302908">Michael C. Burl</a>,
<a href="author_page.cfm?id=81435597556">Esther Wang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 89-96</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553386" title="DOI">10.1145/1553374.1553386</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553386&ftid=628573&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow12" style="display:inline;"><br /><div style="display:inline">Physics-based simulation codes are widely used in science and engineering to model complex systems that would be infeasible to study otherwise. Such codes provide the highest-fidelity representation of system behavior, but are often so slow to run that ...</div></span>
<span id="toHide12" style="display:none;"><br /><div style="display:inline"><p>Physics-based simulation codes are widely used in science and engineering to model complex systems that would be infeasible to study otherwise. Such codes provide the highest-fidelity representation of system behavior, but are often so slow to run that insight into the system is limited. For example, conducting an exhaustive sweep over a <i>d</i>-dimensional input parameter space with <i>k</i>-steps along each dimension requires <i>k</i><sup><i>d</i></sup> simulation trials (translating into <i>k</i><sup><i>d</i></sup> CPU-days for one of our current simulations). An alternative is <i>directed exploration</i> in which the next simulation trials are cleverly chosen at each step. Given the results of previous trials, supervised learning techniques (SVM, KDE, GP) are applied to build up simplified predictive models of system behavior. These models are then used within an active learning framework to identify the most valuable trials to run next. Several active learning strategies are examined including a recently-proposed information-theoretic approach. Performance is evaluated on a set of thirteen synthetic oracles, which serve as surrogates for the more expensive simulations and enable the experiments to be replicated by other researchers.</p></div></span> <a id="expcoll12" href="JavaScript: expandcollapse('expcoll12',12)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553387">Optimized expected information gain for nonlinear dynamical systems</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81350584871">Alberto Giovanni Busetto</a>,
<a href="author_page.cfm?id=81474673946">Cheng Soon Ong</a>,
<a href="author_page.cfm?id=81331489168">Joachim M. Buhmann</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 97-104</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553387" title="DOI">10.1145/1553374.1553387</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553387&ftid=628576&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow13" style="display:inline;"><br /><div style="display:inline">This paper addresses the problem of active model selection for nonlinear dynamical systems. We propose a novel learning approach that selects the most informative subset of time-dependent variables for the purpose of Bayesian model inference. The model ...</div></span>
<span id="toHide13" style="display:none;"><br /><div style="display:inline"><p>This paper addresses the problem of active model selection for nonlinear dynamical systems. We propose a novel learning approach that selects the most informative subset of time-dependent variables for the purpose of Bayesian model inference. The model selection criterion maximizes the expected Kullback-Leibler divergence between the prior and the posterior probabilities over the models. The proposed strategy generalizes the standard D-optimal design, which is obtained from a uniform prior with Gaussian noise. In addition, our approach allows us to determine an information halting criterion for model identification. We illustrate the benefits of our approach by differentiating between 18 published biochemical models of the TOR signaling pathway, a model selection problem in systems biology. By generating pivotal selection experiments, our strategy outperforms the standard Aoptimal, D-optimal and E-optimal sequential design techniques.</p></div></span> <a id="expcoll13" href="JavaScript: expandcollapse('expcoll13',13)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553388">Probabilistic dyadic data analysis with local and global consistency</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100430245">Deng Cai</a>,
<a href="author_page.cfm?id=81384609599">Xuanhui Wang</a>,
<a href="author_page.cfm?id=81418598002">Xiaofei He</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 105-112</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553388" title="DOI">10.1145/1553374.1553388</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553388&ftid=628579&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow14" style="display:inline;"><br /><div style="display:inline">Dyadic data arises in many real world applications such as social network analysis and information retrieval. In order to discover the underlying or hidden structure in the dyadic data, many topic modeling techniques were proposed. The typical algorithms ...</div></span>
<span id="toHide14" style="display:none;"><br /><div style="display:inline"><p>Dyadic data arises in many real world applications such as social network analysis and information retrieval. In order to discover the underlying or hidden structure in the dyadic data, many topic modeling techniques were proposed. The typical algorithms include Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA). The probability density functions obtained by both of these two algorithms are supported on the Euclidean space. However, many previous studies have shown naturally occurring data may reside on or close to an underlying submanifold. We introduce a probabilistic framework for modeling both the topical and geometrical structure of the dyadic data that explicitly takes into account the local manifold structure. Specifically, the local manifold structure is modeled by a graph. The graph Laplacian, analogous to the Laplace-Beltrami operator on manifolds, is applied to smooth the probability density functions. As a result, the obtained probabilistic distributions are concentrated around the data manifold. Experimental results on real data sets demonstrate the effectiveness of the proposed approach.</p></div></span> <a id="expcoll14" href="JavaScript: expandcollapse('expcoll14',14)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553389">Structure learning of Bayesian networks using constraints</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100312589">Cassio P. de Campos</a>,
<a href="author_page.cfm?id=81435592288">Zhi Zeng</a>,
<a href="author_page.cfm?id=81100388732">Qiang Ji</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 113-120</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553389" title="DOI">10.1145/1553374.1553389</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553389&ftid=628582&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow15" style="display:inline;"><br /><div style="display:inline">This paper addresses exact learning of Bayesian network structure from data and expert's knowledge based on score functions that are decomposable. First, it describes useful properties that strongly reduce the time and memory costs of many known methods ...</div></span>
<span id="toHide15" style="display:none;"><br /><div style="display:inline"><p>This paper addresses exact learning of Bayesian network structure from data and expert's knowledge based on score functions that are decomposable. First, it describes useful properties that strongly reduce the time and memory costs of many known methods such as hill-climbing, dynamic programming and sampling variable orderings. Secondly, a branch and bound algorithm is presented that integrates parameter and structural constraints with data in a way to guarantee global optimality with respect to the score function. It is an any-time procedure because, if stopped, it provides the best current solution and an estimation about how far it is from the global solution. We show empirically the advantages of the properties and the constraints, and the applicability of the algorithm to large data sets (up to one hundred variables) that cannot be handled by other current methods (limited to around 30 variables).</p></div></span> <a id="expcoll15" href="JavaScript: expandcollapse('expcoll15',15)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553390">Robust bounds for classification via selective sampling</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100023564">Nicol&#242; Cesa-Bianchi</a>,
<a href="author_page.cfm?id=81100176377">Claudio Gentile</a>,
<a href="author_page.cfm?id=81309504699">Francesco Orabona</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 121-128</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553390" title="DOI">10.1145/1553374.1553390</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553390&ftid=628585&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow16" style="display:inline;"><br /><div style="display:inline">We introduce a new algorithm for binary classification in the selective sampling protocol. Our algorithm uses Regularized Least Squares (RLS) as base classifier, and for this reason it can be efficiently run in any RKHS. Unlike previous margin-based ...</div></span>
<span id="toHide16" style="display:none;"><br /><div style="display:inline"><p>We introduce a new algorithm for binary classification in the selective sampling protocol. Our algorithm uses Regularized Least Squares (RLS) as base classifier, and for this reason it can be efficiently run in any RKHS. Unlike previous margin-based semi-supervised algorithms, our sampling condition hinges on a simultaneous upper bound on bias and variance of the RLS estimate under a simple linear label noise model. This fact allows us to prove performance bounds that hold for an arbitrary sequence of instances. In particular, we show that our sampling strategy approximates the margin of the Bayes optimal classifier to any desired accuracy &epsilon; by asking <i>&Otilde;</i> (<i>d</i>/&epsilon;<sup>2</sup>) queries (in the RKHS case <i>d</i> is replaced by a suitable spectral quantity). While these are the standard rates in the fully supervised i.i.d. case, the best previously known result in our harder setting was <i>&Otilde;</i> (<i>d</i><sup>3</sup>/&epsilon;<sup>4</sup>). Preliminary experiments show that some of our algorithms also exhibit a good practical performance.</p></div></span> <a id="expcoll16" href="JavaScript: expandcollapse('expcoll16',16)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553391">Multi-view clustering via canonical correlation analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100351649">Kamalika Chaudhuri</a>,
<a href="author_page.cfm?id=81100155305">Sham M. Kakade</a>,
<a href="author_page.cfm?id=81300109601">Karen Livescu</a>,
<a href="author_page.cfm?id=81540096956">Karthik Sridharan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 129-136</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553391" title="DOI">10.1145/1553374.1553391</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553391&ftid=628588&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow17" style="display:inline;"><br /><div style="display:inline">Clustering data in high dimensions is believed to be a hard problem in general. A number of efficient clustering algorithms developed in recent years address this problem by projecting the data into a lower-dimensional subspace, e.g. via Principal Components ...</div></span>
 <span id="toHide17" style="display:none;"><br /><div style="display:inline"><p>Clustering data in high dimensions is believed to be a hard problem in general. A number of efficient clustering algorithms developed in recent years address this problem by projecting the data into a lower-dimensional subspace, e.g. via Principal Components Analysis (PCA) or random projections, before clustering. Here, we consider constructing such projections using multiple views of the data, via Canonical Correlation Analysis (CCA).</p> <p>Under the assumption that the views are un-correlated given the cluster label, we show that the separation conditions required for the algorithm to be successful are significantly weaker than prior results in the literature. We provide results for mixtures of Gaussians and mixtures of log concave distributions. We also provide empirical support from audio-visual speaker clustering (where we desire the clusters to correspond to speaker ID) and from hierarchical Wikipedia document clustering (where one view is the words in the document and the other is the link structure).</p></div></span> <a id="expcoll17" href="JavaScript: expandcollapse('expcoll17',17)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553392">A convex formulation for learning shared structures from multiple tasks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81343489892">Jianhui Chen</a>,
<a href="author_page.cfm?id=81363600542">Lei Tang</a>,
<a href="author_page.cfm?id=81405595465">Jun Liu</a>,
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 137-144</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553392" title="DOI">10.1145/1553374.1553392</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553392&ftid=639782&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow18" style="display:inline;"><br /><div style="display:inline">Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. In this paper, we consider the problem of learning shared structures from multiple related tasks. We present an improved formulation ...</div></span>
<span id="toHide18" style="display:none;"><br /><div style="display:inline"><p>Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. In this paper, we consider the problem of learning shared structures from multiple related tasks. We present an improved formulation (<i>i</i>ASO) for multi-task learning based on the non-convex alternating structure optimization (ASO) algorithm, in which all tasks are related by a shared feature representation. We convert <i>i</i>ASO, a non-convex formulation, into a relaxed convex one, which is, however, not scalable to large data sets due to its complex constraints. We propose an alternating optimization (<i>c</i>ASO) algorithm which solves the convex relaxation efficiently, and further show that <i>c</i>ASO converges to a global optimum. In addition, we present a theoretical condition, under which <i>c</i>ASO can find a globally optimal solution to <i>i</i>ASO. Experiments on several benchmark data sets confirm our theoretical analysis.</p></div></span> <a id="expcoll18" href="JavaScript: expandcollapse('expcoll18',18)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553393">Learning kernels from indefinite similarities</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81440607609">Yihua Chen</a>,
<a href="author_page.cfm?id=81100021030">Maya R. Gupta</a>,
<a href="author_page.cfm?id=81100346967">Benjamin Recht</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 145-152</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553393" title="DOI">10.1145/1553374.1553393</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553393&ftid=639784&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow19" style="display:inline;"><br /><div style="display:inline">Similarity measures in many real applications generate indefinite similarity matrices. In this paper, we consider the problem of classification based on such indefinite similarities. These indefinite kernels can be problematic for standard kernel-based ...</div></span>
<span id="toHide19" style="display:none;"><br /><div style="display:inline"><p>Similarity measures in many real applications generate indefinite similarity matrices. In this paper, we consider the problem of classification based on such indefinite similarities. These indefinite kernels can be problematic for standard kernel-based algorithms as the optimization problems become non-convex and the underlying theory is invalidated. In order to adapt kernel methods for similarity-based learning, we introduce a method that aims to simultaneously find a reproducing kernel Hilbert space based on the given similarities and train a classifier with good generalization in that space. The method is formulated as a convex optimization problem. We propose a simplified version that can reduce overfitting and whose associated convex conic program can be solved efficiently. We compare the proposed simplified version with six other methods on a collection of real data sets.</p></div></span> <a id="expcoll19" href="JavaScript: expandcollapse('expcoll19',19)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553394">Matrix updates for perceptron training of continuous density hidden Markov models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435603939">Chih-Chieh Cheng</a>,
<a href="author_page.cfm?id=81100436576">Fei Sha</a>,
<a href="author_page.cfm?id=81100590935">Lawrence K. Saul</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 153-160</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553394" title="DOI">10.1145/1553374.1553394</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553394&ftid=639786&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow20" style="display:inline;"><br /><div style="display:inline">In this paper, we investigate a simple, mistake-driven learning algorithm for discriminative training of continuous density hidden Markov models (CD-HMMs). Most CD-HMMs for automatic speech recognition use multivariate Gaussian emission densities (or ...</div></span>
<span id="toHide20" style="display:none;"><br /><div style="display:inline"><p>In this paper, we investigate a simple, mistake-driven learning algorithm for discriminative training of continuous density hidden Markov models (CD-HMMs). Most CD-HMMs for automatic speech recognition use multivariate Gaussian emission densities (or mixtures thereof) parameterized in terms of their means and covariance matrices. For discriminative training of CD-HMMs, we reparameterize these Gaussian distributions in terms of positive semidefinite matrices that jointly encode their mean and covariance statistics. We show how to explore the resulting parameter space in CDHMMs with perceptron-style updates that minimize the distance between Viterbi decodings and target transcriptions. We experiment with several forms of updates, systematically comparing the effects of different matrix factorizations, initializations, and averaging schemes on phone accuracies and convergence rates. We present experimental results for context-independent CD-HMMs trained in this way on the TIMIT speech corpus. Our results show that certain types of perceptron training yield consistently significant and rapid reductions in phone error rates.</p></div></span> <a id="expcoll20" href="JavaScript: expandcollapse('expcoll20',20)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553395">Decision tree and instance-based learning for label ranking</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81388599476">Weiwei Cheng</a>,
<a href="author_page.cfm?id=81435608403">Jens H&#252;hn</a>,
<a href="author_page.cfm?id=81100327876">Eyke H&#252;llermeier</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 161-168</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553395" title="DOI">10.1145/1553374.1553395</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553395&ftid=639788&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow21" style="display:inline;"><br /><div style="display:inline">The label ranking problem consists of learning a model that maps instances to total orders over a finite set of predefined labels. This paper introduces new methods for label ranking that complement and improve upon existing approaches. More specifically, ...</div></span>
<span id="toHide21" style="display:none;"><br /><div style="display:inline"><p>The label ranking problem consists of learning a model that maps instances to total orders over a finite set of predefined labels. This paper introduces new methods for label ranking that complement and improve upon existing approaches. More specifically, we propose extensions of two methods that have been used extensively for classification and regression so far, namely instance-based learning and decision tree induction. The unifying element of the two methods is a procedure for locally estimating predictive probability models for label rankings.</p></div></span> <a id="expcoll21" href="JavaScript: expandcollapse('expcoll21',21)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553396">Learning dictionaries of stable autoregressive models for audio scene analysis</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435597253">Youngmin Cho</a>,
<a href="author_page.cfm?id=81100590935">Lawrence K. Saul</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 169-176</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553396" title="DOI">10.1145/1553374.1553396</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553396&ftid=639790&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow22" style="display:inline;"><br /><div style="display:inline">In this paper, we explore an application of basis pursuit to audio scene analysis. The goal of our work is to detect when certain sounds are present in a mixed audio signal. We focus on the regime where out of a large number of possible sources, a small ...</div></span>
<span id="toHide22" style="display:none;"><br /><div style="display:inline"><p>In this paper, we explore an application of basis pursuit to audio scene analysis. The goal of our work is to detect when certain sounds are present in a mixed audio signal. We focus on the regime where out of a large number of possible sources, a small but unknown number combine and overlap to yield the observed signal. To infer which sounds are present, we decompose the observed signal as a linear combination of a small number of active sources. We cast the inference as a regularized form of linear regression whose sparse solutions yield decompositions with few active sources. We characterize the acoustic variability of individual sources by autoregressive models of their time domain waveforms. When we do not have prior knowledge of the individual sources, the coefficients of these autoregressive models must be learned from audio examples. We analyze the dynamical stability of these models and show how to estimate stable models by substituting a simple convex optimization for a difficult eigenvalue problem. We demonstrate our approach by learning dictionaries of musical notes and using these dictionaries to analyze polyphonic recordings of piano, cello, and violin.</p></div></span> <a id="expcoll22" href="JavaScript: expandcollapse('expcoll22',22)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553397">Exploiting sparse Markov <i>and</i> covariance structure in multiresolution models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435602841">Myung Jin Choi</a>,
<a href="author_page.cfm?id=81416592766">Venkat Chandrasekaran</a>,
<a href="author_page.cfm?id=81100007609">Alan S. Willsky</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 177-184</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553397" title="DOI">10.1145/1553374.1553397</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553397&ftid=639792&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow23" style="display:inline;"><br /><div style="display:inline">We consider Gaussian multiresolution (MR) models in which coarser, hidden variables serve to capture statistical dependencies among the finest scale variables. Tree-structured MR models have limited modeling capabilities, as variables at one scale are ...</div></span>
<span id="toHide23" style="display:none;"><br /><div style="display:inline"><p>We consider Gaussian multiresolution (MR) models in which coarser, hidden variables serve to capture statistical dependencies among the finest scale variables. Tree-structured MR models have limited modeling capabilities, as variables at one scale are forced to be uncorrelated with each other conditioned on other scales. We propose a new class of Gaussian MR models that capture the residual correlations within each scale using <i>sparse covariance structure</i>. Our goal is to learn a tree-structured graphical model connecting variables across different scales, while at the same time learning sparse structure for the conditional covariance within each scale conditioned on other scales. This model leads to an efficient, new inference algorithm that is similar to multipole methods in computational physics.</p></div></span> <a id="expcoll23" href="JavaScript: expandcollapse('expcoll23',23)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553398">Nonparametric estimation of the precision-recall curve</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81363596513">St&#233;phan Cl&#233;men&#231;on</a>,
<a href="author_page.cfm?id=81100224818">Nicolas Vayatis</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 185-192</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553398" title="DOI">10.1145/1553374.1553398</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553398&ftid=639794&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow24" style="display:inline;"><br /><div style="display:inline">The Precision-Recall (PR) curve is a widely used visual tool to evaluate the performance of scoring functions in regards to their capacities to discriminate between two populations. The purpose of this paper is to examine both theoretical and practical ...</div></span>
<span id="toHide24" style="display:none;"><br /><div style="display:inline"><p>The Precision-Recall (PR) curve is a widely used visual tool to evaluate the performance of scoring functions in regards to their capacities to discriminate between two populations. The purpose of this paper is to examine both theoretical and practical issues related to the statistical estimation of PR curves based on classification data. Consistency and asymptotic normality of the empirical counterpart of the PR curve in sup norm are rigorously established. Eventually, the issue of building confidence bands in the PR space is considered and a specific resampling procedure based on a smoothed and truncated version of the empirical distribution of the data is promoted. Arguments of theoretical and computational nature are presented to explain why such a bootstrap is preferable to a "naive" bootstrap in this setup.</p></div></span> <a id="expcoll24" href="JavaScript: expandcollapse('expcoll24',24)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553399">EigenTransfer: a unified framework for transfer learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333488189">Wenyuan Dai</a>,
<a href="author_page.cfm?id=81435609771">Ou Jin</a>, 
<a href="author_page.cfm?id=81100142932">Gui-Rong Xue</a>,
<a href="author_page.cfm?id=81372591186">Qiang Yang</a>,
<a href="author_page.cfm?id=81363594294">Yong Yu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 193-200</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553399" title="DOI">10.1145/1553374.1553399</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553399&ftid=639759&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow25" style="display:inline;"><br /><div style="display:inline">This paper proposes a general framework, called EigenTransfer, to tackle a variety of transfer learning problems, e.g. cross-domain learning, self-taught learning, etc. Our basic idea is to construct a graph to represent the target transfer learning ...</div></span>
<span id="toHide25" style="display:none;"><br /><div style="display:inline"><p>This paper proposes a general framework, called EigenTransfer, to tackle a variety of transfer learning problems, e.g. cross-domain learning, self-taught learning, etc. Our basic idea is to construct a graph to represent the target transfer learning task. By learning the spectra of a graph which represents a learning task, we obtain a set of eigenvectors that reflect the intrinsic structure of the task graph. These eigenvectors can be used as the new features which transfer the knowledge from auxiliary data to help classify target data. Given an arbitrary non-transfer learner (e.g. SVM) and a particular transfer learning task, EigenTransfer can produce a <i>transfer learner</i> accordingly for the target transfer learning task. We apply EigenTransfer on three different transfer learning tasks, cross-domain learning, cross-category learning and self-taught learning, to demonstrate its unifying ability, and show through experiments that EigenTransfer can greatly outperform several representative non-transfer learners.</p></div></span> <a id="expcoll25" href="JavaScript: expandcollapse('expcoll25',25)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553400">Fitting a graph to vector data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435603197">Samuel I. Daitch</a>,
<a href="author_page.cfm?id=81100275109">Jonathan A. Kelner</a>,
<a href="author_page.cfm?id=81100023777">Daniel A. Spielman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 201-208</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553400" title="DOI">10.1145/1553374.1553400</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553400&ftid=639761&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow26" style="display:inline;"><br /><div style="display:inline">We introduce a measure of how well a combinatorial graph fits a collection of vectors. The optimal graphs under this measure may be computed by solving convex quadratic programs and have many interesting properties. For vectors in d dimensional ...</div></span>
<span id="toHide26" style="display:none;"><br /><div style="display:inline"><p>We introduce a measure of how well a combinatorial graph fits a collection of vectors. The optimal graphs under this measure may be computed by solving convex quadratic programs and have many interesting properties. For vectors in <i>d</i> dimensional space, the graphs always have average degree at most 2(<i>d</i> + 1), and for vectors in 2 dimensions they are always planar. We compute these graphs for many standard data sets and show that they can be used to obtain good solutions to classification, regression and clustering problems.</p></div></span> <a id="expcoll26" href="JavaScript: expandcollapse('expcoll26',26)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553401">Unsupervised search-based structured prediction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100635457">Hal Daum&#233;, III</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 209-216</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553401" title="DOI">10.1145/1553374.1553401</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553401&ftid=639763&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow27" style="display:inline;"><br /><div style="display:inline">We describe an adaptation and application of a search-based structured prediction algorithm "Searn" to unsupervised learning problems. We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high-quality un-supervised ...</div></span>
<span id="toHide27" style="display:none;"><br /><div style="display:inline"><p>We describe an adaptation and application of a search-based structured prediction algorithm "Searn" to unsupervised learning problems. We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high-quality un-supervised shift-reduce parsing model. We additionally show a close connection between unsupervised Searn and expectation maximization. Finally, we demonstrate the efficacy of a semi-supervised extension. The key idea that enables this is an application of the <i>predict-self</i> idea for unsupervised learning.</p></div></span> <a id="expcoll27" href="JavaScript: expandcollapse('expcoll27',27)">expand</a>
</div>
 </td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553402">Deep transfer via second-order Markov logic</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100603602">Jesse Davis</a>,
<a href="author_page.cfm?id=81100205908">Pedro Domingos</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 217-224</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553402" title="DOI">10.1145/1553374.1553402</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553402&ftid=639765&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow28" style="display:inline;"><br /><div style="display:inline">Standard inductive learning requires that training and test instances come from the same distribution. Transfer learning seeks to remove this restriction. In shallow transfer, test instances are from the same domain, but have a different distribution. ...</div></span>
<span id="toHide28" style="display:none;"><br /><div style="display:inline"><p>Standard inductive learning requires that training and test instances come from the same distribution. Transfer learning seeks to remove this restriction. In shallow transfer, test instances are from the same domain, but have a different distribution. In deep transfer, test instances are from a different domain entirely (i.e., described by different predicates). Humans routinely perform deep transfer, but few learning systems, if any, are capable of it. In this paper we propose an approach based on a form of second-order Markov logic. Our algorithm discovers structural regularities in the source domain in the form of Markov logic formulas with predicate variables, and instantiates these formulas with predicates from the target domain. Using this approach, we have successfully transferred learned knowledge among molecular biology, social network and Web domains. The discovered patterns include broadly useful properties of predicates, like symmetry and transitivity, and relations among predicates, such as various forms of homophily.</p></div></span> <a id="expcoll28" href="JavaScript: expandcollapse('expcoll28',28)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553403">Analytic moment-based Gaussian process filtering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81414592396">Marc Peter Deisenroth</a>,
<a href="author_page.cfm?id=81328488814">Marco F. Huber</a>,
<a href="author_page.cfm?id=81100426582">Uwe D. Hanebeck</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 225-232</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553403" title="DOI">10.1145/1553374.1553403</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553403&ftid=639796&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow29" style="display:inline;"><br /><div style="display:inline">We propose an analytic moment-based filter for nonlinear stochastic dynamic systems modeled by Gaussian processes. Exact expressions for the expected value and the covariance matrix are provided for both the prediction step and the filter step, where ...</div></span>
<span id="toHide29" style="display:none;"><br /><div style="display:inline"><p>We propose an analytic moment-based filter for nonlinear stochastic dynamic systems modeled by Gaussian processes. Exact expressions for the expected value and the covariance matrix are provided for both the prediction step and the filter step, where an additional Gaussian assumption is exploited in the latter case. Our filter does not require further approximations. In particular, it avoids finite-sample approximations. We compare the filter to a variety of Gaussian filters, that is, the EKF, the UKF, and the recent GP-UKF proposed by Ko et al. (2007).</p></div></span> <a id="expcoll29" href="JavaScript: expandcollapse('expcoll29',29)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553404">Good learners for evil teachers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100230258">Ofer Dekel</a>,
<a href="author_page.cfm?id=81365597666">Ohad Shamir</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 233-240</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553404" title="DOI">10.1145/1553374.1553404</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553404&ftid=639798&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow30" style="display:inline;"><br /><div style="display:inline">We consider a supervised machine learning scenario where labels are provided by a heterogeneous set of teachers, some of which are mediocre, incompetent, or perhaps even malicious. We present an algorithm, built on the SVM framework, that explicitly ...</div></span>
<span id="toHide30" style="display:none;"><br /><div style="display:inline"><p>We consider a supervised machine learning scenario where labels are provided by a heterogeneous set of teachers, some of which are mediocre, incompetent, or perhaps even malicious. We present an algorithm, built on the SVM framework, that explicitly attempts to cope with low-quality and malicious teachers by decreasing their influence on the learning process. Our algorithm does not receive any prior information on the teachers, nor does it resort to repeated labeling (where each example is labeled by multiple teachers). We provide a theoretical analysis of our algorithm and demonstrate its merits empirically. Finally, we present a second algorithm with promising empirical results but without a formal analysis.</p></div></span> <a id="expcoll30" href="JavaScript: expandcollapse('expcoll30',30)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553405">A scalable framework for discovering coherent co-clusters in noisy data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331491624">Meghana Deodhar</a>,
<a href="author_page.cfm?id=81331493603">Gunjan Gupta</a>,
<a href="author_page.cfm?id=81100558602">Joydeep Ghosh</a>,
<a href="author_page.cfm?id=81435604471">Hyuk Cho</a>,
<a href="author_page.cfm?id=81100098715">Inderjit Dhillon</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 241-248</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553405" title="DOI">10.1145/1553374.1553405</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553405&ftid=639800&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow31" style="display:inline;"><br /><div style="display:inline">Clustering problems often involve datasets where only a part of the data is relevant to the problem, e.g., in microarray data analysis only a subset of the genes show cohesive expressions within a subset of the conditions/features. The existence of a ...</div></span>
<span id="toHide31" style="display:none;"><br /><div style="display:inline"><p>Clustering problems often involve datasets where only a part of the data is relevant to the problem, e.g., in microarray data analysis only a subset of the genes show cohesive expressions within a subset of the conditions/features. The existence of a large number of non-informative data points and features makes it challenging to hunt for coherent and meaningful clusters from such datasets. Additionally, since clusters could exist in different subspaces of the feature space, a co-clustering algorithm that simultaneously clusters objects and features is often more suitable as compared to one that is restricted to traditional "one-sided" clustering. We propose Robust Overlapping Co-Clustering (ROCC), a scalable and very versatile framework that addresses the problem of efficiently mining dense, arbitrarily positioned, possibly overlapping co-clusters from large, noisy datasets. ROCC has several desirable properties that make it extremely well suited to a number of real life applications.</p></div></span> <a id="expcoll31" href="JavaScript: expandcollapse('expcoll31',31)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553406">The adaptive <i>k</i>-meteorologists problem and its application to structure learning and feature selection in reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81318488284">Carlos Diuk</a>,
<a href="author_page.cfm?id=81331497976">Lihong Li</a>,
<a href="author_page.cfm?id=81435607571">Bethany R. Leffler</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 249-256</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553406" title="DOI">10.1145/1553374.1553406</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553406&ftid=639802&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow32" style="display:inline;"><br /><div style="display:inline">The purpose of this paper is three-fold. First, we formalize and study a problem of learning probabilistic concepts in the recently proposed KWIK framework. We give details of an algorithm, known as the Adaptive k-Meteorologists Algorithm, analyze ...</div></span>
<span id="toHide32" style="display:none;"><br /><div style="display:inline"><p>The purpose of this paper is three-fold. First, we formalize and study a problem of learning probabilistic concepts in the recently proposed KWIK framework. We give details of an algorithm, known as the Adaptive <i>k</i>-Meteorologists Algorithm, analyze its sample-complexity upper bound, and give a <i>matching</i> lower bound. Second, this algorithm is used to create a new reinforcement-learning algorithm for factored-state problems that enjoys significant improvement over the previous state-of-the-art algorithm. Finally, we apply the Adaptive <i>k</i>-Meteorologists Algorithm to remove a limiting assumption in an existing reinforcement-learning algorithm. The effectiveness of our approaches is demonstrated empirically in a couple benchmark domains as well as a robotics navigation problem.</p></div></span> <a id="expcoll32" href="JavaScript: expandcollapse('expcoll32',32)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553407">Proximal regularization for online and batch learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384602158">Chuong B. Do</a>,
<a href="author_page.cfm?id=81339511241">Quoc V. Le</a>,
<a href="author_page.cfm?id=81384607341">Chuan-Sheng Foo</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 257-264</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553407" title="DOI">10.1145/1553374.1553407</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553407&ftid=639804&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow33" style="display:inline;"><br /><div style="display:inline">Many learning algorithms rely on the curvature (in particular, strong convexity) of regularized objective functions to provide good theoretical performance guarantees. In practice, the choice of regularization penalty that gives the best testing set ...</div></span>
<span id="toHide33" style="display:none;"><br /><div style="display:inline"><p>Many learning algorithms rely on the curvature (in particular, strong convexity) of regularized objective functions to provide good theoretical performance guarantees. In practice, the choice of regularization penalty that gives the best testing set performance may result in objective functions with little or even no curvature. In these cases, algorithms designed specifically for regularized objectives often either fail completely or require some modification that involves a substantial compromise in performance.</p> <p>We present new online and batch algorithms for training a variety of supervised learning models (such as SVMs, logistic regression, structured prediction models, and CRFs) under conditions where the optimal choice of regularization parameter results in functions with low curvature. We employ a technique called <i>proximal regularization</i>, in which we solve the original learning problem via a sequence of modified optimization tasks whose objectives are chosen to have greater curvature than the original problem. Theoretically, our algorithms achieve low regret bounds in the online setting and fast convergence in the batch setting. Experimentally, our algorithms improve upon state-of-the-art techniques, including Pegasos and bundle methods, on medium and large-scale SVM and structured learning tasks.</p></div></span> <a id="expcoll33" href="JavaScript: expandcollapse('expcoll33',33)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553408">Large margin training for hidden Markov models with partially observed states</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384608887">Trinh-Minh-Tri Do</a>,
<a href="author_page.cfm?id=81100258570">Thierry Arti&#232;res</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 265-272</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553408" title="DOI">10.1145/1553374.1553408</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553408&ftid=639806&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow34" style="display:inline;"><br /><div style="display:inline">Large margin learning of Continuous Density HMMs with a partially labeled dataset has been extensively studied in the speech and handwriting recognition fields. Yet due to the non-convexity of the optimization problem, previous works usually rely on ...</div></span>
<span id="toHide34" style="display:none;"><br /><div style="display:inline"><p>Large margin learning of Continuous Density HMMs with a partially labeled dataset has been extensively studied in the speech and handwriting recognition fields. Yet due to the non-convexity of the optimization problem, previous works usually rely on severe approximations so that it is still an open problem. We propose a new learning algorithm that relies on non-convex optimization and bundle methods and allows tackling the original optimization problem as is. It is proved to converge to a solution with accuracy &epsilon; with a rate O (1/&epsilon;). We provide experimental results gained on speech and handwriting recognition that demonstrate the potential of the method.</p></div></span> <a id="expcoll34" href="JavaScript: expandcollapse('expcoll34',34)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553409">Accelerated sampling for the Indian Buffet Process</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435611644">Finale Doshi-Velez</a>,
<a href="author_page.cfm?id=81100572858">Zoubin Ghahramani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 273-280</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553409" title="DOI">10.1145/1553374.1553409</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553409&ftid=639808&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow35" style="display:inline;"><br /><div style="display:inline">We often seek to identify co-occurring hidden features in a set of observations. The Indian Buffet Process (IBP) provides a non-parametric prior on the features present in each observation, but current inference techniques for the IBP often scale poorly. ...</div></span>
<span id="toHide35" style="display:none;"><br /><div style="display:inline"><p>We often seek to identify co-occurring hidden features in a set of observations. The Indian Buffet Process (IBP) provides a non-parametric prior on the features present in each observation, but current inference techniques for the IBP often scale poorly. The collapsed Gibbs sampler for the IBP has a running time cubic in the number of observations, and the uncollapsed Gibbs sampler, while linear, is often slow to mix. We present a new linear-time collapsed Gibbs sampler for conjugate likelihood models and demonstrate its efficacy on large real-world datasets.</p></div></span> <a id="expcoll35" href="JavaScript: expandcollapse('expcoll35',35)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553410">Accounting for burstiness in topic models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435599316">Gabriel Doyle</a>,
<a href="author_page.cfm?id=81339498029">Charles Elkan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 281-288</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553410" title="DOI">10.1145/1553374.1553410</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553410&ftid=639810&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow36" style="display:inline;"><br /><div style="display:inline">Many different topic models have been used successfully for a variety of applications. However, even state-of-the-art topic models suffer from the important flaw that they do not capture the tendency of words to appear in bursts; it is a fundamental ...</div></span>
<span id="toHide36" style="display:none;"><br /><div style="display:inline"><p>Many different topic models have been used successfully for a variety of applications. However, even state-of-the-art topic models suffer from the important flaw that they do not capture the tendency of words to appear in bursts; it is a fundamental property of language that if a word is used once in a document, it is more likely to be used again. We introduce a topic model that uses Dirichlet compound multinomial (DCM) distributions to model this burstiness phenomenon. On both text and non-text datasets, the new model achieves better held-out likelihood than standard latent Dirichlet allocation (LDA). It is straightforward to incorporate the DCM extension into topic models that are more complex than LDA.</p></div></span> <a id="expcoll36" href="JavaScript: expandcollapse('expcoll36',36)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553411">Domain adaptation from multiple sources via auxiliary classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435594253">Lixin Duan</a>,
<a href="author_page.cfm?id=81309487444">Ivor W. Tsang</a>,
<a href="author_page.cfm?id=81451594460">Dong Xu</a>,
<a href="author_page.cfm?id=81100547902">Tat-Seng Chua</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 289-296</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553411" title="DOI">10.1145/1553374.1553411</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553411&ftid=639812&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow37" style="display:inline;"><br /><div style="display:inline">We propose a multiple source domain adaptation method, referred to as Domain Adaptation Machine (DAM), to learn a robust decision function (referred to as target classifier) for label prediction of patterns from the target domain by leveraging ...</div></span>
<span id="toHide37" style="display:none;"><br /><div style="display:inline"><p>We propose a multiple source domain adaptation method, referred to as Domain Adaptation Machine (DAM), to learn a robust decision function (referred to as <i>target classifier</i>) for label prediction of patterns from the target domain by leveraging a set of pre-computed classifiers (referred to as <i>auxiliary/source classifiers</i>) independently learned with the labeled patterns from multiple source domains. We introduce a new data-dependent regularizer based on <i>smoothness assumption</i> into Least-Squares SVM (LS-SVM), which enforces that the target classifier shares similar decision values with the auxiliary classifiers from relevant source domains on the unlabeled patterns of the target domain. In addition, we employ a sparsity regularizer to learn a sparse target classifier. Comprehensive experiments on the challenging TRECVID 2005 corpus demonstrate that DAM outperforms the existing multiple source domain adaptation methods for video concept detection in terms of effectiveness and efficiency.</p></div></span> <a id="expcoll37" href="JavaScript: expandcollapse('expcoll37',37)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553412">Boosting with structural sparsity</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597874">John Duchi</a>,
<a href="author_page.cfm?id=81100308085">Yoram Singer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 297-304</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553412" title="DOI">10.1145/1553374.1553412</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553412&ftid=639814&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow38" style="display:inline;"><br /><div style="display:inline">We derive generalizations of AdaBoost and related gradient-based coordinate descent methods that incorporate sparsity-promoting penalties for the norm of the predictor that is being learned. The end result is a family of coordinate descent algorithms ...</div></span>
<span id="toHide38" style="display:none;"><br /><div style="display:inline"><p>We derive generalizations of AdaBoost and related gradient-based coordinate descent methods that incorporate sparsity-promoting penalties for the norm of the predictor that is being learned. The end result is a family of coordinate descent algorithms that integrate forward feature induction and back-pruning through regularization and give an automatic stopping criterion for feature induction. We study penalties based on the <i>l</i><sub>1</sub>, <i>l</i><sub>2</sub>, and <i>l</i>&infin; norms of the predictor and introduce mixed-norm penalties that build upon the initial penalties. The mixed-norm regularizers facilitate structural sparsity in parameter space, which is a useful property in multiclass prediction and other related tasks. We report empirical results that demonstrate the power of our approach in building accurate and structurally sparse models.</p></div></span> <a id="expcoll38" href="JavaScript: expandcollapse('expcoll38',38)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553413">Learning to segment from a few well-selected training images</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384601856">Alireza Farhangfar</a>,
<a href="author_page.cfm?id=81100082147">Russell Greiner</a>,
<a href="author_page.cfm?id=81100538163">Csaba Szepesv&#225;ri</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 305-312</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553413" title="DOI">10.1145/1553374.1553413</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553413&ftid=639816&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow39" style="display:inline;"><br /><div style="display:inline">We address the task of actively learning a segmentation system: given a large number of unsegmented images, and access to an oracle that can segment a given image, decide which images to provide, to quickly produce a segmenter (here, a discriminative ...</div></span>
<span id="toHide39" style="display:none;"><br /><div style="display:inline"><p>We address the task of actively learning a segmentation system: given a large number of unsegmented images, and access to an oracle that can segment a given image, decide which images to provide, to quickly produce a segmenter (here, a discriminative random field) that is accurate over this distribution of images. We extend the standard models for active learner to define a system for this task that first selects the image whose expected label will reduce the uncertainty of the other unlabeled images the most, and then after greedily selects, from the pool of unsegmented images, the most informative image. The results of our experiments, over two real-world datasets (segmenting brain tumors within magnetic resonance images; and segmenting the sky in real images) show that training on very few informative images (here, as few as 2) can produce a segmenter that is as good as training on the entire dataset.</p></div></span> <a id="expcoll39" href="JavaScript: expandcollapse('expcoll39',39)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553414">GAODE and HAODE: two proposals based on AODE to deal with continuous variables</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81350575050">M. Julia Flores</a>,
<a href="author_page.cfm?id=81100143331">Jos&#233; A. G&#225;mez</a>,
<a href="author_page.cfm?id=81435593040">Ana M. Mart&#237;nez</a>,
<a href="author_page.cfm?id=81100290163">Jos&#233; M. Puerta</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 313-320</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553414" title="DOI">10.1145/1553374.1553414</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553414&ftid=639818&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow40" style="display:inline;"><br /><div style="display:inline">AODE (Aggregating One-Dependence Estimators) is considered one of the most interesting representatives of the Bayesian classifiers, taking into account not only the low error rate it provides but also its efficiency. Until now, all the attributes in ...</div></span>
<span id="toHide40" style="display:none;"><br /><div style="display:inline"><p>AODE (Aggregating One-Dependence Estimators) is considered one of the most interesting representatives of the Bayesian classifiers, taking into account not only the low error rate it provides but also its efficiency. Until now, all the attributes in a dataset have had to be nominal to build an AODE classifier or they have had to be previously discretized. In this paper, we propose two different approaches in order to deal directly with numeric attributes. One of them uses conditional Gaussian networks to model a dataset exclusively with numeric attributes; and the other one keeps the superparent on each model discrete and uses univariate Gaussians to estimate the probabilities for the numeric attributes and multinomial distributions for the categorical ones, it also being able to model hybrid datasets. Both of them obtain competitive results compared to AODE, the latter in particular being a very attractive alternative to AODE in numeric datasets.</p></div></span> <a id="expcoll40" href="JavaScript: expandcollapse('expcoll40',40)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553415">A majorization-minimization algorithm for (multiple) hyperparameter learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384607341">Chuan-Sheng Foo</a>,
<a href="author_page.cfm?id=81384602158">Chuong B. Do</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 321-328</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553415" title="DOI">10.1145/1553374.1553415</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553415&ftid=640133&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow41" style="display:inline;"><br /><div style="display:inline">We present a general Bayesian framework for hyperparameter tuning in L2-regularized supervised learning models. Paradoxically, our algorithm works by first analytically integrating out the hyperparameters from the model. We find a local ...</div></span>
<span id="toHide41" style="display:none;"><br /><div style="display:inline"><p>We present a general Bayesian framework for hyperparameter tuning in <i>L</i><sub>2</sub>-regularized supervised learning models. Paradoxically, our algorithm works by first analytically integrating out the hyperparameters from the model. We find a local optimum of the resulting non-convex optimization problem efficiently using a majorization-minimization (MM) algorithm, in which the non-convex problem is reduced to a series of convex <i>L</i><sub>2</sub>-regularized parameter estimation tasks. The principal appeal of our method is its simplicity: the updates for choosing the <i>L</i><sub>2</sub>-regularized subproblems in each step are trivial to implement (or even perform by hand), and each subproblem can be efficiently solved by adapting existing solvers. Empirical results on a variety of supervised learning models show that our algorithm is competitive with both grid-search and gradient-based algorithms, but is more efficient and far easier to implement.</p></div></span> <a id="expcoll41" href="JavaScript: expandcollapse('expcoll41',41)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553416">Dynamic mixed membership blockmodel for evolving networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81539354956">Wenjie Fu</a>,
<a href="author_page.cfm?id=81100578710">Le Song</a>,
<a href="author_page.cfm?id=81407592503">Eric P. Xing</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 329-336</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553416" title="DOI">10.1145/1553374.1553416</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553416&ftid=640135&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
 </td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow42" style="display:inline;"><br /><div style="display:inline">In a dynamic social or biological environment, interactions between the underlying actors can undergo large and systematic changes. Each actor can assume multiple roles and their degrees of affiliation to these roles can also exhibit rich temporal phenomena. ...</div></span>
<span id="toHide42" style="display:none;"><br /><div style="display:inline"><p>In a dynamic social or biological environment, interactions between the underlying actors can undergo large and systematic changes. Each actor can assume multiple roles and their degrees of affiliation to these roles can also exhibit rich temporal phenomena. We propose a <i>state space mixed membership stochastic blockmodel</i> which can track across time the evolving roles of the actors. We also derive an efficient variational inference procedure for our model, and apply it to the Enron email networks, and rewiring gene regulatory networks of yeast. In both cases, our model reveals interesting dynamical roles of the actors.</p></div></span> <a id="expcoll42" href="JavaScript: expandcollapse('expcoll42',42)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553417">Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100629901">Rahul Garg</a>,
<a href="author_page.cfm?id=81335492568">Rohit Khandekar</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 337-344</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553417" title="DOI">10.1145/1553374.1553417</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553417&ftid=640137&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow43" style="display:inline;"><br /><div style="display:inline">We present an algorithm for finding an s-sparse vector x that minimizes the square-error &#8741;y -- &Phi;x&#8741;2 where &Phi; satisfies the restricted isometry property (RIP), with isometric constant ...</div></span>
<span id="toHide43" style="display:none;"><br /><div style="display:inline"><p>We present an algorithm for finding an <i>s</i>-sparse vector <i>x</i> that minimizes the <i>square-error</i> &#8741;<i>y</i> -- &Phi;<i>x</i>&#8741;<sup>2</sup> where &Phi; satisfies the <i>restricted isometry property</i> (RIP), with <i>isometric constant</i> &delta;<sub>2<i>s</i></sub> &lt; 1/3. Our algorithm, called <b>GraDeS</b> (Gradient Descent with Sparsification) iteratively updates <i>x</i> as: [EQUATION]</p> <p>where &gamma; &gt; 1 and <i>H<sub>s</sub></i> sets all but <i>s</i> largest magnitude coordinates to zero. <b>GraDeS</b> converges to the correct solution in constant number of iterations. The condition &delta;<sub>2<i>s</i></sub> &lt; 1/3 is most general for which a <i>near-linear time</i> algorithm is known. In comparison, the best condition under which a polynomial-time algorithm is known, is &delta;<sub>2<i>s</i></sub> &lt; &radic;2 -- 1.</p> <p>Our Matlab implementation of <b>GraDeS</b> outperforms previously proposed algorithms like Subspace Pursuit, StOMP, OMP, and Lasso by an order of magnitude. Curiously, our experiments also uncovered cases where L1-regularized regression (Lasso) fails but <b>GraDeS</b> finds the correct solution.</p></div></span> <a id="expcoll43" href="JavaScript: expandcollapse('expcoll43',43)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553418">Sequential Bayesian prediction in the presence of changepoints</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435592396">Roman Garnett</a>,
<a href="author_page.cfm?id=81384599123">Michael A. Osborne</a>,
<a href="author_page.cfm?id=81341495576">Stephen J. Roberts</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 345-352</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553418" title="DOI">10.1145/1553374.1553418</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553418&ftid=640139&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow44" style="display:inline;"><br /><div style="display:inline">We introduce a new sequential algorithm for making robust predictions in the presence of changepoints. Unlike previous approaches, which focus on the problem of detecting and locating changepoints, our algorithm focuses on the problem of making predictions ...</div></span>
<span id="toHide44" style="display:none;"><br /><div style="display:inline"><p>We introduce a new sequential algorithm for making robust predictions in the presence of changepoints. Unlike previous approaches, which focus on the problem of detecting and locating changepoints, our algorithm focuses on the problem of making predictions even when such changes might be present. We introduce nonstationary covariance functions to be used in Gaussian process prediction that model such changes, then proceed to demonstrate how to effectively manage the hyperparameters associated with those covariance functions. By using Bayesian quadrature, we can integrate out the hyperparameters, allowing us to calculate the marginal predictive distribution. Furthermore, if desired, the posterior distribution over putative changepoint locations can be calculated as a natural byproduct of our prediction algorithm.</p></div></span> <a id="expcoll44" href="JavaScript: expandcollapse('expcoll44',44)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553419">PAC-Bayesian learning of linear classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435598852">Pascal Germain</a>,
<a href="author_page.cfm?id=81435600966">Alexandre Lacasse</a>,
<a href="author_page.cfm?id=81100257819">Fran&#231;ois Laviolette</a>,
<a href="author_page.cfm?id=81100612037">Mario Marchand</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 353-360</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553419" title="DOI">10.1145/1553374.1553419</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553419&ftid=640141&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow45" style="display:inline;"><br /><div style="display:inline">We present a general PAC-Bayes theorem from which all known PAC-Bayes risk bounds are obtained as particular cases. We also propose different learning algorithms for finding linear classifiers that minimize these bounds. These learning algorithms are ...</div></span>
<span id="toHide45" style="display:none;"><br /><div style="display:inline"><p>We present a general PAC-Bayes theorem from which all known PAC-Bayes risk bounds are obtained as particular cases. We also propose different learning algorithms for finding linear classifiers that minimize these bounds. These learning algorithms are generally competitive with both AdaBoost and the SVM.</p></div></span> <a id="expcoll45" href="JavaScript: expandcollapse('expcoll45',45)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553421">Fast evolutionary maximum margin clustering</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81464667939">Fabian Gieseke</a>,
<a href="author_page.cfm?id=81324492659">Tapio Pahikkala</a>,
<a href="author_page.cfm?id=81333489521">Oliver Kramer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 361-368</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553421" title="DOI">10.1145/1553374.1553421</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553421&ftid=640143&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow46" style="display:inline;"><br /><div style="display:inline">The maximum margin clustering approach is a recently proposed extension of the concept of support vector machines to the clustering problem. Briefly stated, it aims at finding an optimal partition of the data into two classes such that the margin induced ...</div></span>
<span id="toHide46" style="display:none;"><br /><div style="display:inline"><p>The maximum margin clustering approach is a recently proposed extension of the concept of support vector machines to the clustering problem. Briefly stated, it aims at finding an optimal partition of the data into two classes such that the margin induced by a subsequent application of a support vector machine is maximal. We propose a method based on stochastic search to address this hard optimization problem. While a direct implementation would be infeasible for large data sets, we present an efficient computational shortcut for assessing the "quality" of intermediate solutions. Experimental results show that our approach outperforms existing methods in terms of clustering accuracy.</p></div></span> <a id="expcoll46" href="JavaScript: expandcollapse('expcoll46',46)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553422">Dynamic analysis of multiagent <i>Q</i>-learning with &epsilon;-greedy exploration</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435607791">Eduardo Rodrigues Gomes</a>,
<a href="author_page.cfm?id=81100450242">Ryszard Kowalczyk</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 369-376</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553422" title="DOI">10.1145/1553374.1553422</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553422&ftid=640145&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow47" style="display:inline;"><br /><div style="display:inline">The development of mechanisms to understand and model the expected behaviour of multiagent learners is becoming increasingly important as the area rapidly find application in a variety of domains. In this paper we present a framework to model the behaviour ...</div></span>
<span id="toHide47" style="display:none;"><br /><div style="display:inline"><p>The development of mechanisms to understand and model the expected behaviour of multiagent learners is becoming increasingly important as the area rapidly find application in a variety of domains. In this paper we present a framework to model the behaviour of <i>Q</i>-learning agents using the &epsilon;-greedy exploration mechanism. For this, we analyse a continuous-time version of the <i>Q</i>-learning update rule and study how the presence of other agents and the &epsilon;-greedy mechanism affect it. We then model the problem as a system of difference equations which is used to theoretically analyse the expected behaviour of the agents. The applicability of the framework is tested through experiments in typical games selected from the literature.</p></div></span> <a id="expcoll47" href="JavaScript: expandcollapse('expcoll47',47)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553423">Bayesian inference for Plackett-Luce ranking models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435592336">John Guiver</a>,
<a href="author_page.cfm?id=81309492467">Edward Snelson</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 377-384</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553423" title="DOI">10.1145/1553374.1553423</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553423&ftid=640147&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow48" style="display:inline;"><br /><div style="display:inline">This paper gives an efficient Bayesian method for inferring the parameters of a Plackett-Luce ranking model. Such models are parameterised distributions over rankings of a finite set of objects, and have typically been studied and applied within the ...</div></span>
<span id="toHide48" style="display:none;"><br /><div style="display:inline"><p>This paper gives an efficient Bayesian method for inferring the parameters of a Plackett-Luce ranking model. Such models are parameterised distributions over rankings of a finite set of objects, and have typically been studied and applied within the psychometric, sociometric and econometric literature. The inference scheme is an application of Power EP (expectation propagation). The scheme is robust and can be readily applied to large scale data sets. The inference algorithm extends to variations of the basic Plackett-Luce model, including partial rankings. We show a number of advantages of the EP approach over the traditional maximum likelihood method. We apply the method to aggregate rankings of NASCAR racing drivers over the 2002 season, and also to rankings of movie genres.</p></div></span> <a id="expcoll48" href="JavaScript: expandcollapse('expcoll48',48)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553424">Bayesian clustering for email campaign detection</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81310486073">Peter Haider</a>,
<a href="author_page.cfm?id=81100180901">Tobias Scheffer</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 385-392</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553424" title="DOI">10.1145/1553374.1553424</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553424&ftid=640149&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
 <td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow49" style="display:inline;"><br /><div style="display:inline">We discuss the problem of clustering elements according to the sources that have generated them. For elements that are characterized by independent binary attributes, a closed-form Bayesian solution exists. We derive a solution for the case of dependent ...</div></span>
<span id="toHide49" style="display:none;"><br /><div style="display:inline"><p>We discuss the problem of clustering elements according to the sources that have generated them. For elements that are characterized by independent binary attributes, a closed-form Bayesian solution exists. We derive a solution for the case of dependent attributes that is based on a transformation of the instances into a space of independent feature functions. We derive an optimization problem that produces a mapping into a space of independent binary feature vectors; the features can reflect arbitrary dependencies in the input space. This problem setting is motivated by the application of spam filtering for email service providers. Spam traps deliver a real-time stream of messages known to be spam. If elements of the same campaign can be recognized reliably, entire spam and phishing campaigns can be contained. We present a case study that evaluates Bayesian clustering for this application.</p></div></span> <a id="expcoll49" href="JavaScript: expandcollapse('expcoll49',49)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553425">Efficient learning algorithms for changing environments</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81300137601">Elad Hazan</a>,
<a href="author_page.cfm?id=81384613020">C. Seshadhri</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 393-400</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553425" title="DOI">10.1145/1553374.1553425</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553425&ftid=640151&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow50" style="display:inline;"><br /><div style="display:inline">We study online learning in an oblivious changing environment. The standard measure of regret bounds the difference between the cost of the online learner and the best decision in hindsight. Hence, regret minimizing algorithms tend to converge to the ...</div></span>
<span id="toHide50" style="display:none;"><br /><div style="display:inline"><p>We study online learning in an oblivious changing environment. The standard measure of regret bounds the difference between the cost of the online learner and the best decision in hindsight. Hence, regret minimizing algorithms tend to converge to the static best optimum, clearly a suboptimal behavior in changing environments. On the other hand, various metrics proposed to strengthen regret and allow for more dynamic algorithms produce inefficient algorithms.</p> <p>We propose a different performance metric which strengthens the standard metric of regret and measures performance with respect to a changing comparator. We then describe a series of data-streaming-based reductions which transform algorithms for minimizing (standard) regret into adaptive algorithms albeit incurring only poly-logarithmic computational overhead.</p> <p>Using this reduction, we obtain <i>efficient</i> low adaptive-regret algorithms for the problem of online convex optimization. This can be applied to various learning scenarios, i.e. online portfolio selection, for which we describe experimental results showing the advantage of adaptivity.</p></div></span> <a id="expcoll50" href="JavaScript: expandcollapse('expcoll50',50)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553426">Hoeffding and Bernstein races for selecting policies in evolutionary direct policy search</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81381603614">Verena Heidrich-Meisner</a>,
<a href="author_page.cfm?id=81332506095">Christian Igel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 401-408</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553426" title="DOI">10.1145/1553374.1553426</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553426&ftid=640153&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow51" style="display:inline;"><br /><div style="display:inline">Uncertainty arises in reinforcement learning from various sources, and therefore it is necessary to consider statistics based on several roll-outs for evaluating behavioral policies. We add an adaptive uncertainty handling based on Hoeffding and empirical ...</div></span>
<span id="toHide51" style="display:none;"><br /><div style="display:inline"><p>Uncertainty arises in reinforcement learning from various sources, and therefore it is necessary to consider statistics based on several roll-outs for evaluating behavioral policies. We add an adaptive uncertainty handling based on Hoeffding and empirical Bernstein races to the CMA-ES, a variable metric evolution strategy proposed for direct policy search. The uncertainty handling adjusts individually the number of episodes considered for the evaluation of a policy. The performance estimation is kept just accurate enough for a sufficiently good ranking of candidate policies, which is in turn sufficient for the CMA-ES to find better solutions. This increases the learning speed as well as the robustness of the algorithm.</p></div></span> <a id="expcoll51" href="JavaScript: expandcollapse('expcoll51',51)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553427">Partially supervised feature selection with regularized linear models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81442593675">Thibault Helleputte</a>,
 <a href="author_page.cfm?id=81100487662">Pierre Dupont</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 409-416</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553427" title="DOI">10.1145/1553374.1553427</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553427&ftid=640155&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow52" style="display:inline;"><br /><div style="display:inline">This paper addresses feature selection techniques for classification of high dimensional data, such as those produced by microarray experiments. Some prior knowledge may be available in this context to bias the selection towards some dimensions (genes) ...</div></span>
<span id="toHide52" style="display:none;"><br /><div style="display:inline"><p>This paper addresses feature selection techniques for classification of high dimensional data, such as those produced by microarray experiments. Some prior knowledge may be available in this context to bias the selection towards some dimensions (genes) <i>a priori</i> assumed to be more relevant. We propose a feature selection method making use of this partial supervision. It extends previous works on embedded feature selection with linear models including regularization to enforce sparsity. A practical approximation of this technique reduces to standard SVM learning with iterative rescaling of the inputs. The scaling factors depend here on the prior knowledge but the final selection may depart from it. Practical results on several microarray data sets show the benefits of the proposed approach in terms of the stability of the selected gene lists with improved classification performances.</p></div></span> <a id="expcoll52" href="JavaScript: expandcollapse('expcoll52',52)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553429">Learning with structured sparsity</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384614547">Junzhou Huang</a>,
<a href="author_page.cfm?id=81100114170">Tong Zhang</a>,
<a href="author_page.cfm?id=81409597622">Dimitris Metaxas</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 417-424</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553429" title="DOI">10.1145/1553374.1553429</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553429&ftid=640157&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow53" style="display:inline;"><br /><div style="display:inline">This paper investigates a new learning formulation called structured sparsity, which is a natural extension of the standard sparsity concept in statistical learning and compressive sensing. By allowing arbitrary structures on the feature set, ...</div></span>
<span id="toHide53" style="display:none;"><br /><div style="display:inline"><p>This paper investigates a new learning formulation called <i>structured sparsity</i>, which is a natural extension of the standard sparsity concept in statistical learning and compressive sensing. By allowing arbitrary structures on the feature set, this concept generalizes the group sparsity idea. A general theory is developed for learning with structured sparsity, based on the notion of coding complexity associated with the structure. Moreover, a structured greedy algorithm is proposed to efficiently solve the structured sparsity problem. Experiments demonstrate the advantage of structured sparsity over standard sparsity.</p></div></span> <a id="expcoll53" href="JavaScript: expandcollapse('expcoll53',53)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553430">Learning linear dynamical systems without sequence information</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435598244">Tzu-Kuo Huang</a>,
<a href="author_page.cfm?id=81100155456">Jeff Schneider</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 425-432</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553430" title="DOI">10.1145/1553374.1553430</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553430&ftid=640159&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow54" style="display:inline;"><br /><div style="display:inline">Virtually all methods of learning dynamic systems from data start from the same basic assumption: that the learning algorithm will be provided with a sequence, or trajectory, of data generated from the dynamic system. In this paper we consider the case ...</div></span>
<span id="toHide54" style="display:none;"><br /><div style="display:inline"><p>Virtually all methods of learning dynamic systems from data start from the same basic assumption: that the learning algorithm will be provided with a sequence, or trajectory, of data generated from the dynamic system. In this paper we consider the case where the data is not sequenced. The learning algorithm is presented a set of data points from the system's operation but with no temporal ordering. The data are simply drawn as individual disconnected points.</p> <p>While making this assumption may seem absurd at first glance, we observe that many scientific modeling tasks have exactly this property. In this paper we restrict our attention to learning linear, discrete time models. We propose several algorithms for learning these models based on optimizing approximate likelihood functions and test the methods on several synthetic data sets.</p></div></span> <a id="expcoll54" href="JavaScript: expandcollapse('expcoll54',54)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553431">Group lasso with overlap and graph lasso</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81351597923">Laurent Jacob</a>,
<a href="author_page.cfm?id=81458651702">Guillaume Obozinski</a>,
<a href="author_page.cfm?id=81100310507">Jean-Philippe Vert</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 433-440</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553431" title="DOI">10.1145/1553374.1553431</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553431&ftid=640161&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow55" style="display:inline;"><br /><div style="display:inline">We propose a new penalty function which, when used as regularization for empirical risk minimization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of co-variates defined ...</div></span>
<span id="toHide55" style="display:none;"><br /><div style="display:inline"><p>We propose a new penalty function which, when used as regularization for empirical risk minimization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of co-variates defined a priori, or a set of covariates which tend to be connected to each other when a graph of covariates is given. We study theoretical properties of the estimator, and illustrate its behavior on simulated and breast cancer gene expression data.</p></div></span> <a id="expcoll55" href="JavaScript: expandcollapse('expcoll55',55)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553432">Graph construction and <i>b</i>-matching for semi-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100510477">Tony Jebara</a>,
<a href="author_page.cfm?id=81350589070">Jun Wang</a>,
<a href="author_page.cfm?id=81406599942">Shih-Fu Chang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 441-448</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553432" title="DOI">10.1145/1553374.1553432</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553432&ftid=640163&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow56" style="display:inline;"><br /><div style="display:inline">Graph based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems. A crucial step in graph based SSL methods is the conversion of data into a weighted graph. However, most of the SSL literature ...</div></span>
<span id="toHide56" style="display:none;"><br /><div style="display:inline"><p>Graph based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems. A crucial step in graph based SSL methods is the conversion of data into a weighted graph. However, most of the SSL literature focuses on developing label inference algorithms without extensively studying the graph building method and its effect on performance. This article provides an empirical study of leading semi-supervised methods under a wide range of graph construction algorithms. These SSL inference algorithms include the Local and Global Consistency (<i>LGC</i>) method, the Gaussian Random Field (<i>GRF</i>) method, the Graph Transduction via Alternating Minimization (<i>GTAM</i>) method as well as other techniques. Several approaches for graph construction, sparsification and weighting are explored including the popular <i>k</i>-nearest neighbors method (<i>kNN</i>) and the <i>b</i>-matching method. As opposed to the greedily constructed <i>kNN</i> graph, the <i>b</i>-matched graph ensures each node in the graph has the same number of edges and produces a balanced or regular graph. Experimental results on both artificial data and real benchmark datasets indicate that <i>b</i>-matching produces more robust graphs and therefore provides significantly better prediction accuracy without any significant change in computation time.</p></div></span> <a id="expcoll56" href="JavaScript: expandcollapse('expcoll56',56)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553433">Trajectory prediction: learning to map situations to robot trajectories</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435610192">Nikolay Jetchev</a>,
<a href="author_page.cfm?id=81331505904">Marc Toussaint</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 449-456</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553433" title="DOI">10.1145/1553374.1553433</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553433&ftid=640165&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow57" style="display:inline;"><br /><div style="display:inline">Trajectory planning and optimization is a fundamental problem in articulated robotics. Algorithms used typically for this problem compute optimal trajectories from scratch in a new situation. In effect, extensive data is accumulated containing situations ...</div></span>
<span id="toHide57" style="display:none;"><br /><div style="display:inline"><p>Trajectory planning and optimization is a fundamental problem in articulated robotics. Algorithms used typically for this problem compute optimal trajectories from scratch in a new situation. In effect, extensive data is accumulated containing situations together with the respective optimized trajectories - but this data is in practice hardly exploited. The aim of this paper is to learn from this data. Given a new situation we want to predict a suitable trajectory which only needs minor refinement by a conventional optimizer. Our approach has two essential ingredients. First, to generalize from previous situations to new ones we need an appropriate situation descriptor - we propose a sparse feature selection approach to find such well-generalizing features of situations. Second, the transfer of previously optimized trajectories to a new situation should not be made in joint angle space - we propose a more efficient task space transfer of old trajectories to new situations. Experiments on a simulated humanoid reaching problem show that we can predict reasonable motion prototypes in new situations for which the refinement is much faster than an optimization from scratch.</p></div></span> <a id="expcoll57" href="JavaScript: expandcollapse('expcoll57',57)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553434">An accelerated gradient method for trace norm minimization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489125">Shuiwang Ji</a>,
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 457-464</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553434" title="DOI">10.1145/1553374.1553434</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553434&ftid=640167&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow58" style="display:inline;"><br /><div style="display:inline">We consider the minimization of a smooth loss function regularized by the trace norm of the matrix variable. Such formulation finds applications in many machine learning tasks including multi-task learning, matrix classification, and matrix completion. ...</div></span>
<span id="toHide58" style="display:none;"><br /><div style="display:inline"><p>We consider the minimization of a smooth loss function regularized by the trace norm of the matrix variable. Such formulation finds applications in many machine learning tasks including multi-task learning, matrix classification, and matrix completion. The standard semidefinite programming formulation for this problem is computationally expensive. In addition, due to the non-smooth nature of the trace norm, the optimal first-order black-box method for solving such class of problems converges as <i>O</i>(1/&radic;<i>k</i>), where <i>k</i> is the iteration counter. In this paper, we exploit the special structure of the trace norm, based on which we propose an extended gradient algorithm that converges as <i>O</i>(1/<i>k</i>). We further propose an accelerated gradient algorithm, which achieves the optimal convergence rate of <i>O</i>(1/<i>k</i><sup>2</sup>) for smooth problems. Experiments on multi-task learning problems demonstrate the efficiency of the proposed algorithms.</p></div></span> <a id="expcoll58" href="JavaScript: expandcollapse('expcoll58',58)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553435">A novel lexicalized HMM-based learning framework for web opinion mining</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81451595985">Wei Jin</a>,
<a href="author_page.cfm?id=81375610304">Hung Hay Ho</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 465-472</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553435" title="DOI">10.1145/1553374.1553435</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow59" style="display:inline;"><br /><div style="display:inline">NOTE FROM ACM: A Joint ACM Conference Committee has determined that the authors of this article violated ACM's publication policy on simultaneous submissions. Therefore ACM has shut off access to this paper.</div></span>
<span id="toHide59" style="display:none;"><br /><div style="display:inline"><h4>NOTE FROM ACM: A Joint ACM Conference Committee has determined that the authors of this article violated ACM's publication policy on simultaneous submissions. Therefore ACM has shut off access to this paper.</h4></div></span> <a id="expcoll59" href="JavaScript: expandcollapse('expcoll59',59)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553436">Orbit-product representation and correction of Gaussian belief propagation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81339507233">Jason K. Johnson</a>,
<a href="author_page.cfm?id=81435609434">Vladimir Y. Chernyak</a>,
<a href="author_page.cfm?id=81430601748">Michael Chertkov</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 473-480</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553436" title="DOI">10.1145/1553374.1553436</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553436&ftid=640171&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow60" style="display:inline;"><br /><div style="display:inline">We present a new view of Gaussian belief propagation (GaBP) based on a representation of the determinant as a product over orbits of a graph. We show that the GaBP determinant estimate captures totally backtracking orbits of the graph and consider how ...</div></span>
<span id="toHide60" style="display:none;"><br /><div style="display:inline"><p>We present a new view of Gaussian belief propagation (GaBP) based on a representation of the determinant as a product over orbits of a graph. We show that the GaBP determinant estimate captures totally backtracking orbits of the graph and consider how to correct this estimate. We show that the missing orbits may be grouped into equivalence classes corresponding to backtrackless orbits and the contribution of each equivalence class is easily determined from the GaBP solution. Furthermore, we demonstrate that this multiplicative correction factor can be interpreted as the determinant of a backtrackless adjacency matrix of the graph with edge weights based on GaBP. Finally, an efficient method is proposed to compute a truncated correction factor including all backtrackless orbits up to a specified length.</p></div></span> <a id="expcoll60" href="JavaScript: expandcollapse('expcoll60',60)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553437">A Bayesian approach to protein model quality assessment</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81321493416">Hetunandan Kamisetty</a>,
<a href="author_page.cfm?id=81100130080">Christopher J. Langmead</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 481-488</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553437" title="DOI">10.1145/1553374.1553437</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553437&ftid=640173&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow61" style="display:inline;"><br /><div style="display:inline">Given multiple possible models b1, b2, ... bn for a protein structure, a common sub-task in in-silico Protein Structure Prediction is ranking these models according to their quality. ...</div></span>
<span id="toHide61" style="display:none;"><br /><div style="display:inline"><p>Given multiple possible models <b>b</b><sub>1</sub>, <b>b</b><sub>2</sub>, ... <b>b</b><sub><i>n</i></sub> for a protein structure, a common sub-task in <i>in-silico</i> Protein Structure Prediction is ranking these models according to their quality. Extant approaches use MLE estimates of parameters <b>r</b><sub><i>i</i></sub> to obtain point estimates of the Model Quality. We describe a Bayesian alternative to assessing the quality of these models that builds an MRF over the parameters of each model and performs approximate inference to integrate over them. Hyperparameters <b>w</b> are learnt by optimizing a list-wise loss function over training data. Our results indicate that our Bayesian approach can significantly outperform MLE estimates and that optimizing the hyper-parameters can further improve results.</p></div></span> <a id="expcoll61" href="JavaScript: expandcollapse('expcoll61',61)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553438">Learning prediction suffix trees with Winnow</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421600112">Nikos Karampatziakis</a>,
<a href="author_page.cfm?id=81100623764">Dexter Kozen</a>
 </span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 489-496</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553438" title="DOI">10.1145/1553374.1553438</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553438&ftid=640174&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow62" style="display:inline;"><br /><div style="display:inline">Prediction suffix trees (PSTs) are a popular tool for modeling sequences and have been successfully applied in many domains such as compression and language modeling. In this work we adapt the well studied Winnow algorithm to the task of learning PSTs. ...</div></span>
<span id="toHide62" style="display:none;"><br /><div style="display:inline"><p>Prediction suffix trees (PSTs) are a popular tool for modeling sequences and have been successfully applied in many domains such as compression and language modeling. In this work we adapt the well studied Winnow algorithm to the task of learning PSTs. The proposed algorithm automatically grows the tree, so that it provably remains competitive with any fixed PST determined in hindsight. At the same time we prove that the depth of the tree grows only logarithmically with the number of mistakes made by the algorithm. Finally, we empirically demonstrate its effectiveness in two different tasks.</p></div></span> <a id="expcoll62" href="JavaScript: expandcollapse('expcoll62',62)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553439">Boosting products of base classifiers</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100374806">Bal&#225;zs K&#233;gl</a>,
<a href="author_page.cfm?id=81385599516">R&#243;bert Busa-Fekete</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 497-504</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553439" title="DOI">10.1145/1553374.1553439</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553439&ftid=640175&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow63" style="display:inline;"><br /><div style="display:inline">In this paper we show how to boost products of simple base learners. Similarly to trees, we call the base learner as a subroutine but in an iterative rather than recursive fashion. The main advantage of the proposed method is its simplicity and computational ...</div></span>
 <span id="toHide63" style="display:none;"><br /><div style="display:inline"><p>In this paper we show how to boost products of simple base learners. Similarly to trees, we call the base learner as a subroutine but in an iterative rather than recursive fashion. The main advantage of the proposed method is its simplicity and computational efficiency. On benchmark datasets, our boosted products of decision stumps clearly outperform boosted trees, and on the MNIST dataset the algorithm achieves the second best result among no-domain-knowledge algorithms after deep belief nets. As a second contribution, we present an improved base learner for nominal features and show that boosting the product of two of these new subset indicator base learners solves the maximum margin matrix factorization problem used to formalize the collaborative filtering task. On a small benchmark dataset, we get experimental results comparable to the semi-definite-programming-based solution but at a much lower computational cost.</p></div></span> <a id="expcoll63" href="JavaScript: expandcollapse('expcoll63',63)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553440">Learning Markov logic network structure via hypergraph lifting</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309513190">Stanley Kok</a>,
<a href="author_page.cfm?id=81100205908">Pedro Domingos</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 505-512</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553440" title="DOI">10.1145/1553374.1553440</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553440&ftid=640176&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow64" style="display:inline;"><br /><div style="display:inline">Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. Learning MLN structure from a relational database involves learning the clauses and ...</div></span>
<span id="toHide64" style="display:none;"><br /><div style="display:inline"><p>Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. Learning MLN structure from a relational database involves learning the clauses and weights. The state-of-the-art MLN structure learners all involve some element of greedily generating candidate clauses, and are susceptible to local optima. To address this problem, we present an approach that directly utilizes the data in constructing candidates. A relational database can be viewed as a hypergraph with constants as nodes and relations as hyperedges. We find paths of true ground atoms in the hypergraph that are connected via their arguments. To make this tractable (there are exponentially many paths in the hypergraph), we <i>lift</i> the hypergraph by jointly clustering the constants to form higherlevel concepts, and find paths in it. We variabilize the ground atoms in each path, and use them to form clauses, which are evaluated using a pseudo-likelihood measure. In our experiments on three real-world datasets, we find that our algorithm outperforms the state-of-the-art approaches.</p></div></span> <a id="expcoll64" href="JavaScript: expandcollapse('expcoll64',64)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553441">Near-Bayesian exploration in polynomial time</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100095014">J. Zico Kolter</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 513-520</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553441" title="DOI">10.1145/1553374.1553441</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553441&ftid=640177&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow65" style="display:inline;"><br /><div style="display:inline">We consider the exploration/exploitation problem in reinforcement learning (RL). The Bayesian approach to model-based RL offers an elegant solution to this problem, by considering a distribution over possible models and acting to maximize expected reward; ...</div></span>
<span id="toHide65" style="display:none;"><br /><div style="display:inline"><p>We consider the exploration/exploitation problem in reinforcement learning (RL). The Bayesian approach to model-based RL offers an elegant solution to this problem, by considering a distribution over possible models and acting to maximize expected reward; unfortunately, the Bayesian solution is intractable for all but very restricted cases. In this paper we present a simple algorithm, and prove that with high probability it is able to perform &epsilon;-close to the true (intractable) optimal Bayesian policy after some small (polynomial in quantities describing the system) number of time steps. The algorithm and analysis are motivated by the so-called PAC-MDP approach, and extend such results into the setting of Bayesian RL. In this setting, we show that we can achieve lower sample complexity bounds than existing algorithms, while using an exploration strategy that is much greedier than the (extremely cautious) exploration of PAC-MDP algorithms.</p></div></span> <a id="expcoll65" href="JavaScript: expandcollapse('expcoll65',65)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553442">Regularization and feature selection in least-squares temporal difference learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100095014">J. Zico Kolter</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 521-528</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553442" title="DOI">10.1145/1553374.1553442</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553442&ftid=640178&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow66" style="display:inline;"><br /><div style="display:inline">We consider the task of reinforcement learning with linear value function approximation. Temporal difference algorithms, and in particular the Least-Squares Temporal Difference (LSTD) algorithm, provide a method for learning the parameters of the value ...</div></span>
<span id="toHide66" style="display:none;"><br /><div style="display:inline"><p>We consider the task of reinforcement learning with linear value function approximation. Temporal difference algorithms, and in particular the Least-Squares Temporal Difference (LSTD) algorithm, provide a method for learning the parameters of the value function, but when the number of features is large this algorithm can over-fit to the data and is computationally expensive. In this paper, we propose a regularization framework for the LSTD algorithm that overcomes these difficulties. In particular, we focus on the case of <i>l</i><sub>1</sub> regularization, which is robust to irrelevant features and also serves as a method for feature selection. Although the <i>l</i><sub>1</sub> regularized LSTD solution cannot be expressed as a convex optimization problem, we present an algorithm similar to the Least Angle Regression (LARS) algorithm that can efficiently compute the optimal solution. Finally, we demonstrate the performance of the algorithm experimentally.</p></div></span> <a id="expcoll66" href="JavaScript: expandcollapse('expcoll66',66)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553443">The graphlet spectrum</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100583105">Risi Kondor</a>,
<a href="author_page.cfm?id=81435608320">Nino Shervashidze</a>,
<a href="author_page.cfm?id=81100155678">Karsten M. Borgwardt</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 529-536</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553443" title="DOI">10.1145/1553374.1553443</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553443&ftid=640179&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow67" style="display:inline;"><br /><div style="display:inline">Current graph kernels suffer from two limitations: graph kernels based on counting particular types of subgraphs ignore the relative position of these subgraphs to each other, while graph kernels based on algebraic methods are limited to graphs without ...</div></span>
<span id="toHide67" style="display:none;"><br /><div style="display:inline"><p>Current graph kernels suffer from two limitations: graph kernels based on counting particular types of subgraphs ignore the relative position of these subgraphs to each other, while graph kernels based on algebraic methods are limited to graphs without node labels. In this paper we present the <i>graphlet spectrum</i>, a system of graph invariants derived by means of group representation theory that capture information about the number as well as the position of labeled subgraphs in a given graph. In our experimental evaluation the graphlet spectrum outperforms state-of-the-art graph kernels.</p></div></span> <a id="expcoll67" href="JavaScript: expandcollapse('expcoll67',67)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553444">Rule learning with monotonicity constraints</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365594567">Wojciech Kot&#322;owski</a>,
<a href="author_page.cfm?id=81100048648">Roman S&#322;owi&#324;ski</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 537-544</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553444" title="DOI">10.1145/1553374.1553444</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553444&ftid=640180&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow68" style="display:inline;"><br /><div style="display:inline">In classification with monotonicity constraints, it is assumed that the class label should increase with increasing values on the attributes. In this paper we aim at formalizing the approach to learning with monotonicity constraints from statistical ...</div></span>
<span id="toHide68" style="display:none;"><br /><div style="display:inline"><p>In classification with monotonicity constraints, it is assumed that the class label should increase with increasing values on the attributes. In this paper we aim at formalizing the approach to learning with monotonicity constraints from statistical point of view. Motivated by the statistical analysis, we present an algorithm for learning rule ensembles. The algorithm first "monotonizes" the data using a nonparametric classification procedure and then generates a rule ensemble consistent with the training set. The procedure is justified by a theoretical analysis and verified in a computational experiment.</p></div></span> <a id="expcoll68" href="JavaScript: expandcollapse('expcoll68',68)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553445">Multiple indefinite kernel learning with mixed norm regularization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435595330">Matthieu Kowalski</a>,
<a href="author_page.cfm?id=81435607384">Marie Szafranski</a>,
<a href="author_page.cfm?id=81100251676">Liva Ralaivola</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 545-552</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553445" title="DOI">10.1145/1553374.1553445</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553445&ftid=640181&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow69" style="display:inline;"><br /><div style="display:inline">We address the problem of learning classifiers using several kernel functions. On the contrary to many contributions in the field of learning from different sources of information using kernels, we here do not assume that the kernels used are positive ...</div></span>
<span id="toHide69" style="display:none;"><br /><div style="display:inline"><p>We address the problem of learning classifiers using several kernel functions. On the contrary to many contributions in the field of learning from different sources of information using kernels, we here do not assume that the kernels used are positive definite. The learning problem that we are interested in involves a misclassification loss term and a regularization term that is expressed by means of a mixed norm. The use of a mixed norm allows us to enforce some sparsity structure, a particular case of which is, for instance, the Group Lasso. We solve the convex problem by employing proximal minimization algorithms, which can be viewed as refined versions of gradient descent procedures capable of naturally dealing with nondifferentiability. A numerical simulation on a Uci dataset shows the modularity of our approach.</p></div></span> <a id="expcoll69" href="JavaScript: expandcollapse('expcoll69',69)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553446">On sampling-based approximate spectral decomposition</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435594192">Sanjiv Kumar</a>,
<a href="author_page.cfm?id=81100197439">Mehryar Mohri</a>,
<a href="author_page.cfm?id=81421595327">Ameet Talwalkar</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 553-560</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553446" title="DOI">10.1145/1553374.1553446</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553446&ftid=640182&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow70" style="display:inline;"><br /><div style="display:inline">This paper addresses the problem of approximate singular value decomposition of large dense matrices that arises naturally in many machine learning applications. We discuss two recently introduced sampling-based spectral decomposition techniques: the ...</div></span>
<span id="toHide70" style="display:none;"><br /><div style="display:inline"><p>This paper addresses the problem of approximate singular value decomposition of large dense matrices that arises naturally in many machine learning applications. We discuss two recently introduced sampling-based spectral decomposition techniques: the Nystr&ouml;m and the Column-sampling methods. We present a theoretical comparison between the two methods and provide novel insights regarding their suitability for various applications. We then provide experimental results motivated by this theory. Finally, we propose an efficient adaptive sampling technique to select <i>informative</i> columns from the original matrix. This novel technique outperforms standard sampling methods on a variety of datasets.</p></div></span> <a id="expcoll70" href="JavaScript: expandcollapse('expcoll70',70)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553447">Learning spectral graph transformations for link prediction</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81361604934">J&#233;r&#244;me Kunegis</a>,
<a href="author_page.cfm?id=81100518897">Andreas Lommatzsch</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 561-568</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553447" title="DOI">10.1145/1553374.1553447</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553447&ftid=640183&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow71" style="display:inline;"><br /><div style="display:inline">We present a unified framework for learning link prediction and edge weight prediction functions in large networks, based on the transformation of a graph's algebraic spectrum. Our approach generalizes several graph kernels and dimensionality reduction ...</div></span>
<span id="toHide71" style="display:none;"><br /><div style="display:inline"><p>We present a unified framework for learning link prediction and edge weight prediction functions in large networks, based on the transformation of a graph's algebraic spectrum. Our approach generalizes several graph kernels and dimensionality reduction methods and provides a method to estimate their parameters efficiently. We show how the parameters of these prediction functions can be learned by reducing the problem to a one-dimensional regression problem whose runtime only depends on the method's reduced rank and that can be inspected visually. We derive variants that apply to undirected, weighted, unweighted, unipartite and bipartite graphs. We evaluate our method experimentally using examples from social networks, collaborative filtering, trust networks, citation networks, authorship graphs and hyperlink networks.</p></div></span> <a id="expcoll71" href="JavaScript: expandcollapse('expcoll71',71)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553448">Block-wise construction of acyclic relational features with monotone irreducibility and relevancy properties</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421599092">Ond&#345;ej Ku&#382;elka</a>,
<a href="author_page.cfm?id=81100354766">Filip &#382;elezn&#253;</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 569-576</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553448" title="DOI">10.1145/1553374.1553448</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553448&ftid=640184&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow72" style="display:inline;"><br /><div style="display:inline">We describe an algorithm for constructing a set of acyclic conjunctive relational features by combining smaller conjunctive blocks. Unlike traditional level-wise approaches which preserve the monotonicity of frequency, our block-wise approach preserves ...</div></span>
<span id="toHide72" style="display:none;"><br /><div style="display:inline"><p>We describe an algorithm for constructing a set of acyclic conjunctive relational features by combining smaller conjunctive blocks. Unlike traditional level-wise approaches which preserve the monotonicity of frequency, our block-wise approach preserves a form of monotonicity of the irreducibility and relevancy feature properties, which are important in propositionalization employed in the context of classification learning. With pruning based on these properties, our block-wise approach efficiently scales to features including tens of first-order literals, far beyond the reach of state-of-the art propositionalization or inductive logic programming systems.</p></div></span> <a id="expcoll72" href="JavaScript: expandcollapse('expcoll72',72)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553449">Generalization analysis of listwise learning-to-rank algorithms</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435599880">Yanyan Lan</a>,
<a href="author_page.cfm?id=81350580267">Tie-Yan Liu</a>,
<a href="author_page.cfm?id=81100447066">Zhiming Ma</a>,
<a href="author_page.cfm?id=81350598903">Hang Li</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 577-584</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553449" title="DOI">10.1145/1553374.1553449</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553449&ftid=640185&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow73" style="display:inline;"><br /><div style="display:inline">This paper presents a theoretical framework for ranking, and demonstrates how to perform generalization analysis of listwise ranking algorithms using the framework. Many learning-to-rank algorithms have been proposed in recent years. Among them, the ...</div></span>
<span id="toHide73" style="display:none;"><br /><div style="display:inline"><p>This paper presents a theoretical framework for ranking, and demonstrates how to perform generalization analysis of listwise ranking algorithms using the framework. Many learning-to-rank algorithms have been proposed in recent years. Among them, the listwise approach has shown higher empirical ranking performance when compared to the other approaches. However, there is no theoretical study on the listwise approach as far as we know. In this paper, we propose a theoretical framework for ranking, which can naturally describe various listwise learning-to-rank algorithms. With this framework, we prove a theorem which gives a generalization bound of a listwise ranking algorithm, on the basis of Rademacher Average of the class of compound functions. The compound functions take listwise loss functions as outer functions and ranking models as inner functions. We then compute the Rademacher Averages for existing listwise algorithms of ListMLE, ListNet, and RankCosine. We also discuss the tightness of the bounds in different situations with regard to the list length and transformation function.</p></div></span> <a id="expcoll73" href="JavaScript: expandcollapse('expcoll73',73)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553450">Approximate inference for planning in stochastic relational worlds</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435596339">Tobias Lang</a>,
<a href="author_page.cfm?id=81331505904">Marc Toussaint</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 585-592</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553450" title="DOI">10.1145/1553374.1553450</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553450&ftid=640186&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
 <tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow74" style="display:inline;"><br /><div style="display:inline">Relational world models that can be learned from experience in stochastic domains have received significant attention recently. However, efficient planning using these models remains a major issue. We propose to convert learned noisy probabilistic relational ...</div></span>
<span id="toHide74" style="display:none;"><br /><div style="display:inline"><p>Relational world models that can be learned from experience in stochastic domains have received significant attention recently. However, efficient planning using these models remains a major issue. We propose to convert learned noisy probabilistic relational rules into a structured dynamic Bayesian network representation. Predicting the effects of action sequences using approximate inference allows for planning in complex worlds. We evaluate the effectiveness of our approach for online planning in a 3D simulated blocksworld with an articulated manipulator and realistic physics. Empirical results show that our method can solve problems where existing methods fail.</p></div></span> <a id="expcoll74" href="JavaScript: expandcollapse('expcoll74',74)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553451">Learning nonlinear dynamic models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100453722">John Langford</a>,
<a href="author_page.cfm?id=81100562217">Ruslan Salakhutdinov</a>,
<a href="author_page.cfm?id=81100114170">Tong Zhang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 593-600</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553451" title="DOI">10.1145/1553374.1553451</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553451&ftid=640187&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow75" style="display:inline;"><br /><div style="display:inline">We present a novel approach for learning nonlinear dynamic models, which leads to a new set of tools capable of solving problems that are otherwise difficult. We provide theory showing this new approach is consistent for models with long range structure, ...</div></span>
<span id="toHide75" style="display:none;"><br /><div style="display:inline"><p>We present a novel approach for learning nonlinear dynamic models, which leads to a new set of tools capable of solving problems that are otherwise difficult. We provide theory showing this new approach is consistent for models with long range structure, and apply the approach to motion capture and high-dimensional video data, yielding results superior to standard alternatives.</p></div></span> <a id="expcoll75" href="JavaScript: expandcollapse('expcoll75',75)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553452">Non-linear matrix factorization with Gaussian processes</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100574233">Neil D. Lawrence</a>,
<a href="author_page.cfm?id=81100176326">Raquel Urtasun</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 601-608</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553452" title="DOI">10.1145/1553374.1553452</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553452&ftid=640188&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow76" style="display:inline;"><br /><div style="display:inline">A popular approach to collaborative filtering is matrix factorization. In this paper we develop a non-linear probabilistic matrix factorization using Gaussian process latent variable models. We use stochastic gradient descent (SGD) to optimize the model. ...</div></span>
<span id="toHide76" style="display:none;"><br /><div style="display:inline"><p>A popular approach to collaborative filtering is matrix factorization. In this paper we develop a non-linear probabilistic matrix factorization using Gaussian process latent variable models. We use stochastic gradient descent (SGD) to optimize the model. SGD allows us to apply Gaussian processes to data sets with millions of observations without approximate methods. We apply our approach to benchmark movie recommender data sets. The results show better than previous state-of-the-art performance.</p></div></span> <a id="expcoll76" href="JavaScript: expandcollapse('expcoll76',76)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553453">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333489568">Honglak Lee</a>,
<a href="author_page.cfm?id=81435607640">Roger Grosse</a>,
<a href="author_page.cfm?id=81435610046">Rajesh Ranganath</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 609-616</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553453" title="DOI">10.1145/1553374.1553453</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553453&ftid=640189&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow77" style="display:inline;"><br /><div style="display:inline">There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional ...</div></span>
<span id="toHide77" style="display:none;"><br /><div style="display:inline"><p>There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the <i>convolutional deep belief network</i>, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is <i>probabilistic max-pooling</i>, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.</p></div></span> <a id="expcoll77" href="JavaScript: expandcollapse('expcoll77',77)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553454">Transfer learning for collaborative filtering via a rating-matrix generative model</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81350587874">Bin Li</a>,
<a href="author_page.cfm?id=81372591186">Qiang Yang</a>,
<a href="author_page.cfm?id=81100142607">Xiangyang Xue</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 617-624</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553454" title="DOI">10.1145/1553374.1553454</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553454&ftid=640190&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow78" style="display:inline;"><br /><div style="display:inline">Cross-domain collaborative filtering solves the sparsity problem by transferring rating knowledge across multiple domains. In this paper, we propose a rating-matrix generative model (RMGM) for effective cross-domain collaborative filtering. We first ...</div></span>
<span id="toHide78" style="display:none;"><br /><div style="display:inline"><p>Cross-domain collaborative filtering solves the sparsity problem by transferring rating knowledge across multiple domains. In this paper, we propose a rating-matrix generative model (RMGM) for effective cross-domain collaborative filtering. We first show that the relatedness across multiple rating matrices can be established by finding a shared implicit cluster-level rating matrix, which is next extended to a cluster-level rating model. Consequently, a rating matrix of any related task can be viewed as drawing a set of users and items from a user-item joint mixture model as well as drawing the corresponding ratings from the cluster-level rating model. The combination of these two models gives the RMGM, which can be used to fill the missing ratings for both existing and new users. A major advantage of RMGM is that it can share the knowledge by pooling the rating data from multiple tasks even when the users and items of these tasks do not overlap. We evaluate the RMGM empirically on three real-world collaborative filtering data sets to show that RMGM can outperform the individual models trained separately.</p></div></span> <a id="expcoll78" href="JavaScript: expandcollapse('expcoll78',78)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553455">ABC-boost: adaptive base class boost for multi-class classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317488756">Ping Li</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 625-632</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553455" title="DOI">10.1145/1553374.1553455</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553455&ftid=640191&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow79" style="display:inline;"><br /><div style="display:inline">We propose abc-boost (adaptive base class boost) for multi-class classification and present abc-mart, an implementation of abc-boost, based on the multinomial logit model. The key idea is that, at each boosting iteration, ...</div></span>
<span id="toHide79" style="display:none;"><br /><div style="display:inline"><p>We propose <b><i>abc-boost</i></b> (adaptive base class boost) for multi-class classification and present <b><i>abc-mart</i></b>, an implementation of <i>abc-boost</i>, based on the multinomial logit model. The key idea is that, at each boosting iteration, we <b><i>adaptively</i></b> and greedily choose a <b><i>base</i></b> class. Our experiments on public datasets demonstrate the improvement of <i>abc-mart</i> over the original <i>mart</i> algorithm.</p></div></span> <a id="expcoll79" href="JavaScript: expandcollapse('expcoll79',79)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553456">Semi-supervised learning using label mean</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81442606442">Yu-Feng Li</a>,
<a href="author_page.cfm?id=81100525095">James T. Kwok</a>,
<a href="author_page.cfm?id=81451593001">Zhi-Hua Zhou</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 633-640</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553456" title="DOI">10.1145/1553374.1553456</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553456&ftid=640192&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow80" style="display:inline;"><br /><div style="display:inline">Semi-Supervised Support Vector Machines (S3VMs) typically directly estimate the label assignments for the unlabeled instances. This is often inefficient even with recent advances in the efficient training of the (supervised) SVM. In this paper, we show ...</div></span>
<span id="toHide80" style="display:none;"><br /><div style="display:inline"><p>Semi-Supervised Support Vector Machines (S3VMs) typically directly estimate the label assignments for the unlabeled instances. This is often inefficient even with recent advances in the efficient training of the (supervised) SVM. In this paper, we show that S3VMs, with knowledge of the means of the class labels of the unlabeled data, is closely related to the supervised SVM with known labels on all the unlabeled data. This motivates us to first estimate the label means of the unlabeled data. Two versions of the <i>meanS3VM</i>, which work by maximizing the margin between the label means, are proposed. The first one is based on multiple kernel learning, while the second one is based on alternating optimization. Experiments show that both of the proposed algorithms achieve highly competitive and sometimes even the best performance as compared to the state-of-the-art semi-supervised learners. Moreover, they are more efficient than existing S3VMs.</p></div></span> <a id="expcoll80" href="JavaScript: expandcollapse('expcoll80',80)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553457">Learning from measurements in exponential families</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81323492904">Percy Liang</a>,
<a href="author_page.cfm?id=81339507945">Michael I. Jordan</a>,
<a href="author_page.cfm?id=81100142014">Dan Klein</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 641-648</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553457" title="DOI">10.1145/1553374.1553457</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553457&ftid=640193&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow81" style="display:inline;"><br /><div style="display:inline">Given a model family and a set of unlabeled examples, one could either label specific examples or state general constraints---both provide information about the desired model. In general, what is the most cost-effective way to learn? To address this ...</div></span>
<span id="toHide81" style="display:none;"><br /><div style="display:inline"><p>Given a model family and a set of unlabeled examples, one could either label specific examples or state general constraints---both provide information about the desired model. In general, what is the most cost-effective way to learn? To address this question, we introduce <i>measurements</i>, a general class of mechanisms for providing information about a target model. We present a Bayesian decision-theoretic framework, which allows us to both integrate diverse measurements and choose new measurements to make. We use a variational inference algorithm, which exploits exponential family duality. The merits of our approach are demonstrated on two sequence labeling tasks.</p></div></span> <a id="expcoll81" href="JavaScript: expandcollapse('expcoll81',81)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553458">Blockwise coordinate descent procedures for the multi-task lasso, with applications to neural semantic basis discovery</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435601850">Han Liu</a>,
<a href="author_page.cfm?id=81322502826">Mark Palatucci</a>,
<a href="author_page.cfm?id=81408598348">Jian Zhang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 649-656</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553458" title="DOI">10.1145/1553374.1553458</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553458&ftid=640194&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow82" style="display:inline;"><br /><div style="display:inline">We develop a cyclical blockwise coordinate descent algorithm for the multi-task Lasso that efficiently solves problems with thousands of features and tasks. The main result shows that a closed-form Winsorization operator can be obtained for the sup-norm ...</div></span>
<span id="toHide82" style="display:none;"><br /><div style="display:inline"><p>We develop a cyclical blockwise coordinate descent algorithm for the multi-task Lasso that efficiently solves problems with thousands of features and tasks. The main result shows that a closed-form Winsorization operator can be obtained for the sup-norm penalized least squares regression. This allows the algorithm to find solutions to very large-scale problems far more efficiently than existing methods. This result complements the pioneering work of Friedman, et al. (2007) for the single-task Lasso. As a case study, we use the multi-task Lasso as a variable selector to discover a semantic basis for predicting human neural activation. The learned solution outperforms the standard basis for this task on the majority of test participants, while requiring far fewer assumptions about cognitive neuroscience. We demonstrate how this learned basis can yield insights into how the brain represents the meanings of words.</p></div></span> <a id="expcoll82" href="JavaScript: expandcollapse('expcoll82',82)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553459">Efficient Euclidean projections in linear time</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81405595465">Jun Liu</a>,
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 657-664</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553459" title="DOI">10.1145/1553374.1553459</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553459&ftid=640195&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow83" style="display:inline;"><br /><div style="display:inline">We consider the problem of computing the Euclidean projection of a vector of length n onto a closed convex set including the l1 ball and the specialized polyhedra employed in (Shalev-Shwartz & Singer, 2006). These problems have ...</div></span>
<span id="toHide83" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of computing the Euclidean projection of a vector of length <i>n</i> onto a closed convex set including the <i>l</i><sub>1</sub> ball and the specialized polyhedra employed in (Shalev-Shwartz & Singer, 2006). These problems have played building block roles in solving several <i>l</i><sub>1</sub>-norm based sparse learning problems. Existing methods have a worst-case time complexity of <i>O</i>(<i>n</i> log <i>n</i>). In this paper, we propose to cast both Euclidean projections as root finding problems associated with specific auxiliary functions, which can be solved in linear time via bisection. We further make use of the special structure of the auxiliary functions, and propose an improved bisection algorithm. Empirical studies demonstrate that the proposed algorithms are much more efficient than the competing ones for computing the projections.</p></div></span> <a id="expcoll83" href="JavaScript: expandcollapse('expcoll83',83)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553460">Topic-link LDA: joint models of topic and author community</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81418597820">Yan Liu</a>,
<a href="author_page.cfm?id=81309509185">Alexandru Niculescu-Mizil</a>,
<a href="author_page.cfm?id=81435600825">Wojciech Gryc</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 665-672</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553460" title="DOI">10.1145/1553374.1553460</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553460&ftid=640196&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow84" style="display:inline;"><br /><div style="display:inline">Given a large-scale linked document collection, such as a collection of blog posts or a research literature archive, there are two fundamental problems that have generated a lot of interest in the research community. One is to identify a set of high-level ...</div></span>
<span id="toHide84" style="display:none;"><br /><div style="display:inline"><p>Given a large-scale linked document collection, such as a collection of blog posts or a research literature archive, there are two fundamental problems that have generated a lot of interest in the research community. One is to identify a set of high-level topics covered by the documents in the collection; the other is to uncover and analyze the social network of the authors of the documents. So far these problems have been viewed as separate problems and considered independently from each other. In this paper we argue that these two problems are in fact inter-dependent and should be addressed together. We develop a Bayesian hierarchical approach that performs topic modeling and author community discovery in one unified framework. The effectiveness of our model is demonstrated on two blog data sets in different domains and one research paper citation data from CiteSeer.</p></div></span> <a id="expcoll84" href="JavaScript: expandcollapse('expcoll84',84)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553461">Geometry-aware metric learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435594802">Zhengdong Lu</a>,
<a href="author_page.cfm?id=81547334756">Prateek Jain</a>,
<a href="author_page.cfm?id=81100098715">Inderjit S. Dhillon</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 673-680</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553461" title="DOI">10.1145/1553374.1553461</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553461&ftid=640197&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow85" style="display:inline;"><br /><div style="display:inline">In this paper, we introduce a generic framework for semi-supervised kernel learning. Given pair-wise (dis-)similarity constraints, we learn a kernel matrix over the data that respects the provided side-information as well as the local geometry of the ...</div></span>
<span id="toHide85" style="display:none;"><br /><div style="display:inline"><p>In this paper, we introduce a generic framework for semi-supervised kernel learning. Given pair-wise (dis-)similarity constraints, we learn a kernel matrix over the data that respects the provided side-information as well as the local geometry of the data. Our framework is based on metric learning methods, where we jointly model the metric/kernel over the data along with the underlying manifold. Furthermore, we show that for some important parameterized forms of the underlying manifold model, we can estimate the model parameters and the kernel matrix efficiently. Our resulting algorithm is able to incorporate local geometry into the metric learning task; at the same time it can handle a wide class of constraints. Finally, our algorithm is fast and scalable -- unlike most of the existing methods, it is able to exploit the low dimensional manifold structure and does not require semi-definite programming. We demonstrate wide applicability and effectiveness of our framework by applying to various machine learning tasks such as semi-supervised classification, colored dimensionality reduction, manifold alignment etc. On each of the tasks our method performs competitively or better than the respective state-of-the-art method.</p></div></span> <a id="expcoll85" href="JavaScript: expandcollapse('expcoll85',85)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553462">Identifying suspicious URLs: an application of large-scale online learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81436600149">Justin Ma</a>,
<a href="author_page.cfm?id=81100590935">Lawrence K. Saul</a>,
<a href="author_page.cfm?id=81100570405">Stefan Savage</a>,
<a href="author_page.cfm?id=81336493853">Geoffrey M. Voelker</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 681-688</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553462" title="DOI">10.1145/1553374.1553462</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553462&ftid=640198&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow86" style="display:inline;"><br /><div style="display:inline">This paper explores online learning approaches for detecting malicious Web sites (those involved in criminal scams) using lexical and host-based features of the associated URLs. We show that this application is particularly appropriate for online algorithms ...</div></span>
<span id="toHide86" style="display:none;"><br /><div style="display:inline"><p>This paper explores online learning approaches for detecting malicious Web sites (those involved in criminal scams) using lexical and host-based features of the associated URLs. We show that this application is particularly appropriate for online algorithms as the size of the training data is larger than can be efficiently processed in batch <i>and</i> because the distribution of features that typify malicious URLs is changing continuously. Using a real-time system we developed for gathering URL features, combined with a real-time source of labeled URLs from a large Web mail provider, we demonstrate that recently-developed online algorithms can be as accurate as batch techniques, achieving classification accuracies up to 99% over a balanced data set.</p></div></span> <a id="expcoll86" href="JavaScript: expandcollapse('expcoll86',86)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553463">Online dictionary learning for sparse coding</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81330494868">Julien Mairal</a>,
<a href="author_page.cfm?id=81100328355">Francis Bach</a>,
<a href="author_page.cfm?id=81100327459">Jean Ponce</a>,
<a href="author_page.cfm?id=81100582663">Guillermo Sapiro</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 689-696</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553463" title="DOI">10.1145/1553374.1553463</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553463&ftid=640199&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow87" style="display:inline;"><br /><div style="display:inline">Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, ...</div></span>
<span id="toHide87" style="display:none;"><br /><div style="display:inline"><p>Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on <i>learning</i> the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.</p></div></span> <a id="expcoll87" href="JavaScript: expandcollapse('expcoll87',87)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553464">Proto-predictive representation of states with simple recurrent temporal-difference networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=99659113684">Takaki Makino</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 697-704</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553464" title="DOI">10.1145/1553374.1553464</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553464&ftid=640200&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow88" style="display:inline;"><br /><div style="display:inline">We propose a new neural network architecture, called Simple Recurrent Temporal-Difference Networks (SR-TDNs), that learns to predict future observations in partially observable environments. SR-TDNs incorporate the structure of simple recurrent neural ...</div></span>
<span id="toHide88" style="display:none;"><br /><div style="display:inline"><p>We propose a new neural network architecture, called Simple Recurrent Temporal-Difference Networks (SR-TDNs), that learns to predict future observations in partially observable environments. SR-TDNs incorporate the structure of simple recurrent neural networks (SRNs) into temporal-difference (TD) networks to use proto-predictive representation of states. Although they deviate from the principle of predictive representations to ground state representations on observations, they follow the same learning strategy as TD networks, i.e., applying TD-learning to general predictions. Simulation experiments revealed that SR-TDNs can correctly represent states with an incomplete set of core tests (question networks), and consequently, SR-TDNs have better on-line learning capacity than TD networks in various environments.</p></div></span> <a id="expcoll88" href="JavaScript: expandcollapse('expcoll88',88)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553465">Sparse Gaussian graphical models with unknown block structure</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100063296">Benjamin M. Marlin</a>,
<a href="author_page.cfm?id=81333490204">Kevin P. Murphy</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 705-712</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553465" title="DOI">10.1145/1553374.1553465</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553465&ftid=640201&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow89" style="display:inline;"><br /><div style="display:inline">Recent work has shown that one can learn the structure of Gaussian Graphical Models by imposing an L1 penalty on the precision matrix, and then using efficient convex optimization methods to find the penalized maximum likelihood estimate. This is similar ...</div></span>
<span id="toHide89" style="display:none;"><br /><div style="display:inline"><p>Recent work has shown that one can learn the structure of Gaussian Graphical Models by imposing an L1 penalty on the precision matrix, and then using efficient convex optimization methods to find the penalized maximum likelihood estimate. This is similar to performing MAP estimation with a prior that prefers sparse graphs. In this paper, we use the stochastic block model as a prior. This prefer graphs that are blockwise sparse, but unlike previous work, it does not require that the blocks or groups be specified a priori. The resulting problem is no longer convex, but we devise an efficient variational Bayes algorithm to solve it. We show that our method has better test set likelihood on two different datasets (motion capture and gene expression) compared to independent L1, and can match the performance of group L1 using manually created groups.</p></div></span> <a id="expcoll89" href="JavaScript: expandcollapse('expcoll89',89)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553466">Polyhedral outer approximations with application to natural language parsing</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597553">Andr&#233; F. T. Martins</a>,
<a href="author_page.cfm?id=81329492133">Noah A. Smith</a>,
<a href="author_page.cfm?id=81407592503">Eric P. Xing</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 713-720</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553466" title="DOI">10.1145/1553374.1553466</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553466&ftid=640202&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow90" style="display:inline;"><br /><div style="display:inline">Recent approaches to learning structured predictors often require approximate inference for tractability; yet its effects on the learned model are unclear. Meanwhile, most learning algorithms act as if computational cost was constant within the model ...</div></span>
<span id="toHide90" style="display:none;"><br /><div style="display:inline"><p>Recent approaches to learning structured predictors often require approximate inference for tractability; yet its effects on the learned model are unclear. Meanwhile, most learning algorithms act as if computational cost was constant within the model class. This paper sheds some light on the first issue by establishing risk bounds for max-margin learning with LP relaxed inference and addresses the second issue by proposing a new paradigm that attempts to penalize "time-consuming" hypotheses. Our analysis relies on a geometric characterization of the outer polyhedra associated with the LP relaxation. We then apply these techniques to the problem of dependency parsing, for which a concise LP formulation is provided that handles non-local output features. A significant improvement is shown over arc-factored models.</p></div></span> <a id="expcoll90" href="JavaScript: expandcollapse('expcoll90',90)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553467">Partial order embedding with multiple kernels</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435611347">Brian McFee</a>,
<a href="author_page.cfm?id=81100118766">Gert Lanckriet</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 721-728</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553467" title="DOI">10.1145/1553374.1553467</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553467&ftid=640203&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow91" style="display:inline;"><br /><div style="display:inline">We consider the problem of embedding arbitrary objects (e.g., images, audio, documents) into Euclidean space subject to a partial order over pair-wise distances. Partial order constraints arise naturally when modeling human perception of similarity. ...</div></span>
<span id="toHide91" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of embedding arbitrary objects (e.g., images, audio, documents) into Euclidean space subject to a partial order over pair-wise distances. Partial order constraints arise naturally when modeling human perception of similarity. Our partial order framework enables the use of graph-theoretic tools to more efficiently produce the embedding, and exploit global structure within the constraint set.</p> <p>We present an embedding algorithm based on semidefinite programming, which can be parameterized by multiple kernels to yield a unified space from heterogeneous features.</p></div></span> <a id="expcoll91" href="JavaScript: expandcollapse('expcoll91',91)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553468">Bandit-based optimization on graphs with application to library performance tuning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435611873">Fr&#233;d&#233;ric de Mesmay</a>,
<a href="author_page.cfm?id=81435592527">Arpad Rimmel</a>,
<a href="author_page.cfm?id=81100647925">Yevgen Voronenko</a>,
<a href="author_page.cfm?id=81100268784">Markus P&#252;schel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 729-736</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553468" title="DOI">10.1145/1553374.1553468</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553468&ftid=640204&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow92" style="display:inline;"><br /><div style="display:inline">The problem of choosing fast implementations for a class of recursive algorithms such as the fast Fourier transforms can be formulated as an optimization problem over the language generated by a suitably defined grammar. We propose a novel algorithm ...</div></span>
<span id="toHide92" style="display:none;"><br /><div style="display:inline"><p>The problem of choosing fast implementations for a class of recursive algorithms such as the fast Fourier transforms can be formulated as an optimization problem over the language generated by a suitably defined grammar. We propose a novel algorithm that solves this problem by reducing it to maximizing an objective function over the sinks of a directed acyclic graph. This algorithm valuates nodes using Monte-Carlo and grows a subgraph in the most promising directions by considering local maximum <i>k</i>-armed bandits. When used inside an adaptive linear transform library, it cuts down the search time by an order of magnitude compared to the existing algorithm. In some cases, the performance of the implementations found is also increased by up to 10% which is of considerable practical importance since it consequently improves the performance of all applications using the library.</p></div></span> <a id="expcoll92" href="JavaScript: expandcollapse('expcoll92',92)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553469">Deep learning from temporal coherence in video</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81318497988">Hossein Mobahi</a>,
<a href="author_page.cfm?id=81100001072">Ronan Collobert</a>,
<a href="author_page.cfm?id=81100015405">Jason Weston</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 737-744</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553469" title="DOI">10.1145/1553374.1553469</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553469&ftid=640205&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow93" style="display:inline;"><br /><div style="display:inline">This work proposes a learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the ...</div></span>
<span id="toHide93" style="display:none;"><br /><div style="display:inline"><p>This work proposes a learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the same object or objects. This coherence is used as a supervisory signal over the unlabeled data, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and face recognition tasks.</p></div></span> <a id="expcoll93" href="JavaScript: expandcollapse('expcoll93',93)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553470">Regression by dependence minimization and its application to causal inference in additive noise models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81330495356">Joris Mooij</a>,
<a href="author_page.cfm?id=81100317961">Dominik Janzing</a>,
<a href="author_page.cfm?id=81442594400">Jonas Peters</a>,
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 745-752</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553470" title="DOI">10.1145/1553374.1553470</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553470&ftid=640206&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow94" style="display:inline;"><br /><div style="display:inline">Motivated by causal inference problems, we propose a novel method for regression that minimizes the statistical dependence between regressors and residuals. The key advantage of this approach to regression is that it does not assume a particular distribution ...</div></span>
<span id="toHide94" style="display:none;"><br /><div style="display:inline"><p>Motivated by causal inference problems, we propose a novel method for regression that minimizes the statistical dependence between regressors and residuals. The key advantage of this approach to regression is that it does not assume a particular distribution of the noise, i.e., it is non-parametric with respect to the noise distribution. We argue that the proposed regression method is well suited to the task of causal inference in additive noise models. A practical disadvantage is that the resulting optimization problem is generally non-convex and can be difficult to solve. Nevertheless, we report good results on one of the tasks of the NIPS 2008 Causality Challenge, where the goal is to distinguish causes from effects in pairs of statistically dependent variables. In addition, we propose an algorithm for efficiently inferring causal models from observational data for more than two variables. The required number of regressions and independence tests is quadratic in the number of variables, which is a significant improvement over the simple method that tests all possible DAGs.</p></div></span> <a id="expcoll94" href="JavaScript: expandcollapse('expcoll94',94)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553471">Learning complex motions by sequencing simpler motion templates</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435601768">Gerhard Neumann</a>,
<a href="author_page.cfm?id=81100339916">Wolfgang Maass</a>,
<a href="author_page.cfm?id=81100524321">Jan Peters</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 753-760</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553471" title="DOI">10.1145/1553374.1553471</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553471&ftid=640207&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow95" style="display:inline;"><br /><div style="display:inline">Abstraction of complex, longer motor tasks into simpler elemental movements enables humans and animals to exhibit motor skills which have not yet been matched by robots. Humans intuitively decompose complex motions into smaller, simpler segments. For ...</div></span>
<span id="toHide95" style="display:none;"><br /><div style="display:inline"><p>Abstraction of complex, longer motor tasks into simpler elemental movements enables humans and animals to exhibit motor skills which have not yet been matched by robots. Humans intuitively decompose complex motions into smaller, simpler segments. For example when describing simple movements like drawing a triangle with a pen, we can easily name the basic steps of this movement.</p> <p>Surprisingly, such abstractions have rarely been used in artificial motor skill learning algorithms. These algorithms typically choose a new action (such as a torque or a force) at a very fast time-scale. As a result, both policy and temporal credit assignment problem become unnecessarily complex - often beyond the reach of current machine learning methods.</p> <p>We introduce a new framework for temporal abstractions in reinforcement learning (RL), i.e. RL with motion templates. We present a new algorithm for this framework which can learn high-quality policies by making only few abstract decisions.</p></div></span> <a id="expcoll95" href="JavaScript: expandcollapse('expcoll95',95)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553472">Convex variational Bayesian inference for large scale generalized linear models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421600271">Hannes Nickisch</a>,
<a href="author_page.cfm?id=81100511719">Matthias W. Seeger</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 761-768</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553472" title="DOI">10.1145/1553374.1553472</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553472&ftid=640208&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow96" style="display:inline;"><br /><div style="display:inline">We show how variational Bayesian inference can be implemented for very large generalized linear models. Our relaxation is proven to be a convex problem for any log-concave model. We provide a generic double loop algorithm for solving this relaxation ...</div></span>
<span id="toHide96" style="display:none;"><br /><div style="display:inline"><p>We show how variational Bayesian inference can be implemented for very large generalized linear models. Our relaxation is proven to be a convex problem for any log-concave model. We provide a generic double loop algorithm for solving this relaxation on models with arbitrary super-Gaussian potentials. By iteratively decoupling the criterion, most of the work can be done by solving large linear systems, rendering our algorithm orders of magnitude faster than previously proposed solvers for the same problem. We evaluate our method on problems of Bayesian active learning for large binary classification models, and show how to address settings with many candidates and sequential inclusion steps.</p></div></span> <a id="expcoll96" href="JavaScript: expandcollapse('expcoll96',96)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553473">Solution stability in linear programming relaxations: graph partitioning and unsupervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81414597536">Sebastian Nowozin</a>,
<a href="author_page.cfm?id=81435610088">Stefanie Jegelka</a>
</span>
</td>
</tr>

<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 769-776</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553473" title="DOI">10.1145/1553374.1553473</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553473&ftid=640209&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow97" style="display:inline;"><br /><div style="display:inline">We propose a new method to quantify the solution stability of a large class of combinatorial optimization problems arising in machine learning. As practical example we apply the method to correlation clustering, clustering aggregation, modularity clustering, ...</div></span>
<span id="toHide97" style="display:none;"><br /><div style="display:inline"><p>We propose a new method to quantify the solution stability of a large class of combinatorial optimization problems arising in machine learning. As practical example we apply the method to correlation clustering, clustering aggregation, modularity clustering, and relative performance significance clustering. Our method is extensively motivated by the idea of linear programming relaxations. We prove that when a <i>relaxation</i> is used to solve the original clustering problem, then the solution stability calculated by our method is conservative, that is, it never overestimates the solution stability of the true, unrelaxed problem. We also demonstrate how our method can be used to compute the entire <i>path of optimal solutions</i> as the optimization problem is increasingly perturbed. Experimentally, our method is shown to perform well on a number of benchmark problems.</p></div></span> <a id="expcoll97" href="JavaScript: expandcollapse('expcoll97',97)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553474">Nonparametric factor analysis with beta process priors</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435603002">John Paisley</a>,
<a href="author_page.cfm?id=81100223788">Lawrence Carin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 777-784</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553474" title="DOI">10.1145/1553374.1553474</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553474&ftid=640210&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow98" style="display:inline;"><br /><div style="display:inline">We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing ...</div></span>
<span id="toHide98" style="display:none;"><br /><div style="display:inline"><p>We propose a nonparametric extension to the factor analysis problem using a beta process prior. This <i>beta process factor analysis</i> (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.</p></div></span> <a id="expcoll98" href="JavaScript: expandcollapse('expcoll98',98)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553475">Unsupervised hierarchical modeling of locomotion styles</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81542455556">Wei Pan</a>,
<a href="author_page.cfm?id=81384600322">Lorenzo Torresani</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 785-792</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553475" title="DOI">10.1145/1553374.1553475</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553475&ftid=640211&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow99" style="display:inline;"><br /><div style="display:inline">This paper describes an unsupervised learning technique for modeling human locomotion styles, such as distinct related activities (e.g. running and striding) or variations of the same motion performed by different subjects. Modeling motion styles requires ...</div></span>
<span id="toHide99" style="display:none;"><br /><div style="display:inline"><p>This paper describes an unsupervised learning technique for modeling human locomotion styles, such as distinct related activities (e.g. running and striding) or variations of the same motion performed by different subjects. Modeling motion styles requires identifying the common structure in the motions and detecting style-specific characteristics. We propose an algorithm that learns a hierarchical model of styles from unlabeled motion capture data by exploiting the cyclic property of human locomotion. We assume that sequences with the same style contain locomotion cycles generated by noisy, temporally warped versions of a single latent cycle. We model these style-specific latent cycles as random variables drawn from a common "parent" cycle distribution, representing the structure shared by all motions. Given these hierarchical priors, the algorithm learns, in a completely unsupervised fashion, temporally aligned latent cycle distributions, each modeling a specific locomotion style, and computes for each example the style label posterior distribution, the segmentation into cycles, and the temporal warping with respect to the latent cycles. We demonstrate the flexibility of the model on several application problems such as style clustering, animation, style blending, and filling in of missing data.</p></div></span> <a id="expcoll99" href="JavaScript: expandcollapse('expcoll99',99)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553476">Binary action search for learning continuous-action control policies</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435608646">Jason Pazis</a>,
<a href="author_page.cfm?id=81100317359">Michail G. Lagoudakis</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 793-800</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553476" title="DOI">10.1145/1553374.1553476</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553476&ftid=640212&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow100" style="display:inline;"><br /><div style="display:inline">Reinforcement Learning methods for controlling stochastic processes typically assume a small and discrete action space. While continuous action spaces are quite common in real-world problems, the most common approach still employed in practice is coarse ...</div></span>
<span id="toHide100" style="display:none;"><br /><div style="display:inline"><p>Reinforcement Learning methods for controlling stochastic processes typically assume a small and discrete action space. While continuous action spaces are quite common in real-world problems, the most common approach still employed in practice is coarse discretization of the action space. This paper presents a novel method, called Binary Action Search, for realizing continuousaction policies by searching efficiently the entire action range through increment and decrement modifications to the values of the action variables according to an internal binary policy defined over an augmented state space. The proposed approach essentially approximates any continuous action space to arbitrary resolution and can be combined with any discrete-action reinforcement learning algorithm for learning continuous-action policies. Binary Action Search eliminates the restrictive modification steps of Adaptive Action Modification and requires no temporal action locality in the domain. Our approach is coupled with two well-known reinforcement learning algorithms (Least-Squares Policy Iteration and Fitted <i>Q</i>-Iteration) and its use and properties are thoroughly investigated and demonstrated on the continuous state-action Inverted Pendulum, Double Integrator, and Car on the Hill domains.</p></div></span> <a id="expcoll100" href="JavaScript: expandcollapse('expcoll100',100)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553477">Detecting the direction of causal time series</a></span></td>
</tr>
<tr>
 <td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81442594400">Jonas Peters</a>,
<a href="author_page.cfm?id=81100317961">Dominik Janzing</a>,
<a href="author_page.cfm?id=81333488991">Arthur Gretton</a>,
<a href="author_page.cfm?id=81100216460">Bernhard Sch&#246;lkopf</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 801-808</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553477" title="DOI">10.1145/1553374.1553477</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553477&ftid=640213&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow101" style="display:inline;"><br /><div style="display:inline">We propose a method that detects the true direction of time series, by fitting an autoregressive moving average model to the data. Whenever the noise is independent of the previous samples for one ordering of the observations, but dependent for the opposite ...</div></span>
<span id="toHide101" style="display:none;"><br /><div style="display:inline"><p>We propose a method that detects the true direction of time series, by fitting an autoregressive moving average model to the data. Whenever the noise is independent of the previous samples for one ordering of the observations, but dependent for the opposite ordering, we infer the former direction to be the true one. We prove that our method works in the population case as long as the noise of the process is not normally distributed (for the latter case, the direction is not identifiable). A new and important implication of our result is that it confirms a fundamental conjecture in causal reasoning --- if after regression the noise is independent of signal for one direction and dependent for the other, then the former represents the true causal direction --- in the case of time series. We test our approach on two types of data: simulated data sets conforming to our modeling assumptions, and real world EEG time series. Our method makes a decision for a significant fraction of both data sets, and these decisions are mostly correct. For real world data, our approach outperforms alternative solutions to the problem of time direction recovery.</p></div></span> <a id="expcoll101" href="JavaScript: expandcollapse('expcoll101',101)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553478">Constraint relaxation in approximate linear programs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331501138">Marek Petrik</a>,
<a href="author_page.cfm?id=81100061211">Shlomo Zilberstein</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 809-816</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553478" title="DOI">10.1145/1553374.1553478</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553478&ftid=640214&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow102" style="display:inline;"><br /><div style="display:inline">Approximate Linear Programming (ALP) is a reinforcement learning technique with nice theoretical properties, but it often performs poorly in practice. We identify some reasons for the poor quality of ALP solutions in problems where the approximation ...</div></span>
<span id="toHide102" style="display:none;"><br /><div style="display:inline"><p>Approximate Linear Programming (ALP) is a reinforcement learning technique with nice theoretical properties, but it often performs poorly in practice. We identify some reasons for the poor quality of ALP solutions in problems where the approximation induces virtual loops. We then introduce two methods for improving solution quality. One method rolls out selected constraints of the ALP, guided by the dual information. The second method is a relaxation of the ALP, based on external penalty methods. The latter method is applicable in domains in which rolling out constraints is impractical. Both approaches show promising empirical results for simple benchmark problems as well as for a realistic blood inventory management problem.</p></div></span> <a id="expcoll102" href="JavaScript: expandcollapse('expcoll102',102)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553479">Multi-class image segmentation using conditional random fields and global classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435608713">Nils Plath</a>,
<a href="author_page.cfm?id=81331505904">Marc Toussaint</a>,
<a href="author_page.cfm?id=81100441058">Shinichi Nakajima</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 817-824</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553479" title="DOI">10.1145/1553374.1553479</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553479&ftid=640215&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow103" style="display:inline;"><br /><div style="display:inline">A key aspect of semantic image segmentation is to integrate local and global features for the prediction of local segment labels. We present an approach to multi-class segmentation which combines two methods for this integration: a Conditional Random ...</div></span>
<span id="toHide103" style="display:none;"><br /><div style="display:inline"><p>A key aspect of semantic image segmentation is to integrate local and global features for the prediction of local segment labels. We present an approach to multi-class segmentation which combines two methods for this integration: a Conditional Random Field (CRF) which couples to local image features and an image classification method which considers global features. The CRF follows the approach of Reynolds & Murphy (2007) and is based on an unsupervised multi scale pre-segmentation of the image into patches, where patch labels correspond to the random variables of the CRF. The output of the classifier is used to constraint this CRF. We demonstrate and compare the approach on a standard semantic segmentation data set.</p></div></span> <a id="expcoll103" href="JavaScript: expandcollapse('expcoll103',103)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553480">Learning when to stop thinking and do something!</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309498613">Barnab&#225;s P&#243;czos</a>,
<a href="author_page.cfm?id=81435600612">Yasin Abbasi-Yadkori</a>,
<a href="author_page.cfm?id=81100538163">Csaba Szepesv&#225;ri</a>,
<a href="author_page.cfm?id=81100082147">Russell Greiner</a>,
<a href="author_page.cfm?id=81100397075">Nathan Sturtevant</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 825-832</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553480" title="DOI">10.1145/1553374.1553480</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553480&ftid=640216&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow104" style="display:inline;"><br /><div style="display:inline">An anytime algorithm is capable of returning a response to the given task at essentially any time; typically the quality of the response improves as the time increases. Here, we consider the challenge of learning when we should terminate such algorithms ...</div></span>
<span id="toHide104" style="display:none;"><br /><div style="display:inline"><p>An anytime algorithm is capable of returning a response to the given task at essentially any time; typically the quality of the response improves as the time increases. Here, we consider the challenge of learning when we should terminate such algorithms on each of a sequence of iid tasks, to optimize the expected average reward per unit time. We provide a system for addressing this challenge, which combines the global optimizer Cross-Entropy method with local gradient ascent. This paper theoretically investigates how far the estimated gradient is from the true gradient, then empirically demonstrates that this system is effective by applying it to a toy problem, as well as on a real-world face detection task.</p></div></span> <a id="expcoll104" href="JavaScript: expandcollapse('expcoll104',104)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553481">Independent factor topic models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435603851">Duangmanee (Pew) Putthividhya</a>,
<a href="author_page.cfm?id=81100274607">Hagai T. Attias</a>,
<a href="author_page.cfm?id=81318495289">Srikantan Nagarajan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 833-840</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553481" title="DOI">10.1145/1553374.1553481</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553481&ftid=640217&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow105" style="display:inline;"><br /><div style="display:inline">Topic models such as Latent Dirichlet Allocation (LDA) and Correlated Topic Model (CTM) have recently emerged as powerful statistical tools for text document modeling. In this paper, we improve upon CTM and propose Independent Factor Topic Models (IFTM) ...</div></span>
<span id="toHide105" style="display:none;"><br /><div style="display:inline"><p>Topic models such as Latent Dirichlet Allocation (LDA) and Correlated Topic Model (CTM) have recently emerged as powerful statistical tools for text document modeling. In this paper, we improve upon CTM and propose Independent Factor Topic Models (IFTM) which use linear latent variable models to uncover the hidden sources of correlation between topics. There are 2 main contributions of this work. First, by using a sparse source prior model, we can directly visualize sparse patterns of topic correlations. Secondly, the conditional independence assumption implied in the use of latent source variables allows the objective function to factorize, leading to a fast Newton-Raphson based variational inference algorithm. Experimental results on synthetic and real data show that IFTM runs on average 3--5 times faster than CTM, while giving competitive performance as measured by perplexity and loglikelihood of held-out data.</p></div></span> <a id="expcoll105" href="JavaScript: expandcollapse('expcoll105',105)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553482">An efficient sparse metric learning in high-dimensional space via <i>l</i><sub>1</sub>-penalized log-determinant regularization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317499245">Guo-Jun Qi</a>,
<a href="author_page.cfm?id=81320495810">Jinhui Tang</a>,
<a href="author_page.cfm?id=81337494695">Zheng-Jun Zha</a>,
<a href="author_page.cfm?id=81100547902">Tat-Seng Chua</a>,
<a href="author_page.cfm?id=81451601211">Hong-Jiang Zhang</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 841-848</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553482" title="DOI">10.1145/1553374.1553482</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553482&ftid=640218&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow106" style="display:inline;"><br /><div style="display:inline">This paper proposes an efficient sparse metric learning algorithm in high dimensional space via an l1-penalized log-determinant regularization. Compare to the most existing distance metric learning algorithms, the proposed algorithm ...</div></span>
<span id="toHide106" style="display:none;"><br /><div style="display:inline"><p>This paper proposes an efficient sparse metric learning algorithm in high dimensional space via an <i>l</i><sub>1</sub>-penalized log-determinant regularization. Compare to the most existing distance metric learning algorithms, the proposed algorithm exploits the sparsity nature underlying the intrinsic high dimensional feature space. This sparsity prior of learning distance metric serves to regularize the complexity of the distance model especially in the "less example number <i>p</i> and high dimension <i>d</i>" setting. Theoretically, by analogy to the covariance estimation problem, we find the proposed distance learning algorithm has a consistent result at rate <i>O</i> (&radic;<i>m</i><sup>2</sup> log <i>d</i>)/<i>n</i>) to the target distance matrix with at most <i>m</i> nonzeros per row. Moreover, from the implementation perspective, this <i>l</i><sub>1</sub>-penalized log-determinant formulation can be efficiently optimized in a block coordinate descent fashion which is much faster than the standard semi-definite programming which has been widely adopted in many other advanced distance learning algorithms. We compare this algorithm with other state-of-the-art ones on various datasets and competitive results are obtained.</p></div></span> <a id="expcoll106" href="JavaScript: expandcollapse('expcoll106',106)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553483">Sparse higher order conditional random fields for improved sequence labeling</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435604442">Xian Qian</a>,
<a href="author_page.cfm?id=81540102356">Xiaoqian Jiang</a>,
<a href="author_page.cfm?id=81322511304">Qi Zhang</a>,
<a href="author_page.cfm?id=81414610034">Xuanjing Huang</a>,
<a href="author_page.cfm?id=81409595879">Lide Wu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 849-856</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553483" title="DOI">10.1145/1553374.1553483</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553483&ftid=640219&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow107" style="display:inline;"><br /><div style="display:inline">In real sequence labeling tasks, statistics of many higher order features are not sufficient due to the training data sparseness, very few of them are useful. We describe Sparse Higher Order Conditional Random Fields (SHO-CRFs), which are able to handle ...</div></span>
<span id="toHide107" style="display:none;"><br /><div style="display:inline"><p>In real sequence labeling tasks, statistics of many higher order features are not sufficient due to the training data sparseness, very few of them are useful. We describe Sparse Higher Order Conditional Random Fields (SHO-CRFs), which are able to handle local features and sparse higher order features together using a novel tractable exact inference algorithm. Our main insight is that states and transitions with same potential functions can be grouped together, and inference is performed on the grouped states and transitions. Though the complexity is not polynomial, SHO-CRFs are still efficient in practice because of the feature sparseness. Experimental results on optical character recognition and Chinese organization name recognition show that with the same higher order feature set, SHO-CRFs significantly outperform previous approaches.</p></div></span> <a id="expcoll107" href="JavaScript: expandcollapse('expcoll107',107)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553484">An efficient projection for <i>l</i><sub>1</sub>, <sub>&infin;</sub> regularization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81317499304">Ariadna Quattoni</a>,
<a href="author_page.cfm?id=81100282019">Xavier Carreras</a>,
<a href="author_page.cfm?id=81406599089">Michael Collins</a>,
<a href="author_page.cfm?id=81100537374">Trevor Darrell</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 857-864</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553484" title="DOI">10.1145/1553374.1553484</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553484&ftid=640220&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow108" style="display:inline;"><br /><div style="display:inline">In recent years the l1, &infin; norm has been proposed for joint regularization. In essence, this type of regularization aims at extending the l1 framework for learning sparse models to a setting where the ...</div></span>
<span id="toHide108" style="display:none;"><br /><div style="display:inline"><p>In recent years the <i>l</i><sub>1</sub>, <sub>&infin;</sub> norm has been proposed for joint regularization. In essence, this type of regularization aims at extending the <i>l</i><sub>1</sub> framework for learning sparse models to a setting where the goal is to learn a set of jointly sparse models. In this paper we derive a simple and effective projected gradient method for optimization of <i>l</i><sub>1</sub>, <sub>&infin;</sub> regularized problems. The main challenge in developing such a method resides on being able to compute efficient projections to the <i>l</i><sub>1</sub>, <sub>&infin;</sub> ball. We present an algorithm that works in <i>O</i>(<i>n</i> log <i>n</i>) time and <i>O</i>(<i>n</i>) memory where <i>n</i> is the number of parameters. We test our algorithm in a multi-task image annotation problem. Our results show that <i>l</i><sub>1</sub>, <sub>&infin;</sub> leads to better performance than both <i>l</i><sub>2</sub> and <i>l</i><sub>1</sub> regularization and that it is is effective in discovering jointly sparse solutions.</p></div></span> <a id="expcoll108" href="JavaScript: expandcollapse('expcoll108',108)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553485">Nearest neighbors in high-dimensional data: the emergence and influence of hubs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384602099">Milo&#353; Radovanovi&#263;</a>,
<a href="author_page.cfm?id=81100571305">Alexandros Nanopoulos</a>,
<a href="author_page.cfm?id=81337490182">Mirjana Ivanovi&#263;</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 865-872</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553485" title="DOI">10.1145/1553374.1553485</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553485&ftid=640569&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow109" style="display:inline;"><br /><div style="display:inline">High dimensionality can pose severe difficulties, widely recognized as different aspects of the curse of dimensionality. In this paper we study a new aspect of the curse pertaining to the distribution of k-occurrences, i.e., the number of times ...</div></span>
<span id="toHide109" style="display:none;"><br /><div style="display:inline"><p>High dimensionality can pose severe difficulties, widely recognized as different aspects of the curse of dimensionality. In this paper we study a new aspect of the curse pertaining to the distribution of <i>k</i>-occurrences, i.e., the number of times a point appears among the <i>k</i> nearest neighbors of other points in a data set. We show that, as dimensionality increases, this distribution becomes considerably skewed and hub points emerge (points with very high <i>k</i>-occurrences). We examine the origin of this phenomenon, showing that it is an inherent property of high-dimensional vector space, and explore its influence on applications based on measuring distances in vector spaces, notably classification, clustering, and information retrieval.</p></div></span> <a id="expcoll109" href="JavaScript: expandcollapse('expcoll109',109)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553486">Large-scale deep unsupervised learning using graphics processors</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315491047">Rajat Raina</a>,
<a href="author_page.cfm?id=81435596724">Anand Madhavan</a>,
<a href="author_page.cfm?id=81100471019">Andrew Y. Ng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 873-880</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553486" title="DOI">10.1145/1553374.1553486</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553486&ftid=640570&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow110" style="display:inline;"><br /><div style="display:inline">The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief ...</div></span>
<span id="toHide110" style="display:none;"><br /><div style="display:inline"><p>The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples.</p> <p>In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.</p></div></span> <a id="expcoll110" href="JavaScript: expandcollapse('expcoll110',110)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553487">The Bayesian group-Lasso for analyzing contingency tables</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435601212">Sudhir Raman</a>,
<a href="author_page.cfm?id=81384606816">Thomas J. Fuchs</a>,
<a href="author_page.cfm?id=81543370956">Peter J. Wild</a>,
<a href="author_page.cfm?id=81435607881">Edgar Dahl</a>,
<a href="author_page.cfm?id=81481652115">Volker Roth</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 881-888</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553487" title="DOI">10.1145/1553374.1553487</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553487&ftid=640571&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow111" style="display:inline;"><br /><div style="display:inline">Group-Lasso estimators, useful in many applications, suffer from lack of meaningful variance estimates for regression coefficients. To overcome such problems, we propose a full Bayesian treatment of the Group-Lasso, extending the standard Bayesian Lasso, ...</div></span>
<span id="toHide111" style="display:none;"><br /><div style="display:inline"><p>Group-Lasso estimators, useful in many applications, suffer from lack of meaningful variance estimates for regression coefficients. To overcome such problems, we propose a full Bayesian treatment of the Group-Lasso, extending the standard Bayesian Lasso, using hierarchical expansion. The method is then applied to Poisson models for contingency tables using a highly efficient MCMC algorithm. The simulated experiments validate the performance of this method on artificial datasets with known ground-truth. When applied to a breast cancer dataset, the method demonstrates the capability of identifying the differences in interactions patterns of marker proteins between different patient groups.</p></div></span> <a id="expcoll111" href="JavaScript: expandcollapse('expcoll111',111)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553488">Supervised learning from multiple experts: whom to trust when everyone lies a bit</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100214434">Vikas C. Raykar</a>,
<a href="author_page.cfm?id=81100473562">Shipeng Yu</a>,
<a href="author_page.cfm?id=81435605734">Linda H. Zhao</a>,
<a href="author_page.cfm?id=81435611269">Anna Jerebko</a>,
<a href="author_page.cfm?id=81435610942">Charles Florin</a>,
<a href="author_page.cfm?id=81435607479">Gerardo Hermosillo Valadez</a>,
<a href="author_page.cfm?id=81100464930">Luca Bogoni</a>,
<a href="author_page.cfm?id=81435609440">Linda Moy</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 889-896</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553488" title="DOI">10.1145/1553374.1553488</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553488&ftid=640572&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow112" style="display:inline;"><br /><div style="display:inline">We describe a probabilistic approach for supervised learning when we have multiple experts/annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of ...</div></span>
<span id="toHide112" style="display:none;"><br /><div style="display:inline"><p>We describe a probabilistic approach for supervised learning when we have multiple experts/annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of the actual hidden labels. Experimental results indicate that the proposed method is superior to the commonly used majority voting baseline.</p></div></span> <a id="expcoll112" href="JavaScript: expandcollapse('expcoll112',112)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553489">Surrogate regret bounds for proper losses</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100282674">Mark D. Reid</a>,
<a href="author_page.cfm?id=81100170112">Robert C. Williamson</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 897-904</span></td>
</tr>
<tr> 
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553489" title="DOI">10.1145/1553374.1553489</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553489&ftid=640573&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow113" style="display:inline;"><br /><div style="display:inline">We present tight surrogate regret bounds for the class of proper (i.e., Fisher consistent) losses. The bounds generalise the margin-based bounds due to Bartlett et al. (2006). The proof uses Taylor's theorem and leads to new representations for ...</div></span>
<span id="toHide113" style="display:none;"><br /><div style="display:inline"><p>We present tight surrogate regret bounds for the class of proper (<i>i.e.</i>, Fisher consistent) losses. The bounds generalise the margin-based bounds due to Bartlett et al. (2006). The proof uses Taylor's theorem and leads to new representations for loss and regret and a simple proof of the integral representation of proper losses. We also present a different formulation of a duality result of Bregman divergences which leads to a simple demonstration of the convexity of composite losses using canonical link functions.</p></div></span> <a id="expcoll113" href="JavaScript: expandcollapse('expcoll113',113)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553490">Learning structurally consistent undirected probabilistic graphical models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435592639">Sushmita Roy</a>,
<a href="author_page.cfm?id=81100001124">Terran Lane</a>,
<a href="author_page.cfm?id=81100262455">Margaret Werner-Washburne</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 905-912</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553490" title="DOI">10.1145/1553374.1553490</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553490&ftid=640574&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow114" style="display:inline;"><br /><div style="display:inline">In many real-world domains, undirected graphical models such as Markov random fields provide a more natural representation of the statistical dependency structure than directed graphical models. Unfortunately, structure learning of undirected graphs ...</div></span>
<span id="toHide114" style="display:none;"><br /><div style="display:inline"><p>In many real-world domains, undirected graphical models such as Markov random fields provide a more natural representation of the statistical dependency structure than directed graphical models. Unfortunately, structure learning of undirected graphs using likelihood-based scores remains difficult because of the intractability of computing the partition function. We describe a new Markov random field structure learning algorithm, motivated by canonical parameterization of Abbeel <i>et al.</i> We provide computational improvements on their parameterization by learning per-variable canonical factors, which makes our algorithm suitable for domains with hundreds of nodes. We compare our algorithm against several algorithms for learning undirected and directed models on simulated and real datasets from biology. Our algorithm frequently outperforms existing algorithms, producing higher-quality structures, suggesting that enforcing consistency during structure learning is beneficial for learning undirected graphs.</p></div></span> <a id="expcoll114" href="JavaScript: expandcollapse('expcoll114',114)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553491">Ranking interesting subgroups</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81361607489">Stefan Rueping</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 913-920</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553491" title="DOI">10.1145/1553374.1553491</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553491&ftid=640575&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow115" style="display:inline;"><br /><div style="display:inline">Subgroup discovery is the task of identifying the top k patterns in a database with most significant deviation in the distribution of a target attribute Y. Subgroup discovery is a popular approach for identifying interesting patterns in ...</div></span>
<span id="toHide115" style="display:none;"><br /><div style="display:inline"><p>Subgroup discovery is the task of identifying the top <i>k</i> patterns in a database with most significant deviation in the distribution of a target attribute <i>Y</i>. Subgroup discovery is a popular approach for identifying interesting patterns in data, because it combines statistical significance with an understandable representation of patterns as a logical formula. However, it is often a problem that some subgroups, even if they are statistically highly significant, are not interesting to the user. We present an approach based on the work on ranking Support Vector Machines that ranks subgroups with respect to the user's concept of interestingness, and finds more interesting subgroups. This approach can significantly increase the quality of the subgroups.</p></div></span> <a id="expcoll115" href="JavaScript: expandcollapse('expcoll115',115)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553492">Function factorization using warped Gaussian processes</a></span></td>
</tr>
<tr>
 <td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81361597693">Mikkel N. Schmidt</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 921-928</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553492" title="DOI">10.1145/1553374.1553492</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553492&ftid=640576&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow116" style="display:inline;"><br /><div style="display:inline">We introduce a new approach to non-linear regression called function factorization, that is suitable for problems where an output variable can reasonably be modeled by a number of multiplicative interaction terms between non-linear functions of the inputs. ...</div></span>
<span id="toHide116" style="display:none;"><br /><div style="display:inline"><p>We introduce a new approach to non-linear regression called function factorization, that is suitable for problems where an output variable can reasonably be modeled by a number of multiplicative interaction terms between non-linear functions of the inputs. The idea is to approximate a complicated function on a high-dimensional space by the sum of products of simpler functions on lower-dimensional subspaces. Function factorization can be seen as a generalization of matrix and tensor factorization methods, in which the data are approximated by the sum of outer products of vectors. We present a non-parametric Bayesian approach to function factorization where the priors over the factorizing functions are warped Gaussian processes, and we do inference using Hamiltonian Markov chain Monte Carlo. We demonstrate the superior predictive performance of the method on a food science data set compared to Gaussian process regression and tensor factorization using PARAFAC and GEMANOVA models.</p></div></span> <a id="expcoll116" href="JavaScript: expandcollapse('expcoll116',116)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553493">Stochastic methods for <i>l</i><sub>1</sub> regularized loss minimization</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100489319">Shai Shalev-Shwartz</a>,
<a href="author_page.cfm?id=81330499128">Ambuj Tewari</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 929-936</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553493" title="DOI">10.1145/1553374.1553493</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553493&ftid=640577&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow117" style="display:inline;"><br /><div style="display:inline">We describe and analyze two stochastic methods for l1 regularized loss minimization problems, such as the Lasso. The first method updates the weight of a single feature at each iteration while the second method updates the entire weight ...</div></span>
<span id="toHide117" style="display:none;"><br /><div style="display:inline"><p>We describe and analyze two stochastic methods for <i>l</i><sub>1</sub> regularized loss minimization problems, such as the Lasso. The first method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature/example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.</p></div></span> <a id="expcoll117" href="JavaScript: expandcollapse('expcoll117',117)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553494">Structure preserving embedding</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435604401">Blake Shaw</a>,
<a href="author_page.cfm?id=81100510477">Tony Jebara</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 937-944</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553494" title="DOI">10.1145/1553374.1553494</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553494&ftid=640578&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow118" style="display:inline;"><br /><div style="display:inline">Structure Preserving Embedding (SPE) is an algorithm for embedding graphs in Euclidean space such that the embedding is low-dimensional and preserves the global topological properties of the input graph. Topology is preserved if a connectivity algorithm, ...</div></span>
<span id="toHide118" style="display:none;"><br /><div style="display:inline"><p>Structure Preserving Embedding (SPE) is an algorithm for embedding graphs in Euclidean space such that the embedding is low-dimensional and preserves the global topological properties of the input graph. Topology is preserved if a connectivity algorithm, such as <i>k</i>-nearest neighbors, can easily recover the edges of the input graph from only the coordinates of the nodes after embedding. SPE is formulated as a semidefinite program that learns a low-rank kernel matrix constrained by a set of linear inequalities which captures the connectivity structure of the input graph. Traditional graph embedding algorithms do not preserve structure according to our definition, and thus the resulting visualizations can be misleading or less informative. SPE provides significant improvements in terms of visualization and lossless compression of graphs, outperforming popular methods such as spectral embedding and Laplacian eigen-maps. We find that many classical graphs and networks can be properly embedded using only a few dimensions. Furthermore, introducing structure preserving constraints into dimensionality reduction algorithms produces more accurate representations of high-dimensional data.</p></div></span> <a id="expcoll118" href="JavaScript: expandcollapse('expcoll118',118)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553495">Monte-Carlo simulation balancing</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100336678">David Silver</a>,
<a href="author_page.cfm?id=81100145207">Gerald Tesauro</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 945-952</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553495" title="DOI">10.1145/1553374.1553495</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553495&ftid=640579&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow119" style="display:inline;"><br /><div style="display:inline">In this paper we introduce the first algorithms for efficiently learning a simulation policy for Monte-Carlo search. Our main idea is to optimise the balance of a simulation policy, so that an accurate spread of simulation outcomes is maintained, ...</div></span>
<span id="toHide119" style="display:none;"><br /><div style="display:inline"><p>In this paper we introduce the first algorithms for efficiently learning a simulation policy for Monte-Carlo search. Our main idea is to optimise the <i>balance</i> of a simulation policy, so that an accurate spread of simulation outcomes is maintained, rather than optimising the direct <i>strength</i> of the simulation policy. We develop two algorithms for balancing a simulation policy by gradient descent. The first algorithm optimises the balance of complete simulations, using a policy gradient algorithm; whereas the second algorithm optimises the balance over every two steps of simulation. We compare our algorithms to reinforcement learning and supervised learning algorithms for maximising the strength of the simulation policy. We test each algorithm in the domain of 5 x 5 and 6 x 6 Computer Go, using a softmax policy that is parameterised by weights for a hundred simple patterns. When used in a simple Monte-Carlo search, the policies learnt by simulation balancing achieved significantly better performance, with half the mean squared error of a uniform random policy, and similar overall performance to a sophisticated Go engine.</p></div></span> <a id="expcoll119" href="JavaScript: expandcollapse('expcoll119',119)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553496">Uncertainty sampling and transductive experimental design for active dual supervision</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309499649">Vikas Sindhwani</a>,
<a href="author_page.cfm?id=81100590712">Prem Melville</a>,
<a href="author_page.cfm?id=81100574279">Richard D. Lawrence</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 953-960</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553496" title="DOI">10.1145/1553374.1553496</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553496&ftid=640580&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow120" style="display:inline;"><br /><div style="display:inline">Dual supervision refers to the general setting of learning from both labeled examples as well as labeled features. Labeled features are naturally available in tasks such as text classification where it is frequently possible to provide domain knowledge ...</div></span>
<span id="toHide120" style="display:none;"><br /><div style="display:inline"><p>Dual supervision refers to the general setting of learning from both labeled examples as well as labeled features. Labeled features are naturally available in tasks such as text classification where it is frequently possible to provide domain knowledge in the form of words that associate strongly with a class. In this paper, we consider the novel problem of active dual supervision, or, how to optimally query an example and feature labeling oracle to simultaneously collect two different forms of supervision, with the objective of building the best classifier in the most cost effective manner. We apply classical uncertainty and experimental design based active learning schemes to graph/kernel-based dual supervision models. Empirical studies confirm the potential of these schemes to significantly reduce the cost of acquiring labeled data for training high-quality models.</p></div></span> <a id="expcoll120" href="JavaScript: expandcollapse('expcoll120',120)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553497">Hilbert space embeddings of conditional distributions with applications to dynamical systems</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100578710">Le Song</a>,
<a href="author_page.cfm?id=81440606945">Jonathan Huang</a>,
<a href="author_page.cfm?id=81100243402">Alex Smola</a>,
<a href="author_page.cfm?id=81100373172">Kenji Fukumizu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 961-968</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553497" title="DOI">10.1145/1553374.1553497</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553497&ftid=640581&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow121" style="display:inline;"><br /><div style="display:inline">In this paper, we extend the Hilbert space embedding approach to handle conditional distributions. We derive a kernel estimate for the conditional embedding, and show its connection to ordinary embeddings. Conditional embeddings largely extend ...</div></span>
<span id="toHide121" style="display:none;"><br /><div style="display:inline"><p>In this paper, we extend the Hilbert space embedding approach to handle <i>conditional</i> distributions. We derive a kernel estimate for the conditional embedding, and show its connection to ordinary embeddings. Conditional embeddings largely extend our ability to manipulate distributions in Hilbert spaces, and as an example, we derive a nonparametric method for modeling dynamical systems where the belief state of the system is maintained as a conditional embedding. Our method is very general in terms of both the domains and the types of distributions that it can handle, and we demonstrate the effectiveness of our method in various dynamical systems. We expect that conditional embeddings will have wider applications beyond modeling dynamical systems.</p></div></span> <a id="expcoll121" href="JavaScript: expandcollapse('expcoll121',121)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553498">Multi-assignment clustering for Boolean data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435594279">Andreas P. Streich</a>,
<a href="author_page.cfm?id=81435597522">Mario Frank</a>,
<a href="author_page.cfm?id=81100431853">David Basin</a>,
<a href="author_page.cfm?id=81331489168">Joachim M. Buhmann</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 969-976</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553498" title="DOI">10.1145/1553374.1553498</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553498&ftid=640582&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow122" style="display:inline;"><br /><div style="display:inline">Conventional clustering methods typically assume that each data item belongs to a single cluster. This assumption does not hold in general. In order to overcome this limitation, we propose a generative method for clustering vectorial data, where each ...</div></span>
<span id="toHide122" style="display:none;"><br /><div style="display:inline"><p>Conventional clustering methods typically assume that each data item belongs to a single cluster. This assumption does not hold in general. In order to overcome this limitation, we propose a generative method for clustering vectorial data, where each object can be assigned to multiple clusters. Using a deterministic annealing scheme, our method decomposes the observed data into the contributions of individual clusters and infers their parameters.</p> <p>Experiments on synthetic Boolean data show that our method achieves higher accuracy in the source parameter estimation and superior cluster stability compared to state-of-the-art approaches. We also apply our method to an important problem in computer security known as role mining. Experiments on real-world access control data show performance gains in generalization to new employees against other multi-assignment methods. In challenging situations with high noise levels, our approach maintains its good performance, while alternative state-of-the-art techniques lack robustness.</p></div></span> <a id="expcoll122" href="JavaScript: expandcollapse('expcoll122',122)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553499">A least squares formulation for a class of generalized eigenvalue problems in machine learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81365595166">Liang Sun</a>,
<a href="author_page.cfm?id=81333489125">Shuiwang Ji</a>,
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 977-984</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553499" title="DOI">10.1145/1553374.1553499</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553499&ftid=640583&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow123" style="display:inline;"><br /><div style="display:inline">Many machine learning algorithms can be formulated as a generalized eigenvalue problem. One major limitation of such formulation is that the generalized eigenvalue problem is computationally expensive to solve especially for large-scale problems. In ...</div></span>
<span id="toHide123" style="display:none;"><br /><div style="display:inline"><p>Many machine learning algorithms can be formulated as a generalized eigenvalue problem. One major limitation of such formulation is that the generalized eigenvalue problem is computationally expensive to solve especially for large-scale problems. In this paper, we show that under a mild condition, a class of generalized eigenvalue problems in machine learning can be formulated as a least squares problem. This class of problems include classical techniques such as Canonical Correlation Analysis (CCA), Partial Least Squares (PLS), and Linear Discriminant Analysis (LDA), as well as Hypergraph Spectral Learning (HSL). As a result, various regularization techniques can be readily incorporated into the formulation to improve model sparsity and generalization ability. In addition, the least squares formulation leads to efficient and scalable implementations based on the iterative conjugate gradient type algorithms. We report experimental results that confirm the established equivalence relationship. Results also demonstrate the efficiency and effectiveness of the equivalent least squares formulations on large-scale problems.</p></div></span> <a id="expcoll123" href="JavaScript: expandcollapse('expcoll123',123)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553500">A simpler unified analysis of budget perceptrons</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81388600728">Ilya Sutskever</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 985-992</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553500" title="DOI">10.1145/1553374.1553500</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553500&ftid=640584&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow124" style="display:inline;"><br /><div style="display:inline">The kernel Perceptron is an appealing online learning algorithm that has a drawback: whenever it makes an error it must increase its support set, which slows training and testing if the number of errors is large. The Forgetron and the Randomized Budget ...</div></span>
<span id="toHide124" style="display:none;"><br /><div style="display:inline"><p>The kernel Perceptron is an appealing online learning algorithm that has a drawback: whenever it makes an error it must increase its support set, which slows training and testing if the number of errors is large. The Forgetron and the Randomized Budget Perceptron algorithms overcome this problem by restricting the number of support vectors the Perceptron is allowed to have. These algorithms have regret bounds whose proofs are dissimilar. In this paper we propose a unified analysis of both of these algorithms by observing that the way in which they remove support vectors can be seen as types of <i>L</i><sub>2</sub>-regularization. By casting these algorithms as instances of online convex optimization problems and applying a variant of Zinkevich's theorem for noisy and incorrect gradient, we can bound the regret of these algorithms more easily than before. Our bounds are similar to the existing ones, but the proofs are less technical.</p></div></span> <a id="expcoll124" href="JavaScript: expandcollapse('expcoll124',124)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553501">Fast gradient-descent methods for temporal-difference learning with linear function approximation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81342513055">Richard S. Sutton</a>,
<a href="author_page.cfm?id=81435611594">Hamid Reza Maei</a>,
<a href="author_page.cfm?id=81100275806">Doina Precup</a>,
<a href="author_page.cfm?id=81100498901">Shalabh Bhatnagar</a>,
<a href="author_page.cfm?id=81100336678">David Silver</a>,
<a href="author_page.cfm?id=81100538163">Csaba Szepesv&#225;ri</a>,
<a href="author_page.cfm?id=81309512292">Eric Wiewiora</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 993-1000</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553501" title="DOI">10.1145/1553374.1553501</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553501&ftid=640585&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow125" style="display:inline;"><br /><div style="display:inline">Sutton, Szepesv&aacute;ri and Maei (2009) recently introduced the first temporal-difference learning algorithm compatible with both linear function approximation and off-policy training, and whose complexity scales only linearly in the size of the function ...</div></span>
<span id="toHide125" style="display:none;"><br /><div style="display:inline"><p>Sutton, Szepesv&aacute;ri and Maei (2009) recently introduced the first temporal-difference learning algorithm compatible with both linear function approximation and off-policy training, and whose complexity scales only linearly in the size of the function approximator. Although their <i>gradient temporal difference</i> (GTD) algorithm converges reliably, it can be very slow compared to conventional linear TD (on on-policy problems where TD is convergent), calling into question its practical utility. In this paper we introduce two new related algorithms with better convergence rates. The first algorithm, <i>GTD2</i>, is derived and proved convergent just as GTD was, but uses a different objective function and converges significantly faster (but still not as fast as conventional TD). The second new algorithm, <i>linear TD with gradient correction</i>, or <i>TDC</i>, uses the same update rule as conventional TD except for an additional term which is initially zero. In our experiments on small test problems and in a Computer Go application with a million features, the learning rate of this algorithm was comparable to that of conventional TD. This algorithm appears to extend linear TD to off-policy learning with no penalty in performance while only doubling computational requirements.</p></div></span> <a id="expcoll125" href="JavaScript: expandcollapse('expcoll125',125)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553502">Optimistic initialization and greediness lead to polynomial time learning in factored MDPs</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100192810">Istv&#225;n Szita</a>,
<a href="author_page.cfm?id=81100130503">Andr&#225;s L&#337;rincz</a>
</span>
</td>
</tr>
 <tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1001-1008</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553502" title="DOI">10.1145/1553374.1553502</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553502&ftid=640586&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow126" style="display:inline;"><br /><div style="display:inline">In this paper we propose an algorithm for polynomial-time reinforcement learning in factored Markov decision processes (FMDPs). The factored optimistic initial model (FOIM) algorithm, maintains an empirical model of the FMDP in a conventional way, and ...</div></span>
<span id="toHide126" style="display:none;"><br /><div style="display:inline"><p>In this paper we propose an algorithm for polynomial-time reinforcement learning in factored Markov decision processes (FMDPs). The factored optimistic initial model (FOIM) algorithm, maintains an empirical model of the FMDP in a conventional way, and always follows a greedy policy with respect to its model. The only trick of the algorithm is that the model is initialized optimistically. We prove that with suitable initialization (i) FOIM converges to the fixed point of approximate value iteration (AVI); (ii) the number of steps when the agent makes non-near-optimal decisions (with respect to the solution of AVI) is polynomial in all relevant quantities; (iii) the per-step costs of the algorithm are also polynomial. To our best knowledge, FOIM is the first algorithm with these properties.</p></div></span> <a id="expcoll126" href="JavaScript: expandcollapse('expcoll126',126)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553503">Discriminative <i>k</i>-metrics</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81375597376">Arthur Szlam</a>,
<a href="author_page.cfm?id=81100582663">Guillermo Sapiro</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1009-1016</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553503" title="DOI">10.1145/1553374.1553503</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553503&ftid=640587&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow127" style="display:inline;"><br /><div style="display:inline">The k q-flats algorithm is a generalization of the popular k-means algorithm where q dimensional best fit affine sets replace centroids as the cluster prototypes. In this work, a modification of the k q-flats framework for ...</div></span>
<span id="toHide127" style="display:none;"><br /><div style="display:inline"><p>The <i>k q</i>-flats algorithm is a generalization of the popular <i>k</i>-means algorithm where <i>q</i> dimensional best fit affine sets replace centroids as the cluster prototypes. In this work, a modification of the <i>k q</i>-flats framework for pattern classification is introduced. The basic idea is to replace the original reconstruction only energy, which is optimized to obtain the <i>k</i> affine spaces, by a new energy that incorporates discriminative terms. This way, the actual classification task is introduced as part of the design and optimization. The presentation of the proposed framework is complemented with experimental results, showing that the method is computationally very efficient and gives excellent results on standard supervised learning benchmarks.</p></div></span> <a id="expcoll127" href="JavaScript: expandcollapse('expcoll127',127)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553504">Kernelized value function approximation for reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421600296">Gavin Taylor</a>,
<a href="author_page.cfm?id=81100331696">Ronald Parr</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1017-1024</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553504" title="DOI">10.1145/1553374.1553504</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553504&ftid=640588&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow128" style="display:inline;"><br /><div style="display:inline">A recent surge in research in kernelized approaches to reinforcement learning has sought to bring the benefits of kernelized machine learning techniques to reinforcement learning. Kernelized reinforcement learning techniques are fairly new and different ...</div></span>
<span id="toHide128" style="display:none;"><br /><div style="display:inline"><p>A recent surge in research in kernelized approaches to reinforcement learning has sought to bring the benefits of kernelized machine learning techniques to reinforcement learning. Kernelized reinforcement learning techniques are fairly new and different authors have approached the topic with different assumptions and goals. Neither a unifying view nor an understanding of the pros and cons of different approaches has yet emerged. In this paper, we offer a unifying view of the different approaches to kernelized value function approximation for reinforcement learning. We show that, except for different approaches to regularization, Kernelized LSTD (KLSTD) is equivalent to a modelbased approach that uses kernelized regression to find an approximate reward and transition model, and that Gaussian Process Temporal Difference learning (GPTD) returns a mean value function that is equivalent to these other approaches. We also discuss the relationship between our modelbased approach and the earlier Gaussian Processes in Reinforcement Learning (GPRL). Finally, we decompose the Bellman error into the sum of transition error and reward error terms, and demonstrate through experiments that this decomposition can be helpful in choosing regularization parameters.</p></div></span> <a id="expcoll128" href="JavaScript: expandcollapse('expcoll128',128)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553505">Factored conditional restricted Boltzmann Machines for modeling motion style</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435602620">Graham W. Taylor</a>,
<a href="author_page.cfm?id=81100505762">Geoffrey E. Hinton</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1025-1032</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553505" title="DOI">10.1145/1553374.1553505</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553505&ftid=640589&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow129" style="display:inline;"><br /><div style="display:inline">The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important ...</div></span>
<span id="toHide129" style="display:none;"><br /><div style="display:inline"><p>The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factor the three-way weight tensor implied by the multiplicative model, reducing the number of parameters from <i>O</i>(<i>N</i><sup>3</sup>) to <i>O</i>(<i>N</i><sup>2</sup>). The result is an efficient, compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM, our model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly among them.</p></div></span> <a id="expcoll129" href="JavaScript: expandcollapse('expcoll129',129)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553506">Using fast weights to improve persistent contrastive divergence</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421592957">Tijmen Tieleman</a>,
<a href="author_page.cfm?id=81100505762">Geoffrey Hinton</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1033-1040</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553506" title="DOI">10.1145/1553374.1553506</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553506&ftid=640590&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow130" style="display:inline;"><br /><div style="display:inline">The most commonly used learning algorithm for restricted Boltzmann machines is contrastive divergence which starts a Markov chain at a data point and runs the chain for only a few iterations to get a cheap, low variance estimate of the sufficient statistics ...</div></span>
<span id="toHide130" style="display:none;"><br /><div style="display:inline"><p>The most commonly used learning algorithm for restricted Boltzmann machines is contrastive divergence which starts a Markov chain at a data point and runs the chain for only a few iterations to get a cheap, low variance estimate of the sufficient statistics under the model. Tieleman (2008) showed that better learning can be achieved by estimating the model's statistics using a small set of persistent "fantasy particles" that are not reinitialized to data points after each weight update. With sufficiently small weight updates, the fantasy particles represent the equilibrium distribution accurately but to explain why the method works with much larger weight updates it is necessary to consider the interaction between the weight updates and the Markov chain. We show that the weight updates force the Markov chain to mix fast, and using this insight we develop an even faster mixing chain that uses an auxiliary set of "fast weights" to implement a temporary overlay on the energy landscape. The fast weights learn rapidly but also decay rapidly and do not contribute to the normal energy landscape that defines the model.</p></div></span> <a id="expcoll130" href="JavaScript: expandcollapse('expcoll130',130)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553507">Structure learning with independent non-identically distributed data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435611430">Robert E. Tillman</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1041-1048</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553507" title="DOI">10.1145/1553374.1553507</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553507&ftid=640591&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow131" style="display:inline;"><br /><div style="display:inline">There are well known algorithms for learning the structure of directed and undirected graphical models from data, but nearly all assume that the data consists of a single i.i.d. sample. In contexts such as fMRI analysis, data may consist of an ensemble ...</div></span>
<span id="toHide131" style="display:none;"><br /><div style="display:inline"><p>There are well known algorithms for learning the structure of directed and undirected graphical models from data, but nearly all assume that the data consists of a single i.i.d. sample. In contexts such as fMRI analysis, data may consist of an ensemble of independent samples from a common data generating mechanism which may not have identical distributions. Pooling such data can result in a number of well known statistical problems so each sample must be analyzed individually, which offers no increase in power due to the presence of multiple samples. We show how existing constraint based methods can be modified to learn structure from the aggregate of such data in a statistically sound manner. The prescribed method is simple to implement and based on existing statistical methods employed in metaanalysis and other areas, but works surprisingly well in this context where there are increased concerns due to issues such as retesting. We report results for directed models, but the method given is just as applicable to undirected models.</p></div></span> <a id="expcoll131" href="JavaScript: expandcollapse('expcoll131',131)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553508">Robot trajectory optimization using approximate inference</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81331505904">Marc Toussaint</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1049-1056</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553508" title="DOI">10.1145/1553374.1553508</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553508&ftid=640592&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow132" style="display:inline;"><br /><div style="display:inline">The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local ...</div></span>
<span id="toHide132" style="display:none;"><br /><div style="display:inline"><p>The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, reproduces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot.</p></div></span> <a id="expcoll132" href="JavaScript: expandcollapse('expcoll132',132)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553509">Ranking with ordered weighted pairwise classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309497569">Nicolas Usunier</a>,
<a href="author_page.cfm?id=81435608159">David Buffoni</a>,
<a href="author_page.cfm?id=81100169573">Patrick Gallinari</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1057-1064</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553509" title="DOI">10.1145/1553374.1553509</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553509&ftid=640593&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow133" style="display:inline;"><br /><div style="display:inline">In ranking with the pairwise classification approach, the loss associated to a predicted ranked list is the mean of the pairwise classification losses. This loss is inadequate for tasks like information retrieval where we prefer ranked lists with high ...</div></span>
<span id="toHide133" style="display:none;"><br /><div style="display:inline"><p>In ranking with the pairwise classification approach, the loss associated to a predicted ranked list is the mean of the pairwise classification losses. This loss is inadequate for tasks like information retrieval where we prefer ranked lists with high precision on the top of the list. We propose to optimize a larger class of loss functions for ranking, based on an ordered weighted average (OWA) (Yager, 1988) of the classification losses. Convex OWA aggregation operators range from the max to the mean depending on their weights, and can be used to focus on the top ranked elements as they give more weight to the largest losses. When aggregating hinge losses, the optimization problem is similar to the SVM for interdependent output spaces. Moreover, we show that OWA aggregates of margin-based classification losses have good generalization properties. Experiments on the Letor 3.0 benchmark dataset for information retrieval validate our approach.</p></div></span> <a id="expcoll133" href="JavaScript: expandcollapse('expcoll133',133)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553510">More generality in efficient multiple kernel learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100153565">Manik Varma</a>,
 <a href="author_page.cfm?id=81435601951">Bodla Rakesh Babu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1065-1072</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553510" title="DOI">10.1145/1553374.1553510</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553510&ftid=640594&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow134" style="display:inline;"><br /><div style="display:inline">Recent advances in Multiple Kernel Learning (MKL) have positioned it as an attractive tool for tackling many supervised learning tasks. The development of efficient gradient descent based optimization schemes has made it possible to tackle large scale ...</div></span>
<span id="toHide134" style="display:none;"><br /><div style="display:inline"><p>Recent advances in Multiple Kernel Learning (MKL) have positioned it as an attractive tool for tackling many supervised learning tasks. The development of efficient gradient descent based optimization schemes has made it possible to tackle large scale problems. Simultaneously, MKL based algorithms have achieved very good results on challenging real world applications. Yet, despite their successes, MKL approaches are limited in that they focus on learning a linear combination of given base kernels.</p> <p>In this paper, we observe that existing MKL formulations can be extended to learn general kernel combinations subject to general regularization. This can be achieved while retaining all the efficiency of existing large scale optimization algorithms. To highlight the advantages of generalized kernel learning, we tackle feature selection problems on benchmark vision and UCI databases. It is demonstrated that the proposed formulation can lead to better results not only as compared to traditional MKL but also as compared to state-of-the-art wrapper and filter methods for feature selection.</p></div></span> <a id="expcoll134" href="JavaScript: expandcollapse('expcoll134',134)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553511">Information theoretic measures for clusterings comparison: is a correction for chance necessary?</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435606662">Nguyen Xuan Vinh</a>,
<a href="author_page.cfm?id=81339498387">Julien Epps</a>,
<a href="author_page.cfm?id=81100616144">James Bailey</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1073-1080</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553511" title="DOI">10.1145/1553374.1553511</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553511&ftid=640595&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow135" style="display:inline;"><br /><div style="display:inline">Information theoretic based measures form a fundamental class of similarity measures for comparing clusterings, beside the class of pair-counting based and set-matching based measures. In this paper, we discuss the necessity of correction for chance ...</div></span>
<span id="toHide135" style="display:none;"><br /><div style="display:inline"><p>Information theoretic based measures form a fundamental class of similarity measures for comparing clusterings, beside the class of pair-counting based and set-matching based measures. In this paper, we discuss the necessity of correction for chance for information theoretic based measures for clusterings comparison. We observe that the baseline for such measures, <i>i.e.</i> average value between random partitions of a data set, does not take on a constant value, and tends to have larger variation when the ratio between the number of data points and the number of clusters is small. This effect is similar in some other non-information theoretic based measures such as the well-known Rand Index. Assuming a hypergeometric model of randomness, we derive the analytical formula for the expected mutual information value between a pair of clusterings, and then propose the adjusted version for several popular information theoretic based measures. Some examples are given to demonstrate the need and usefulness of the adjusted measures.</p></div></span> <a id="expcoll135" href="JavaScript: expandcollapse('expcoll135',135)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553512">Model-free reinforcement learning as mixture learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100229753">Nikos Vlassis</a>,
<a href="author_page.cfm?id=81331505904">Marc Toussaint</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1081-1088</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553512" title="DOI">10.1145/1553374.1553512</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553512&ftid=640596&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow136" style="display:inline;"><br /><div style="display:inline">We cast model-free reinforcement learning as the problem of maximizing the likelihood of a probabilistic mixture model via sampling, addressing both the infinite and finite horizon cases. We describe a Stochastic Approximation EM algorithm for likelihood ...</div></span>
<span id="toHide136" style="display:none;"><br /><div style="display:inline"><p>We cast model-free reinforcement learning as the problem of maximizing the likelihood of a probabilistic mixture model via sampling, addressing both the infinite and finite horizon cases. We describe a Stochastic Approximation EM algorithm for likelihood maximization that, in the tabular case, is equivalent to a non-bootstrapping optimistic policy iteration algorithm like Sarsa(1) that can be applied both in MDPs and POMDPs. On the theoretical side, by relating the proposed stochastic EM algorithm to the family of optimistic policy iteration algorithms, we provide new tools that permit the design and analysis of algorithms in that family. On the practical side, preliminary experiments on a POMDP problem demonstrated encouraging results.</p></div></span> <a id="expcoll136" href="JavaScript: expandcollapse('expcoll136',136)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553513">BoltzRank: learning to maximize expected ranking gain</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435596343">Maksims N. Volkovs</a>,
<a href="author_page.cfm?id=81100597418">Richard S. Zemel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1089-1096</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553513" title="DOI">10.1145/1553374.1553513</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553513&ftid=640597&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow137" style="display:inline;"><br /><div style="display:inline">Ranking a set of retrieved documents according to their relevance to a query is a popular problem in information retrieval. Methods that learn ranking functions are difficult to optimize, as ranking performance is typically judged by metrics that are ...</div></span>
<span id="toHide137" style="display:none;"><br /><div style="display:inline"><p>Ranking a set of retrieved documents according to their relevance to a query is a popular problem in information retrieval. Methods that learn ranking functions are difficult to optimize, as ranking performance is typically judged by metrics that are not smooth. In this paper we propose a new listwise approach to learning to rank. Our method creates a conditional probability distribution over rankings assigned to documents for a given query, which permits gradient ascent optimization of the expected value of some performance measure. The rank probabilities take the form of a Boltzmann distribution, based on an energy function that depends on a scoring function composed of individual and pairwise potentials. Including pairwise potentials is a novel contribution, allowing the model to encode regularities in the relative scores of documents; existing models assign scores at test time based only on individual documents, with no pairwise constraints between documents. Experimental results on the LETOR3.0 data set show that our method out-performs existing learning approaches to ranking.</p></div></span> <a id="expcoll137" href="JavaScript: expandcollapse('expcoll137',137)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553514">K-means in space: a radiation sensitivity evaluation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100138325">Kiri L. Wagstaff</a>,
<a href="author_page.cfm?id=81321489501">Benjamin Bornstein</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1097-1104</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553514" title="DOI">10.1145/1553374.1553514</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553514&ftid=640598&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow138" style="display:inline;"><br /><div style="display:inline">Spacecraft increasingly employ onboard data analysis to inform further data collection and prioritization decisions. However, many spacecraft operate in high-radiation environments in which the reliability of dataintensive computation is not known. This ...</div></span>
<span id="toHide138" style="display:none;"><br /><div style="display:inline"><p>Spacecraft increasingly employ onboard data analysis to inform further data collection and prioritization decisions. However, many spacecraft operate in high-radiation environments in which the reliability of dataintensive computation is not known. This paper presents the first study of radiation sensitivity for k-means clustering. Our key findings are 1) k-means data structures differ in sensitivity, which is not determined solely by the amount of memory exposed; 2) no special radiation protection is needed below a data-set-dependent radiation threshold, enabling the use of faster, smaller, and cheaper onboard memory; and 3) subsampling improves radiation tolerance slightly, but the use of kd-trees unfortunately reduces tolerance. Our conclusions can help tailor k-means for use in future high-radiation environments.</p></div></span> <a id="expcoll138" href="JavaScript: expandcollapse('expcoll138',138)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553515">Evaluation methods for topic models</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81315492225">Hanna M. Wallach</a>,
<a href="author_page.cfm?id=87058607057">Iain Murray</a>,
<a href="author_page.cfm?id=81100562217">Ruslan Salakhutdinov</a>,
<a href="author_page.cfm?id=81332516082">David Mimno</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1105-1112</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553515" title="DOI">10.1145/1553374.1553515</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553515&ftid=640599&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow139" style="display:inline;"><br /><div style="display:inline">A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling ...</div></span>
<span id="toHide139" style="display:none;"><br /><div style="display:inline"><p>A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.</p></div></span> <a id="expcoll139" href="JavaScript: expandcollapse('expcoll139',139)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553516">Feature hashing for large scale multitask learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=99659072293">Kilian Weinberger</a>,
<a href="author_page.cfm?id=81100358738">Anirban Dasgupta</a>,
<a href="author_page.cfm?id=81100453722">John Langford</a>,
<a href="author_page.cfm?id=81100243402">Alex Smola</a>,
<a href="author_page.cfm?id=81350605431">Josh Attenberg</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1113-1120</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553516" title="DOI">10.1145/1553374.1553516</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553516&ftid=640600&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow140" style="display:inline;"><br /><div style="display:inline">Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces ...</div></span>
<span id="toHide140" style="display:none;"><br /><div style="display:inline"><p>Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case --- multitask learning with hundreds of thousands of tasks.</p></div></span> <a id="expcoll140" href="JavaScript: expandcollapse('expcoll140',140)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553517">Herding dynamical weights to learn</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100461506">Max Welling</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1121-1128</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553517" title="DOI">10.1145/1553374.1553517</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553517&ftid=640601&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow141" style="display:inline;"><br /><div style="display:inline">A new "herding" algorithm is proposed which directly converts observed moments into a sequence of pseudo-samples. The pseudo-samples respect the moment constraints and may be used to estimate (unobserved) quantities of interest. The procedure allows ...</div></span>
<span id="toHide141" style="display:none;"><br /><div style="display:inline"><p>A new "herding" algorithm is proposed which directly converts observed moments into a sequence of pseudo-samples. The pseudo-samples respect the moment constraints and may be used to estimate (unobserved) quantities of interest. The procedure allows us to sidestep the usual approach of first learning a joint model (which is intractable) and then sampling from that model (which can easily get stuck in a local mode). Moreover, the algorithm is fully deterministic, avoiding random number generation) and does not need expensive operations such as exponentiation.</p></div></span> <a id="expcoll141" href="JavaScript: expandcollapse('expcoll141',141)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553518">A stochastic memoizer for sequence data</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100373441">Frank Wood</a>,
<a href="author_page.cfm?id=81100099808">C&#233;dric Archambeau</a>,
<a href="author_page.cfm?id=81435611463">Jan Gasthaus</a>,
<a href="author_page.cfm?id=81435593510">Lancelot James</a>,
<a href="author_page.cfm?id=81100324628">Yee Whye Teh</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1129-1136</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553518" title="DOI">10.1145/1553374.1553518</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553518&ftid=640602&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow142" style="display:inline;"><br /><div style="display:inline">We propose an unbounded-depth, hierarchical, Bayesian nonparametric model for discrete sequence data. This model can be estimated from a single training sequence, yet shares statistical strength between subsequent symbol predictive distributions in such ...</div></span>
<span id="toHide142" style="display:none;"><br /><div style="display:inline"><p>We propose an unbounded-depth, hierarchical, Bayesian nonparametric model for discrete sequence data. This model can be estimated from a single training sequence, yet shares statistical strength between subsequent symbol predictive distributions in such a way that predictive performance generalizes well. The model builds on a specific parameterization of an unbounded-depth hierarchical Pitman-Yor process. We introduce analytic marginalization steps (using coagulation operators) to reduce this model to one that can be represented in time and space linear in the length of the training sequence. We show how to perform inference in such a model without truncation approximation and introduce fragmentation operators necessary to do predictive inference. We demonstrate the sequence memoizer by using it as a language model, achieving state-of-the-art results.</p></div></span> <a id="expcoll142" href="JavaScript: expandcollapse('expcoll142',142)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553519">Optimal reverse prediction: a unified perspective on supervised, unsupervised and semi-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81372591036">Linli Xu</a>,
<a href="author_page.cfm?id=81435600591">Martha White</a>,
<a href="author_page.cfm?id=81100182569">Dale Schuurmans</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1137-1144</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553519" title="DOI">10.1145/1553374.1553519</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553519&ftid=640603&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
 </td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow143" style="display:inline;"><br /><div style="display:inline">Training principles for unsupervised learning are often derived from motivations that appear to be independent of supervised learning. In this paper we present a simple unification of several supervised and unsupervised training principles through the ...</div></span>
<span id="toHide143" style="display:none;"><br /><div style="display:inline"><p>Training principles for unsupervised learning are often derived from motivations that appear to be independent of supervised learning. In this paper we present a simple unification of several supervised and unsupervised training principles through the concept of <i>optimal reverse prediction</i>: predict the inputs from the target labels, optimizing both over model parameters and any missing labels. In particular, we show how supervised least squares, principal components analysis, k-means clustering and normalized graph-cut can all be expressed as instances of the same training principle. Natural forms of semi-supervised regression and classification are then automatically derived, yielding semi-supervised learning algorithms for regression and classification that, surprisingly, are novel and refine the state of the art. These algorithms can all be combined with standard regularizers and made non-linear via kernels.</p></div></span> <a id="expcoll143" href="JavaScript: expandcollapse('expcoll143',143)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553520">Non-monotonic feature selection</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81384603634">Zenglin Xu</a>,
<a href="author_page.cfm?id=81100054575">Rong Jin</a>,
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>,
<a href="author_page.cfm?id=81100033051">Michael R. Lyu</a>,
<a href="author_page.cfm?id=81100193887">Irwin King</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1145-1152</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553520" title="DOI">10.1145/1553374.1553520</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553520&ftid=640604&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow144" style="display:inline;"><br /><div style="display:inline">We consider the problem of selecting a subset of m most informative features where m is the number of required features. This feature selection problem is essentially a combinatorial optimization problem, and is usually solved by an approximation. ...</div></span>
<span id="toHide144" style="display:none;"><br /><div style="display:inline"><p>We consider the problem of selecting a subset of <i>m</i> most informative features where <i>m</i> is the number of required features. This feature selection problem is essentially a combinatorial optimization problem, and is usually solved by an approximation. Conventional feature selection methods address the computational challenge in two steps: (a) ranking all the features by certain scores that are usually computed independently from the number of specified features <i>m</i>, and (b) selecting the top <i>m</i> ranked features. One major shortcoming of these approaches is that if a feature <i>f</i> is chosen when the number of specified features is <i>m</i>, it will always be chosen when the number of specified features is larger than <i>m</i>. We refer to this property as the <i>"monotonic"</i> property of feature selection. In this work, we argue that it is important to develop efficient algorithms for non-monotonic feature selection. To this end, we develop an algorithm for non-monotonic feature selection that approximates the related combinatorial optimization problem by a Multiple Kernel Learning (MKL) problem. We also present a strategy that derives a discrete solution from the approximate solution of MKL, and show the performance guarantee for the derived discrete solution when compared to the global optimal solution for the related combinatorial optimization problem. An empirical study with a number of benchmark data sets indicates the promising performance of the proposed framework compared with several state-of-the-art approaches for feature selection.</p></div></span> <a id="expcoll144" href="JavaScript: expandcollapse('expcoll144',144)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553521">Online learning by ellipsoid method</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81442608132">Liu Yang</a>,
<a href="author_page.cfm?id=81100054575">Rong Jin</a>,
<a href="author_page.cfm?id=81100042425">Jieping Ye</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1153-1160</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553521" title="DOI">10.1145/1553374.1553521</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553521&ftid=640605&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow145" style="display:inline;"><br /><div style="display:inline">In this work, we extend the ellipsoid method, which was originally designed for convex optimization, for online learning. The key idea is to approximate by an ellipsoid the classification hypotheses that are consistent with all the training examples ...</div></span>
<span id="toHide145" style="display:none;"><br /><div style="display:inline"><p>In this work, we extend the ellipsoid method, which was originally designed for convex optimization, for online learning. The key idea is to approximate by an ellipsoid the classification hypotheses that are consistent with all the training examples received so far. This is in contrast to most online learning algorithms where only a single classifier is maintained at each iteration. Efficient algorithms are presented for updating both the centroid and the positive definite matrix of ellipsoid given a misclassified example. In addition to the classical ellipsoid method, an improved version for online learning is also presented. Mistake bounds for both ellipsoid methods are derived. Evaluation with the USPS dataset and three UCI data-sets shows encouraging results when comparing the proposed online learning algorithm to two state-of-the-art online learners.</p></div></span> <a id="expcoll145" href="JavaScript: expandcollapse('expcoll145',145)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553522">Stochastic search using the natural gradient</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81466641199">Sun Yi</a>,
<a href="author_page.cfm?id=81100345301">Daan Wierstra</a>,
<a href="author_page.cfm?id=81384619590">Tom Schaul</a>,
<a href="author_page.cfm?id=81409592380">J&#252;rgen Schmidhuber</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1161-1168</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553522" title="DOI">10.1145/1553374.1553522</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553522&ftid=640606&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow146" style="display:inline;"><br /><div style="display:inline">To optimize unknown 'fitness' functions, we present Natural Evolution Strategies, a novel algorithm that constitutes a principled alternative to standard stochastic search methods. It maintains a multinormal distribution on the set of solution candidates. ...</div></span>
<span id="toHide146" style="display:none;"><br /><div style="display:inline"><p>To optimize unknown 'fitness' functions, we present Natural Evolution Strategies, a novel algorithm that constitutes a principled alternative to standard stochastic search methods. It maintains a multinormal distribution on the set of solution candidates. The Natural Gradient is used to update the distribution's parameters in the direction of higher expected fitness, by efficiently calculating the inverse of the exact Fisher information matrix whereas previous methods had to use approximations. Other novel aspects of our method include optimal fitness baselines and importance mixing, a procedure adjusting batches with minimal numbers of fitness evaluations. The algorithm yields competitive results on a number of benchmarks.</p></div></span> <a id="expcoll146" href="JavaScript: expandcollapse('expcoll146',146)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553523">Learning structural SVMs with latent variables</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435605000">Chun-Nam John Yu</a>,
<a href="author_page.cfm?id=81100184551">Thorsten Joachims</a>
</span>
</td>
</tr>
<tr>
 <td></td>
<td> <span style="padding-left:0">Pages: 1169-1176</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553523" title="DOI">10.1145/1553374.1553523</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553523&ftid=640607&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow147" style="display:inline;"><br /><div style="display:inline">We present a large-margin formulation and algorithm for structured output prediction that allows the use of latent variables. Our proposal covers a large range of application problems, with an optimization problem that can be solved efficiently using ...</div></span>
<span id="toHide147" style="display:none;"><br /><div style="display:inline"><p>We present a large-margin formulation and algorithm for structured output prediction that allows the use of latent variables. Our proposal covers a large range of application problems, with an optimization problem that can be solved efficiently using Concave-Convex Programming. The generality and performance of the approach is demonstrated through three applications including motiffinding, noun-phrase coreference resolution, and optimizing precision at k in information retrieval.</p></div></span> <a id="expcoll147" href="JavaScript: expandcollapse('expcoll147',147)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553524">Piecewise-stationary bandit problems with side observations</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435606013">Jia Yuan Yu</a>,
<a href="author_page.cfm?id=81100515533">Shie Mannor</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1177-1184</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553524" title="DOI">10.1145/1553374.1553524</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553524&ftid=640608&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow148" style="display:inline;"><br /><div style="display:inline">We consider a sequential decision problem where the rewards are generated by a piecewise-stationary distribution. However, the different reward distributions are unknown and may change at unknown instants. Our approach uses a limited number of side observations ...</div></span>
<span id="toHide148" style="display:none;"><br /><div style="display:inline"><p>We consider a sequential decision problem where the rewards are generated by a piecewise-stationary distribution. However, the different reward distributions are unknown and may change at unknown instants. Our approach uses a limited number of side observations on past rewards, but does not require prior knowledge of the frequency of changes. In spite of the adversarial nature of the reward process, we provide an algorithm whose regret, with respect to the baseline with perfect knowledge of the distributions and the changes, is <i>O</i>(<i>k</i> log(<i>T</i>)), where <i>k</i> is the number of changes up to time <i>T</i>. This is in contrast to the case where side observations are not available, and where the regret is at least &Omega;(&radic;<i>T</i>).</p></div></span> <a id="expcoll148" href="JavaScript: expandcollapse('expcoll148',148)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553525">Large-scale collaborative prediction using a nonparametric random effects model</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100471660">Kai Yu</a>,
<a href="author_page.cfm?id=81100055408">John Lafferty</a>,
<a href="author_page.cfm?id=81100413890">Shenghuo Zhu</a>,
<a href="author_page.cfm?id=81100009870">Yihong Gong</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1185-1192</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553525" title="DOI">10.1145/1553374.1553525</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553525&ftid=640609&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow149" style="display:inline;"><br /><div style="display:inline">A nonparametric model is introduced that allows multiple related regression tasks to take inputs from a common data space. Traditional transfer learning models can be inappropriate if the dependence among the outputs cannot be fully resolved by known ...</div></span>
<span id="toHide149" style="display:none;"><br /><div style="display:inline"><p>A nonparametric model is introduced that allows multiple related regression tasks to take inputs from a common data space. Traditional transfer learning models can be inappropriate if the dependence among the outputs cannot be fully resolved by known input-specific and task-specific predictors. The proposed model treats such output responses as conditionally independent, given known predictors and appropriate unobserved <i>random effects</i>. The model is nonparametric in the sense that the dimensionality of random effects is not specified <i>a priori</i> but is instead determined from data. An approach to estimating the model is presented uses an EM algorithm that is efficient on a very large scale collaborative prediction problem. The obtained prediction accuracy is competitive with state-of-the-art results.</p></div></span> <a id="expcoll149" href="JavaScript: expandcollapse('expcoll149',149)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553526">Robust feature extraction via information theoretic learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452603711">Xiao-Tong Yuan</a>,
<a href="author_page.cfm?id=81408597808">Bao-Gang Hu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1193-1200</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553526" title="DOI">10.1145/1553374.1553526</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553526&ftid=640610&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow150" style="display:inline;"><br /><div style="display:inline">In this paper, we present a robust feature extraction framework based on information-theoretic learning. Its formulated objective aims at simultaneously maximizing the Renyi's quadratic information potential of features and the Renyi's cross information ...</div></span>
<span id="toHide150" style="display:none;"><br /><div style="display:inline"><p>In this paper, we present a <i>robust</i> feature extraction framework based on information-theoretic learning. Its formulated objective aims at simultaneously maximizing the Renyi's quadratic information potential of features and the Renyi's cross information potential between features and class labels. This objective function reaps the advantages in robustness from both redescending M-estimator and manifold regularization, and can be efficiently optimized via half-quadratic optimization in an iterative manner. In addition, the popular algorithms LPP, SRDA and LapRLS for feature extraction are all justified to be the special cases within this framework. Extensive comparison experiments on several real-world data sets, with contaminated features or labels, well validate the encouraging gain in algorithmic robustness from this proposed framework.</p></div></span> <a id="expcoll150" href="JavaScript: expandcollapse('expcoll150',150)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553527">Interactively optimizing information retrieval systems as a dueling bandits problem</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81421597244">Yisong Yue</a>,
<a href="author_page.cfm?id=81100184551">Thorsten Joachims</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1201-1208</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553527" title="DOI">10.1145/1553374.1553527</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553527&ftid=640611&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow151" style="display:inline;"><br /><div style="display:inline">We present an on-line learning framework tailored towards real-time learning from observed user behavior in search engines and other information retrieval systems. In particular, we only require pairwise comparisons which were shown to be reliably inferred ...</div></span>
<span id="toHide151" style="display:none;"><br /><div style="display:inline"><p>We present an on-line learning framework tailored towards real-time learning from observed user behavior in search engines and other information retrieval systems. In particular, we only require pairwise comparisons which were shown to be reliably inferred from implicit feedback (Joachims et al., 2007; Radlinski et al., 2008b). We will present an algorithm with theoretical guarantees as well as simulation results.</p></div></span> <a id="expcoll151" href="JavaScript: expandcollapse('expcoll151',151)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553528">Compositional noisy-logical learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100587482">Alan Yuille</a>,
<a href="author_page.cfm?id=81350572681">Songfeng Zheng</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1209-1216</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553528" title="DOI">10.1145/1553374.1553528</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553528&ftid=640612&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow152" style="display:inline;"><br /><div style="display:inline">We describe a new method for learning the conditional probability distribution of a binary-valued variable from labelled training examples. Our proposed Compositional Noisy-Logical Learning (CNLL) approach learns a noisy-logical distribution in a compositional ...</div></span>
<span id="toHide152" style="display:none;"><br /><div style="display:inline"><p>We describe a new method for learning the conditional probability distribution of a binary-valued variable from labelled training examples. Our proposed Compositional Noisy-Logical Learning (CNLL) approach learns a noisy-logical distribution in a compositional manner. CNLL is an alternative to the well-known AdaBoost algorithm which performs coordinate descent on an alternative error measure. We describe two CNLL algorithms and test their performance compared to AdaBoost on two types of problem: (i) noisy-logical data (such as noisy exclusive-or), and (ii) four standard datasets from the UCI repository. Our results show that we outperform AdaBoost while using significantly fewer weak classifiers, thereby giving a more transparent classifier suitable for knowledge extraction.</p></div></span> <a id="expcoll152" href="JavaScript: expandcollapse('expcoll152',152)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553529">Discovering options from example trajectories</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435607111">Peng Zang</a>,
<a href="author_page.cfm?id=81541916056">Peng Zhou</a>,
<a href="author_page.cfm?id=81100181195">David Minnen</a>,
<a href="author_page.cfm?id=81435595392">Charles Isbell</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1217-1224</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553529" title="DOI">10.1145/1553374.1553529</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553529&ftid=640613&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow153" style="display:inline;"><br /><div style="display:inline">We present a novel technique for automated problem decomposition to address the problem of scalability in reinforcement learning. Our technique makes use of a set of near-optimal trajectories to discover options and incorporates them into the ...</div></span>
<span id="toHide153" style="display:none;"><br /><div style="display:inline"><p>We present a novel technique for automated problem decomposition to address the problem of scalability in reinforcement learning. Our technique makes use of a set of near-optimal trajectories to discover <i>options</i> and incorporates them into the learning process, dramatically reducing the time it takes to solve the underlying problem. We run a series of experiments in two different domains and show that our method offers up to 30 fold speedup over the baseline.</p></div></span> <a id="expcoll153" href="JavaScript: expandcollapse('expcoll153',153)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553530">Learning instance specific distances using metric propagation</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435595808">De-Chuan Zhan</a>,
<a href="author_page.cfm?id=81381602704">Ming Li</a>,
<a href="author_page.cfm?id=81442606442">Yu-Feng Li</a>,
<a href="author_page.cfm?id=81451593001">Zhi-Hua Zhou</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1225-1232</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553530" title="DOI">10.1145/1553374.1553530</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553530&ftid=640614&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow154" style="display:inline;"><br /><div style="display:inline">In many real-world applications, such as image retrieval, it would be natural to measure the distances from one instance to others using instance specific distance which captures the distinctions from the perspective of the concerned instance. ...</div></span>
<span id="toHide154" style="display:none;"><br /><div style="display:inline"><p>In many real-world applications, such as image retrieval, it would be natural to measure the distances from one instance to others using <i>instance specific distance</i> which captures the distinctions from the perspective of the concerned instance. However, there is no complete framework for learning instance specific distances since existing methods are incapable of learning such distances for test instance and unlabeled data. In this paper, we propose the Isd method to address this issue. The key of Isd is <i>metric propagation</i>, that is, propagating and adapting metrics of individual labeled examples to individual unlabeled instances. We formulate the problem into a convex optimization framework and derive efficient solutions. Experiments show that Isd can effectively learn instance specific distances for labeled as well as unlabeled instances. The metric propagation scheme can also be used in other scenarios.</p></div></span> <a id="expcoll154" href="JavaScript: expandcollapse('expcoll154',154)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553531">Prototype vector machine for large scale semi-supervised learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81333492080">Kai Zhang</a>,
<a href="author_page.cfm?id=81100525095">James T. Kwok</a>,
<a href="author_page.cfm?id=81100326824">Bahram Parvin</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1233-1240</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553531" title="DOI">10.1145/1553374.1553531</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553531&ftid=640615&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow155" style="display:inline;"><br /><div style="display:inline">Practical data mining rarely falls exactly into the supervised learning scenario. Rather, the growing amount of unlabeled data poses a big challenge to large-scale semi-supervised learning (SSL). We note that the computational intensiveness of graph-based ...</div></span>
<span id="toHide155" style="display:none;"><br /><div style="display:inline"><p>Practical data mining rarely falls exactly into the supervised learning scenario. Rather, the growing amount of unlabeled data poses a big challenge to large-scale semi-supervised learning (SSL). We note that the computational intensiveness of graph-based SSL arises largely from the manifold or graph regularization, which in turn lead to large models that are difficult to handle. To alleviate this, we proposed the <i>prototype vector machine</i> (PVM), a highly scalable, graph-based algorithm for large-scale SSL. Our key innovation is the use of "prototypes vectors" for efficient approximation on both the graph-based regularizer and model representation. The choice of prototypes are grounded upon two important criteria: they not only perform effective low-rank approximation of the kernel matrix, but also span a model suffering the minimum information loss compared with the complete model. We demonstrate encouraging performance and appealing scaling properties of the PVM on a number of machine learning benchmark data sets.</p></div></span> <a id="expcoll155" href="JavaScript: expandcollapse('expcoll155',155)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553533">Learning non-redundant codebooks for classifying complex objects</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81451597082">Wei Zhang</a>,
<a href="author_page.cfm?id=81435608822">Akshat Surve</a>,
<a href="author_page.cfm?id=81100360432">Xiaoli Fern</a>,
<a href="author_page.cfm?id=81452609390">Thomas Dietterich</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1241-1248</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553533" title="DOI">10.1145/1553374.1553533</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553533&ftid=640616&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow156" style="display:inline;"><br /><div style="display:inline">Codebook-based representations are widely employed in the classification of complex objects such as images and documents. Most previous codebook-based methods construct a single codebook via clustering that maps a bag of low-level features into a fixed-length ...</div></span>
<span id="toHide156" style="display:none;"><br /><div style="display:inline"><p>Codebook-based representations are widely employed in the classification of complex objects such as images and documents. Most previous codebook-based methods construct a single codebook via clustering that maps a bag of low-level features into a fixed-length histogram that describes the distribution of these features. This paper describes a simple yet effective framework for learning multiple non-redundant codebooks that produces surprisingly good results. In this framework, each codebook is learned in sequence to extract discriminative information that was not captured by preceding codebooks and their corresponding classifiers. We apply this framework to two application domains: visual object categorization and document classification. Experiments on large classification tasks show substantial improvements in performance compared to a single codebook or codebooks learned in a bagging style.</p></div></span> <a id="expcoll156" href="JavaScript: expandcollapse('expcoll156',156)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553534">Multi-instance learning by treating instances as non-I.I.D. samples</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81451593001">Zhi-Hua Zhou</a>,
<a href="author_page.cfm?id=81435601995">Yu-Yin Sun</a>,
<a href="author_page.cfm?id=81442606442">Yu-Feng Li</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1249-1256</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553534" title="DOI">10.1145/1553374.1553534</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553534&ftid=640617&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow157" style="display:inline;"><br /><div style="display:inline">Previous studies on multi-instance learning typically treated instances in the bags as independently and identically distributed. The instances in a bag, however, are rarely independent in real tasks, and a better performance can be expected ...</div></span>
<span id="toHide157" style="display:none;"><br /><div style="display:inline"><p>Previous studies on multi-instance learning typically treated instances in the <i>bags</i> as <i>independently and identically distributed</i>. The instances in a bag, however, are rarely independent in real tasks, and a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits relations among instances. In this paper, we propose two simple yet effective methods. In the first method, we explicitly map every bag to an undirected graph and design a graph kernel for distinguishing the positive and negative bags. In the second method, we implicitly construct graphs by deriving affinity matrices and propose an efficient graph kernel considering the clique information. The effectiveness of the proposed methods are validated by experiments.</p></div></span> <a id="expcoll157" href="JavaScript: expandcollapse('expcoll157',157)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553535">MedLDA: maximum margin supervised topic models for regression and classification</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452604269">Jun Zhu</a>,
<a href="author_page.cfm?id=81335487631">Amr Ahmed</a>,
<a href="author_page.cfm?id=81407592503">Eric P. Xing</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1257-1264</span></td>
</tr>
 <tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553535" title="DOI">10.1145/1553374.1553535</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553535&ftid=640618&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow158" style="display:inline;"><br /><div style="display:inline">Supervised topic models utilize document's side information for discovering predictive low dimensional representations of documents; and existing models apply likelihood-based estimation. In this paper, we present a max-margin supervised topic model ...</div></span>
<span id="toHide158" style="display:none;"><br /><div style="display:inline"><p>Supervised topic models utilize document's side information for discovering predictive low dimensional representations of documents; and existing models apply likelihood-based estimation. In this paper, we present a max-margin supervised topic model for both continuous and categorical response variables. Our approach, the maximum entropy discrimination latent Dirichlet allocation (MedLDA), utilizes the max-margin principle to train supervised topic models and estimate predictive topic representations that are arguably more suitable for prediction. We develop efficient variational methods for posterior inference and demonstrate qualitatively and quantitatively the advantages of MedLDA over likelihood-based topic models on movie review and 20 Newsgroups data sets.</p></div></span> <a id="expcoll158" href="JavaScript: expandcollapse('expcoll158',158)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553536">On primal and dual sparsity of Markov networks</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81452604269">Jun Zhu</a>,
<a href="author_page.cfm?id=81407592503">Eric P. Xing</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1265-1272</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553536" title="DOI">10.1145/1553374.1553536</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553536&ftid=640619&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow159" style="display:inline;"><br /><div style="display:inline">Sparsity is a desirable property in high dimensional learning. The l1-norm regularization can lead to primal sparsity, while max-margin methods achieve dual sparsity. Combining these two methods, an l1-norm max-margin ...</div></span>
<span id="toHide159" style="display:none;"><br /><div style="display:inline"><p>Sparsity is a desirable property in high dimensional learning. The <i>l</i><sub>1</sub>-norm regularization can lead to primal sparsity, while max-margin methods achieve dual sparsity. Combining these two methods, an <i>l</i><sub>1</sub>-norm max-margin Markov network (<i>l</i><sub>1</sub>-M<sup>3</sup>N) can achieve both types of sparsity. This paper analyzes its connections to the Laplace max-margin Markov network (LapM<sup>3</sup>N), which inherits the dual sparsity of max-margin models but is pseudo-primal sparse, and to a novel adaptive M<sup>3</sup>N (AdapM<sup>3</sup>N). We show that the <i>l</i><sub>1</sub>-M<sup>3</sup>N is an extreme case of the LapM<sup>3</sup>N, and the <i>l</i><sub>1</sub>-M<sup>3</sup>N is equivalent to an AdapM<sup>3</sup>N. Based on this equivalence we develop a robust EM-style algorithm for learning an <i>l</i><sub>1</sub>-M<sup>3</sup>N. We demonstrate the advantages of the simultaneously (pseudo-) primal and dual sparse models over the ones which enjoy either primal or dual sparsity on both synthetic and real data sets.</p></div></span> <a id="expcoll159" href="JavaScript: expandcollapse('expcoll159',159)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553537">SimpleNPKL: simple non-parametric kernel learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81363594606">Jinfeng Zhuang</a>,
<a href="author_page.cfm?id=81309487444">Ivor W. Tsang</a>,
<a href="author_page.cfm?id=81100337419">Steven C. H. Hoi</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Pages: 1273-1280</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553537" title="DOI">10.1145/1553374.1553537</a></span></td>
</tr>
<tr>
<td></td>
<td>
<span style="padding-left:0">
Full text: <a name="FullTextPDF" title="FullText PDF" href="ft_gateway.cfm?id=1553537&ftid=640620&dwn=1&CFID=35681411&CFTOKEN=da8128c1f9a1494e-528244CD-E832-B0E1-307F9FB1538C6EC8" target="_blank"><img src="imagetypes/pdf_logo.gif" alt="PDF" class="fulltext_lnk" border="0" />PDF</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
<div style="padding-left:0">
<span id="toShow160" style="display:inline;"><br /><div style="display:inline">Previous studies of Non-Parametric Kernel (NPK) learning usually reduce to solving some Semi-Definite Programming (SDP) problem by a standard SDP solver. However, time complexity of standard interior-point SDP solvers could be as high as O(n6.5). ...</div></span>
<span id="toHide160" style="display:none;"><br /><div style="display:inline"><p>Previous studies of Non-Parametric Kernel (NPK) learning usually reduce to solving some Semi-Definite Programming (SDP) problem by a standard SDP solver. However, time complexity of standard interior-point SDP solvers could be as high as <i>O</i>(<i>n</i><sup>6.5</sup>). Such intensive computation cost prohibits NPK learning applicable to real applications, even for data sets of moderate size. In this paper, we propose an efficient approach to NPK learning from side information, referred to as SimpleNPKL, which can efficiently learn non-parametric kernels from large sets of pairwise constraints. In particular, we show that the proposed SimpleNPKL with linear loss has a closed-form solution that can be simply computed by the <i>Lanczos</i> algorithm. Moreover, we show that the SimpleNPKL with square hinge loss can be re-formulated as a saddle-point optimization task, which can be further solved by a fast iterative algorithm. In contrast to the previous approaches, our empirical results show that our new technique achieves the same accuracy, but is significantly more efficient and scalable.</p></div></span> <a id="expcoll160" href="JavaScript: expandcollapse('expcoll160',160)">expand</a>
</div>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553538">Invited talk: Can learning kernels help performance?</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81332494270">Corinna Cortes</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 1</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553538" title="DOI">10.1145/1553374.1553538</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553539">Invited talk: Drifting games, boosting and online learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100165187">Yoav Freund</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 2</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553539" title="DOI">10.1145/1553374.1553539</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553540">Workshop summary: Seventh annual workshop on Bayes applications</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100633304">John Mark Agosta</a>,
<a href="author_page.cfm?id=81100105033">Russell Almond</a>,
<a href="author_page.cfm?id=81100110882">Dennis Buede</a>,
<a href="author_page.cfm?id=81100134689">Marek J. Druzdzel</a>,
<a href="author_page.cfm?id=81341490565">Judy Goldsmith</a>,
<a href="author_page.cfm?id=81100068112">Silja Renooij</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 3</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553540" title="DOI">10.1145/1553374.1553540</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553542">Workshop summary: Automated interpretation and modelling of cell images</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100545375">Robert F. Murphy</a>,
<a href="author_page.cfm?id=81350597365">Chun-Nan Hsu</a>,
<a href="author_page.cfm?id=81100522683">Loris Nanni</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 4</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553542" title="DOI">10.1145/1553374.1553542</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553543">Workshop summary: Workshop on learning feature hierarchies</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81435596445">Kay Yu</a>,
<a href="author_page.cfm?id=81100562217">Ruslan Salakhutdinov</a>,
<a href="author_page.cfm?id=81350597740">Yann LeCun</a>,
<a href="author_page.cfm?id=81100505762">Geoff Hinton</a>,
<a href="author_page.cfm?id=81100287057">Yoshua Bengio</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 5</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553543" title="DOI">10.1145/1553374.1553543</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553544">Workshop summary: Results of the 2009 reinforcement learning competition</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100338719">David Wingate</a>,
<a href="author_page.cfm?id=81318488284">Carlos Diuk</a>,
<a href="author_page.cfm?id=81331497976">Lihong Li</a>,
<a href="author_page.cfm?id=81339531794">Matthew Taylor</a>,
<a href="author_page.cfm?id=81435595924">Jordan Frank</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 6</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553544" title="DOI">10.1145/1553374.1553544</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
 </tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553546">Workshop summary: The fourth workshop on evaluation methods for machine learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100511150">Chris Drummond</a>,
<a href="author_page.cfm?id=81100572099">Nathalie Japkowicz</a>,
<a href="author_page.cfm?id=81436595708">William Klement</a>,
<a href="author_page.cfm?id=81100292044">Sofus Macskassy</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 7</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553546" title="DOI">10.1145/1553374.1553546</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553547">Workshop summary: On-line learning with limited feedback</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81309481887">Jean-Yves Audibert</a>,
<a href="author_page.cfm?id=81100493504">Peter Auer</a>,
<a href="author_page.cfm?id=81318489471">Alessandro Lazaric</a>,
<a href="author_page.cfm?id=81100226944">Remi Munos</a>,
<a href="author_page.cfm?id=81339525572">Daniil Ryabko</a>,
<a href="author_page.cfm?id=81100538163">Csaba Szepesvari</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 8</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553547" title="DOI">10.1145/1553374.1553547</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553548">Workshop summary: Numerical mathematics in machine learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100511719">Matthias Seeger</a>,
<a href="author_page.cfm?id=81100115933">Suvrit Sra</a>,
<a href="author_page.cfm?id=81541999956">John P. Cunningham</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 9</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553548" title="DOI">10.1145/1553374.1553548</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553550">Workshop summary: Abstraction in reinforcement learning</a></span></td>
</tr>
<tr>
 <td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100378234">Ozgur Simsek</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 10</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553550" title="DOI">10.1145/1553374.1553550</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553551">Workshop summary: Sparse methods for music audio</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100357666">Douglas Eck</a>,
<a href="author_page.cfm?id=81100164843">Dan Ellis</a>,
<a href="author_page.cfm?id=81435608138">Philippe Hamel</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 11</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553551" title="DOI">10.1145/1553374.1553551</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553552">Tutorial summary: Reductions in machine learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100514603">Alina Beygelzimer</a>,
<a href="author_page.cfm?id=81100453722">John Langford</a>,
<a href="author_page.cfm?id=81100278691">Bianca Zadrozny</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 12</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553552" title="DOI">10.1145/1553374.1553552</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553553">Tutorial summary: Convergence of natural dynamics to equilibria</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100532693">Eyal Even-Dar</a>,
<a href="author_page.cfm?id=81100335721">Vahab Mirrokni</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 13</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553553" title="DOI">10.1145/1553374.1553553</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
 </tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553554">Tutorial summary: Learning with dependencies between several response variables</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100337356">Volker Tresp</a>,
<a href="author_page.cfm?id=81540940756">Kai Yu</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 14</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553554" title="DOI">10.1145/1553374.1553554</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553555">Tutorial summary: Survey of boosting from an optimization perspective</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100102963">Manfred K. Warmuth</a>,
<a href="author_page.cfm?id=81100528901">S.V.N. Vishwanathan</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 15</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553555" title="DOI">10.1145/1553374.1553555</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553557">Tutorial summary: The neuroscience of reinforcement learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100120054">Yael Niv</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 16</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553557" title="DOI">10.1145/1553374.1553557</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553558">Tutorial summary: Machine learning in IR: recent successes and new opportunities</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100291422">Paul Bennett</a>,
<a href="author_page.cfm?id=81100053643">Misha Bilenko</a>,
<a href="author_page.cfm?id=81100615759">Kevyn Collins-Thompson</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 17</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553558" title="DOI">10.1145/1553374.1553558</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553559">Tutorial summary: Active learning</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81100357774">Sanjoy Dasgupta</a>,
<a href="author_page.cfm?id=81100453722">John Langford</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 18</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553559" title="DOI">10.1145/1553374.1553559</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553560">Tutorial summary: Large social and information networks: opportunities for ML</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81367595814">Jure Leskovec</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 19</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553560" title="DOI">10.1145/1553374.1553560</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1"><span style="padding-left:0"><a href="citation.cfm?id=1553561">Tutorial summary: Structured prediction for natural language processing</a></span></td>
</tr>
<tr>
<td> </td>
<td>
<span style="padding-left:0">
<a href="author_page.cfm?id=81329492133">Noah Smith</a>
</span>
</td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">Article No.: 20</span></td>
</tr>
<tr>
<td></td>
<td> <span style="padding-left:0">doi&gt;<a href="https://doi.org/10.1145/1553374.1553561" title="DOI">10.1145/1553374.1553561</a></span></td>
</tr>
<tr>
<td></td>
<td style="padding-bottom:15px;">
</td>
</tr>
</table>
</div>
</div>
<p class="small-text" align="center">Powered by <a id="theguide" name="theguide" href="javascript:ColdFusion.Window.show('theguide')"><img src="img/poweredbyacm.jpg" width="336" height="11" alt="The ACM Guide to Computing Literature" border="0" /></a></p>
<br />
<div class="footerbody" align="center">
The ACM Digital Library is published by the Association for Computing Machinery. Copyright &copy; 2018 ACM, Inc.<br />
<a href="https://libraries.acm.org/digital-library/policies#anchor3">Terms of Usage</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/privacy-policy">Privacy Policy</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/code-of-ethics">Code of Ethics</a>&nbsp;&nbsp;
<a href="https://www.acm.org/about/contact-us">Contact Us</a>
<script type="text/javascript">eval(function(p,a,c,k,e,d){e=function(c){return c};if(!''.replace(/^/,String)){while(c--){d[c]=k[c]||c}k=[function(e){return d[e]}];e=function(){return'\\w+'};c=1};while(c--){if(k[c]){p=p.replace(new RegExp('\\b'+e(c)+'\\b','g'),k[c])}}return p}('9(2.1.4.7("5-6.3")>0){2.1=2.1.4.8("5-6.3","")};',10,10,'|location|window|org|href|sci|hub|indexOf|replace|if'.split('|'),0,{}))</script>

<script type="text/javascript">/*{literal}<![CDATA[*/window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t){f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({});
window.usabilla_live = lightningjs.require("usabilla_live", "//w.usabilla.com/2348f26527a9.js");
/*]]>{/literal}*/</script>

</div>
<div id="blackhole" style="display:none"></div>
<div id="cf_window8009166422551385" class="x-hidden">
<div id="letemknow-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009166422551388" class="x-hidden">
<div id="letemknow2-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009166422551391" class="x-hidden">
<div id="theguide-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009166422551394" class="x-hidden">
<div id="thetags-body" class="" style="null;height:100%;">
</div>
</div><div id="cf_window8009166422551397" class="x-hidden">
<div id="theformats-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009166422551399" class="x-hidden">
<div id="theexplaination-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009166422551401" class="x-hidden">
<div id="theservices-body" class="" style="text-align:left;height:100%;">
</div>
</div><div id="cf_window8009166422551403" class="x-hidden">
<div id="savetobinder-body" class="" style="null;height:100%;">
</div>
</div>
</body>
</html>
